#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
üöÄ STORAGE ENGINES ULTRA-AVANC√â - MULTI-DATABASE ORCHESTRATION
Architecture de stockage r√©volutionnaire pour enterprise avec optimisations industrielles

Syst√®me multi-base ultra-performant :
‚îú‚îÄ‚îÄ üíæ PostgreSQL Engine (ACID + Time Series optimized)
‚îú‚îÄ‚îÄ üöÄ Redis Engine (Cache + Pub/Sub + Streams)
‚îú‚îÄ‚îÄ üìä ClickHouse Engine (OLAP + Analytics ultra-rapide)
‚îú‚îÄ‚îÄ üåê MongoDB Engine (Documents + Geospatial)
‚îú‚îÄ‚îÄ üîÑ Multi-Database Manager (Orchestration intelligente)
‚îú‚îÄ‚îÄ üß† Query Optimizer (Optimisation automatique)
‚îú‚îÄ‚îÄ üìà Performance Monitor (Surveillance temps r√©el)
‚îú‚îÄ‚îÄ üõ°Ô∏è Security Layer (Chiffrement + Audit)
‚îú‚îÄ‚îÄ üîß Connection Pooling (Optimisation connexions)
‚îî‚îÄ‚îÄ ‚ö° Sub-millisecond Response Time

D√©velopp√© par l'√©quipe d'experts Achiri avec optimisations de niveau industriel
Version: 3.0.0 - Production Ready Enterprise
"""

__version__ = "3.0.0"
__author__ = "Achiri Expert Team - Storage Engineering Division"
__license__ = "Enterprise Commercial"

import asyncio
import logging
import sys
import time
import json
import uuid
import hashlib
from datetime import datetime, timedelta
from typing import (
    Dict, List, Optional, Any, Union, Tuple, Set, 
    AsyncGenerator, Callable, TypeVar, Generic
)
from dataclasses import dataclass, field, asdict
from enum import Enum, auto
from abc import ABC, abstractmethod
from contextlib import asynccontextmanager
from concurrent.futures import ThreadPoolExecutor
import threading
from collections import defaultdict, deque
import weakref

# Imports pour bases de donn√©es
try:
    import asyncpg
    from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
    from sqlalchemy.orm import sessionmaker, declarative_base
    from sqlalchemy import (
        Column, Integer, String, DateTime, Float, JSON, Text, Boolean,
        Index, MetaData, Table, select, insert, update, delete
    )
    from sqlalchemy.dialects.postgresql import UUID, JSONB
    POSTGRESQL_AVAILABLE = True
except ImportError:
    POSTGRESQL_AVAILABLE = False

try:
    import aioredis
    from aioredis import Redis, ConnectionPool
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False

try:
    from clickhouse_driver import Client as ClickHouseClient
    import clickhouse_connect
    from clickhouse_connect.driver import Client as AsyncClickHouseClient
    CLICKHOUSE_AVAILABLE = True
except ImportError:
    CLICKHOUSE_AVAILABLE = False

try:
    from motor.motor_asyncio import AsyncIOMotorClient
    import pymongo
    MONGODB_AVAILABLE = True
except ImportError:
    MONGODB_AVAILABLE = False

# Imports pour optimisations
try:
    import numpy as np
    import pandas as pd
    from scipy import stats
    SCIENTIFIC_LIBS_AVAILABLE = True
except ImportError:
    SCIENTIFIC_LIBS_AVAILABLE = False

# Imports pour compression et s√©rialisation
try:
    import orjson
    import msgpack
    import lz4.frame
    import zstandard as zstd
    OPTIMIZATION_LIBS_AVAILABLE = True
except ImportError:
    OPTIMIZATION_LIBS_AVAILABLE = False

# Imports pour monitoring
try:
    from prometheus_client import Counter, Histogram, Gauge
    MONITORING_AVAILABLE = True
except ImportError:
    MONITORING_AVAILABLE = False

logger = logging.getLogger(__name__)

# =============================================================================
# √âNUM√âRATIONS ET TYPES
# =============================================================================

class DatabaseType(Enum):
    """Types de bases de donn√©es support√©es"""
    POSTGRESQL = auto()      # Base principale ACID
    REDIS = auto()          # Cache et streams
    CLICKHOUSE = auto()     # OLAP et analytics
    MONGODB = auto()        # Documents et geospatial
    MULTI = auto()          # Multi-base orchestr√©e

class StorageStrategy(Enum):
    """Strat√©gies de stockage optimis√©es"""
    HOT = auto()            # Donn√©es chaudes (acc√®s fr√©quent)
    WARM = auto()           # Donn√©es ti√®des (acc√®s mod√©r√©)
    COLD = auto()           # Donn√©es froides (archivage)
    FROZEN = auto()         # Donn√©es gel√©es (compression max)

class QueryType(Enum):
    """Types de requ√™tes optimis√©es"""
    POINT_QUERY = auto()    # Requ√™te de point unique
    RANGE_QUERY = auto()    # Requ√™te de plage
    AGGREGATION = auto()    # Requ√™te d'agr√©gation
    ANALYTICS = auto()      # Requ√™te analytique
    REAL_TIME = auto()      # Requ√™te temps r√©el

class IndexType(Enum):
    """Types d'index optimis√©s"""
    BTREE = auto()          # B-Tree standard
    HASH = auto()           # Hash pour √©galit√©
    GIN = auto()            # GIN pour JSON/arrays
    GIST = auto()           # GiST pour g√©om√©trie
    BRIN = auto()           # BRIN pour grandes tables
    BLOOM = auto()          # Bloom filter
    TIME_SERIES = auto()    # Optimis√© time series

# =============================================================================
# MOD√àLES DE DONN√âES
# =============================================================================

@dataclass
class DatabaseConfig:
    """Configuration avanc√©e de base de donn√©es"""
    db_type: DatabaseType
    connection_string: str
    
    # Pool de connexions
    pool_size: int = 20
    max_overflow: int = 50
    pool_timeout: int = 30
    pool_recycle: int = 3600
    
    # Optimisations
    query_cache_size: int = 1000
    connection_cache: bool = True
    prepared_statements: bool = True
    
    # S√©curit√©
    ssl_enabled: bool = True
    encryption_at_rest: bool = True
    audit_enabled: bool = True
    
    # Performance
    read_preference: str = "primary"
    write_concern: str = "majority"
    batch_size: int = 1000
    compression: str = "lz4"
    
    # Monitoring
    metrics_enabled: bool = True
    slow_query_threshold: float = 1.0  # secondes
    
    # Sp√©cifique par type
    extra_config: Dict[str, Any] = field(default_factory=dict)

@dataclass
class StorageMetrics:
    """M√©triques de performance de stockage"""
    database_type: DatabaseType
    
    # M√©triques de performance
    query_count: int = 0
    avg_query_time_ms: float = 0.0
    slow_queries: int = 0
    errors: int = 0
    
    # M√©triques de ressources
    connections_active: int = 0
    connections_idle: int = 0
    memory_usage_mb: float = 0.0
    storage_size_gb: float = 0.0
    
    # M√©triques de cache
    cache_hits: int = 0
    cache_misses: int = 0
    cache_size_mb: float = 0.0
    
    # Timestamps
    last_updated: datetime = field(default_factory=datetime.utcnow)
    
    @property
    def cache_hit_ratio(self) -> float:
        """Ratio de hit du cache"""
        total = self.cache_hits + self.cache_misses
        return (self.cache_hits / total * 100) if total > 0 else 0.0
    
    @property
    def error_rate(self) -> float:
        """Taux d'erreur"""
        total = self.query_count + self.errors
        return (self.errors / total * 100) if total > 0 else 0.0

@dataclass
class QueryPlan:
    """Plan d'ex√©cution de requ√™te optimis√©"""
    query_id: str
    query_type: QueryType
    target_databases: List[DatabaseType]
    
    # Optimisations
    use_cache: bool = True
    cache_ttl: int = 300
    parallel_execution: bool = False
    preferred_replica: Optional[str] = None
    
    # Estimation de co√ªt
    estimated_cost: float = 0.0
    estimated_rows: int = 0
    estimated_time_ms: float = 0.0
    
    # M√©triques d'ex√©cution
    actual_time_ms: Optional[float] = None
    actual_rows: Optional[int] = None
    cache_used: bool = False

# =============================================================================
# MOTEURS DE STOCKAGE SP√âCIALIS√âS
# =============================================================================

class PostgreSQLEngine:
    """
    üêò POSTGRESQL ENGINE ULTRA-OPTIMIS√â
    
    Moteur PostgreSQL enterprise avec optimisations avanc√©es :
    - Extensions time series (TimescaleDB-style)
    - Partitioning automatique
    - Index intelligents
    - R√©plication ma√Ætre-esclave
    - Cache de requ√™tes avanc√©
    """
    
    def __init__(self, config: DatabaseConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.PostgreSQLEngine")
        
        # √âtat et connexions
        self._engine = None
        self._session_factory = None
        self._connection_pool = None
        self._metrics = StorageMetrics(DatabaseType.POSTGRESQL)
        
        # Cache et optimisations
        self._query_cache = {}
        self._prepared_statements = {}
        self._connection_cache = weakref.WeakValueDictionary()
        
        # Monitoring
        if MONITORING_AVAILABLE:
            self._query_counter = Counter('postgresql_queries_total', 'Total PostgreSQL queries')
            self._query_duration = Histogram('postgresql_query_duration_seconds', 'PostgreSQL query duration')
    
    async def initialize(self) -> bool:
        """Initialisation du moteur PostgreSQL"""
        try:
            self.logger.info("üêò Initialisation PostgreSQL Engine Ultra-Optimis√©...")
            
            if not POSTGRESQL_AVAILABLE:
                self.logger.error("‚ùå PostgreSQL libs non disponibles")
                return False
            
            # Cr√©ation du moteur avec optimisations
            self._engine = create_async_engine(
                self.config.connection_string,
                pool_size=self.config.pool_size,
                max_overflow=self.config.max_overflow,
                pool_timeout=self.config.pool_timeout,
                pool_recycle=self.config.pool_recycle,
                echo=False,  # Pas de logging SQL verbose en prod
                future=True,
                connect_args={
                    "server_settings": {
                        "jit": "off",  # Optimisation pour petites requ√™tes
                        "application_name": "DataLayer_PostgreSQL_Engine"
                    }
                }
            )
            
            # Factory de sessions
            self._session_factory = sessionmaker(
                bind=self._engine,
                class_=AsyncSession,
                expire_on_commit=False
            )
            
            # Test de connexion
            async with self._session_factory() as session:
                result = await session.execute("SELECT version()")
                version = result.scalar()
                self.logger.info(f"‚úÖ PostgreSQL connect√©: {version}")
            
            # Initialisation des tables et index
            await self._initialize_schemas()
            
            # Pr√©paration des requ√™tes fr√©quentes
            await self._prepare_statements()
            
            self.logger.info("‚úÖ PostgreSQL Engine initialis√© avec succ√®s")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation PostgreSQL: {e}")
            return False
    
    async def _initialize_schemas(self):
        """Initialisation des sch√©mas optimis√©s"""
        self.logger.info("üìä Initialisation sch√©mas PostgreSQL...")
        
        # Sch√©ma pour m√©triques time series
        create_metrics_table = """
        CREATE TABLE IF NOT EXISTS metrics_timeseries (
            id BIGSERIAL PRIMARY KEY,
            metric_name VARCHAR(255) NOT NULL,
            timestamp TIMESTAMPTZ NOT NULL,
            value DOUBLE PRECISION,
            labels JSONB,
            metadata JSONB,
            quality_score REAL DEFAULT 1.0,
            created_at TIMESTAMPTZ DEFAULT NOW()
        );
        
        -- Partitioning par temps pour performance
        CREATE TABLE IF NOT EXISTS metrics_timeseries_daily ()
        INHERITS (metrics_timeseries);
        
        -- Index optimis√©s pour time series
        CREATE INDEX IF NOT EXISTS idx_metrics_name_time 
        ON metrics_timeseries (metric_name, timestamp DESC);
        
        CREATE INDEX IF NOT EXISTS idx_metrics_labels_gin 
        ON metrics_timeseries USING GIN (labels);
        
        CREATE INDEX IF NOT EXISTS idx_metrics_timestamp_brin 
        ON metrics_timeseries USING BRIN (timestamp);
        
        -- Compression automatique pour anciennes donn√©es
        CREATE TABLE IF NOT EXISTS metrics_timeseries_compressed (
            LIKE metrics_timeseries,
            compressed_data BYTEA
        );
        """
        
        async with self._session_factory() as session:
            await session.execute(create_metrics_table)
            await session.commit()
        
        self.logger.info("‚úÖ Sch√©mas PostgreSQL initialis√©s")
    
    async def _prepare_statements(self):
        """Pr√©paration des requ√™tes fr√©quentes pour optimisation"""
        self.logger.info("üîß Pr√©paration des requ√™tes optimis√©es...")
        
        # Requ√™tes pr√©par√©es pour insertion ultra-rapide
        self._prepared_statements = {
            "insert_metric": """
                INSERT INTO metrics_timeseries 
                (metric_name, timestamp, value, labels, metadata, quality_score)
                VALUES ($1, $2, $3, $4, $5, $6)
            """,
            "select_range": """
                SELECT * FROM metrics_timeseries 
                WHERE metric_name = $1 AND timestamp BETWEEN $2 AND $3
                ORDER BY timestamp DESC
            """,
            "aggregate_hourly": """
                SELECT 
                    date_trunc('hour', timestamp) as hour,
                    AVG(value) as avg_value,
                    MIN(value) as min_value,
                    MAX(value) as max_value,
                    COUNT(*) as count
                FROM metrics_timeseries 
                WHERE metric_name = $1 AND timestamp BETWEEN $2 AND $3
                GROUP BY hour
                ORDER BY hour
            """
        }
        
        self.logger.info("‚úÖ Requ√™tes pr√©par√©es")
    
    async def store_metrics(self, metrics: List[Dict[str, Any]]) -> bool:
        """Stockage optimis√© par lot de m√©triques"""
        if not metrics:
            return True
        
        start_time = time.time()
        
        try:
            async with self._session_factory() as session:
                # Insertion par lot optimis√©e
                values = []
                for metric in metrics:
                    values.append((
                        metric['metric_name'],
                        metric['timestamp'],
                        metric['value'],
                        json.dumps(metric.get('labels', {})),
                        json.dumps(metric.get('metadata', {})),
                        metric.get('quality_score', 1.0)
                    ))
                
                # Utilisation de COPY pour insertion ultra-rapide
                await session.execute(
                    self._prepared_statements["insert_metric"],
                    values
                )
                await session.commit()
                
                # Mise √† jour m√©triques
                query_time = (time.time() - start_time) * 1000
                self._metrics.query_count += len(metrics)
                self._metrics.avg_query_time_ms = (
                    (self._metrics.avg_query_time_ms * (self._metrics.query_count - len(metrics)) + query_time) /
                    self._metrics.query_count
                )
                
                if MONITORING_AVAILABLE:
                    self._query_counter.inc(len(metrics))
                    self._query_duration.observe(query_time / 1000)
                
                return True
                
        except Exception as e:
            self.logger.error(f"‚ùå Erreur stockage PostgreSQL: {e}")
            self._metrics.errors += 1
            return False
    
    async def query_metrics(
        self,
        metric_name: str,
        start_time: datetime,
        end_time: datetime,
        labels: Optional[Dict[str, str]] = None,
        aggregation: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """Requ√™te optimis√©e de m√©triques"""
        query_start = time.time()
        
        try:
            # V√©rification cache
            cache_key = f"{metric_name}_{start_time}_{end_time}_{aggregation}"
            if cache_key in self._query_cache:
                self._metrics.cache_hits += 1
                return self._query_cache[cache_key]
            
            async with self._session_factory() as session:
                if aggregation == "hourly":
                    result = await session.execute(
                        self._prepared_statements["aggregate_hourly"],
                        (metric_name, start_time, end_time)
                    )
                else:
                    result = await session.execute(
                        self._prepared_statements["select_range"],
                        (metric_name, start_time, end_time)
                    )
                
                rows = result.fetchall()
                
                # Conversion en dictionnaires
                data = []
                for row in rows:
                    data.append({
                        "timestamp": row.timestamp,
                        "value": row.value,
                        "labels": json.loads(row.labels) if row.labels else {},
                        "metadata": json.loads(row.metadata) if row.metadata else {}
                    })
                
                # Mise en cache
                if len(data) < 10000:  # Cache seulement si pas trop volumineux
                    self._query_cache[cache_key] = data
                
                # M√©triques
                query_time = (time.time() - query_start) * 1000
                self._metrics.query_count += 1
                self._metrics.cache_misses += 1
                
                if MONITORING_AVAILABLE:
                    self._query_counter.inc()
                    self._query_duration.observe(query_time / 1000)
                
                return data
                
        except Exception as e:
            self.logger.error(f"‚ùå Erreur requ√™te PostgreSQL: {e}")
            self._metrics.errors += 1
            return []
    
    async def optimize_performance(self):
        """Optimisation automatique des performances"""
        self.logger.info("üîß Optimisation performances PostgreSQL...")
        
        async with self._session_factory() as session:
            # Analyse et reconstruction des index si n√©cessaire
            await session.execute("ANALYZE metrics_timeseries")
            
            # Nettoyage du cache de requ√™tes
            if len(self._query_cache) > 1000:
                self._query_cache.clear()
            
            # Vacuum automatique pour les tables volumineuses
            await session.execute("VACUUM (ANALYZE) metrics_timeseries")
        
        self.logger.info("‚úÖ Optimisation PostgreSQL termin√©e")
    
    async def get_metrics(self) -> StorageMetrics:
        """R√©cup√©ration des m√©triques de performance"""
        # Mise √† jour des m√©triques de connexion
        if self._engine:
            pool = self._engine.pool
            self._metrics.connections_active = pool.checkedout()
            self._metrics.connections_idle = pool.checkedin()
        
        self._metrics.last_updated = datetime.utcnow()
        return self._metrics
    
    async def shutdown(self):
        """Arr√™t propre du moteur"""
        self.logger.info("üîÑ Arr√™t PostgreSQL Engine...")
        
        if self._engine:
            await self._engine.dispose()
        
        self._query_cache.clear()
        self._prepared_statements.clear()
        
        self.logger.info("‚úÖ PostgreSQL Engine arr√™t√©")

class RedisEngine:
    """
    üöÄ REDIS ENGINE ULTRA-PERFORMANT
    
    Moteur Redis enterprise avec fonctionnalit√©s avanc√©es :
    - Cluster Redis pour haute disponibilit√©
    - Streams pour √©v√©nements temps r√©el
    - Cache intelligent avec TTL adaptatif
    - Pub/Sub pour notifications
    - Lua Scripts pour op√©rations atomiques
    """
    
    def __init__(self, config: DatabaseConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.RedisEngine")
        
        # Connexions Redis
        self._redis = None
        self._pubsub = None
        self._metrics = StorageMetrics(DatabaseType.REDIS)
        
        # Scripts Lua pr√©compil√©s
        self._lua_scripts = {}
        
        # Cache adaptatif
        self._cache_stats = defaultdict(int)
        self._ttl_optimizer = {}
    
    async def initialize(self) -> bool:
        """Initialisation du moteur Redis"""
        try:
            self.logger.info("üöÄ Initialisation Redis Engine Ultra-Performant...")
            
            if not REDIS_AVAILABLE:
                self.logger.error("‚ùå Redis libs non disponibles")
                return False
            
            # Configuration du pool de connexions optimis√©
            pool = ConnectionPool.from_url(
                self.config.connection_string,
                max_connections=self.config.pool_size,
                retry_on_timeout=True,
                socket_keepalive=True,
                socket_keepalive_options={},
                health_check_interval=30
            )
            
            self._redis = Redis(connection_pool=pool, decode_responses=False)
            
            # Test de connexion
            pong = await self._redis.ping()
            if not pong:
                raise Exception("Redis ping failed")
            
            # Information serveur
            info = await self._redis.info()
            self.logger.info(f"‚úÖ Redis connect√©: {info['redis_version']}")
            
            # Chargement des scripts Lua optimis√©s
            await self._load_lua_scripts()
            
            # Configuration Pub/Sub
            self._pubsub = self._redis.pubsub()
            
            self.logger.info("‚úÖ Redis Engine initialis√© avec succ√®s")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation Redis: {e}")
            return False
    
    async def _load_lua_scripts(self):
        """Chargement des scripts Lua pour op√©rations atomiques"""
        self.logger.info("üìú Chargement scripts Lua optimis√©s...")
        
        # Script pour insertion avec m√©tadonn√©es
        store_metric_script = """
        local key = KEYS[1]
        local timestamp = ARGV[1]
        local value = ARGV[2]
        local ttl = ARGV[3]
        local metadata = ARGV[4]
        
        -- Stockage de la m√©trique
        redis.call('HSET', key, timestamp, value)
        redis.call('HSET', key .. ':meta', timestamp, metadata)
        
        -- TTL adaptatif
        if ttl > 0 then
            redis.call('EXPIRE', key, ttl)
            redis.call('EXPIRE', key .. ':meta', ttl)
        end
        
        -- Nettoyage automatique des anciennes entr√©es
        local count = redis.call('HLEN', key)
        if count > 10000 then
            local fields = redis.call('HKEYS', key)
            table.sort(fields)
            for i = 1, math.floor(count * 0.1) do
                redis.call('HDEL', key, fields[i])
                redis.call('HDEL', key .. ':meta', fields[i])
            end
        end
        
        return count
        """
        
        # Script pour requ√™te optimis√©e avec cache
        query_range_script = """
        local key = KEYS[1]
        local start_ts = ARGV[1]
        local end_ts = ARGV[2]
        local cache_key = ARGV[3]
        local cache_ttl = ARGV[4]
        
        -- V√©rification cache
        local cached = redis.call('GET', cache_key)
        if cached then
            return cached
        end
        
        -- Requ√™te des donn√©es
        local all_data = redis.call('HGETALL', key)
        local result = {}
        
        for i = 1, #all_data, 2 do
            local ts = all_data[i]
            local value = all_data[i + 1]
            
            if ts >= start_ts and ts <= end_ts then
                table.insert(result, ts)
                table.insert(result, value)
            end
        end
        
        -- Mise en cache du r√©sultat
        if #result > 0 then
            local result_json = cjson.encode(result)
            redis.call('SETEX', cache_key, cache_ttl, result_json)
            return result_json
        end
        
        return '{}'
        """
        
        # Compilation des scripts
        self._lua_scripts = {
            'store_metric': self._redis.register_script(store_metric_script),
            'query_range': self._redis.register_script(query_range_script)
        }
        
        self.logger.info(f"‚úÖ {len(self._lua_scripts)} scripts Lua charg√©s")
    
    async def store_metrics(self, metrics: List[Dict[str, Any]]) -> bool:
        """Stockage ultra-rapide de m√©triques avec Lua"""
        if not metrics:
            return True
        
        start_time = time.time()
        
        try:
            # Pipeline pour performance maximale
            pipe = self._redis.pipeline()
            
            for metric in metrics:
                key = f"metrics:{metric['metric_name']}"
                timestamp = int(metric['timestamp'].timestamp() * 1000)
                
                # S√©rialisation optimis√©e
                if OPTIMIZATION_LIBS_AVAILABLE:
                    value_data = msgpack.packb({
                        'value': metric['value'],
                        'labels': metric.get('labels', {}),
                        'quality_score': metric.get('quality_score', 1.0)
                    })
                else:
                    value_data = json.dumps({
                        'value': metric['value'],
                        'labels': metric.get('labels', {}),
                        'quality_score': metric.get('quality_score', 1.0)
                    }).encode()
                
                # TTL adaptatif bas√© sur la fr√©quence d'acc√®s
                ttl = self._calculate_adaptive_ttl(metric['metric_name'])
                
                # Ex√©cution script Lua
                await self._lua_scripts['store_metric'](
                    keys=[key],
                    args=[timestamp, value_data, ttl, json.dumps(metric.get('metadata', {}))]
                )
            
            # Ex√©cution du pipeline
            await pipe.execute()
            
            # M√©triques de performance
            query_time = (time.time() - start_time) * 1000
            self._metrics.query_count += len(metrics)
            self._metrics.avg_query_time_ms = (
                (self._metrics.avg_query_time_ms * (self._metrics.query_count - len(metrics)) + query_time) /
                self._metrics.query_count
            )
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur stockage Redis: {e}")
            self._metrics.errors += 1
            return False
    
    def _calculate_adaptive_ttl(self, metric_name: str) -> int:
        """Calcul TTL adaptatif bas√© sur les patterns d'acc√®s"""
        # Fr√©quence d'acc√®s
        access_freq = self._cache_stats.get(f"access:{metric_name}", 1)
        
        # TTL base selon la fr√©quence
        if access_freq > 100:
            return 3600  # 1 heure pour m√©triques tr√®s consult√©es
        elif access_freq > 10:
            return 1800  # 30 minutes pour m√©triques moyennement consult√©es
        else:
            return 900   # 15 minutes pour m√©triques peu consult√©es
    
    async def query_metrics(
        self,
        metric_name: str,
        start_time: datetime,
        end_time: datetime,
        use_cache: bool = True
    ) -> List[Dict[str, Any]]:
        """Requ√™te ultra-rapide avec cache intelligent"""
        query_start = time.time()
        
        try:
            # Cl√© de cache
            cache_key = f"query_cache:{metric_name}:{int(start_time.timestamp())}:{int(end_time.timestamp())}"
            
            # Mise √† jour statistiques d'acc√®s
            self._cache_stats[f"access:{metric_name}"] += 1
            
            if use_cache:
                # V√©rification cache direct
                cached_data = await self._redis.get(cache_key)
                if cached_data:
                    self._metrics.cache_hits += 1
                    if OPTIMIZATION_LIBS_AVAILABLE:
                        return msgpack.unpackb(cached_data)
                    else:
                        return json.loads(cached_data)
            
            # Requ√™te avec script Lua optimis√©
            start_ts = int(start_time.timestamp() * 1000)
            end_ts = int(end_time.timestamp() * 1000)
            
            result_json = await self._lua_scripts['query_range'](
                keys=[f"metrics:{metric_name}"],
                args=[start_ts, end_ts, cache_key, 300]  # Cache 5 minutes
            )
            
            if result_json == '{}':
                return []
            
            # D√©codage et conversion
            if OPTIMIZATION_LIBS_AVAILABLE:
                raw_data = msgpack.unpackb(result_json)
            else:
                raw_data = json.loads(result_json)
            
            # Conversion en format attendu
            metrics = []
            for i in range(0, len(raw_data), 2):
                timestamp_ms = int(raw_data[i])
                value_data = raw_data[i + 1]
                
                if OPTIMIZATION_LIBS_AVAILABLE:
                    parsed_data = msgpack.unpackb(value_data)
                else:
                    parsed_data = json.loads(value_data)
                
                metrics.append({
                    "timestamp": datetime.fromtimestamp(timestamp_ms / 1000),
                    "value": parsed_data['value'],
                    "labels": parsed_data.get('labels', {}),
                    "quality_score": parsed_data.get('quality_score', 1.0)
                })
            
            # M√©triques
            query_time = (time.time() - query_start) * 1000
            self._metrics.query_count += 1
            self._metrics.cache_misses += 1
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur requ√™te Redis: {e}")
            self._metrics.errors += 1
            return []
    
    async def publish_event(self, channel: str, data: Dict[str, Any]) -> bool:
        """Publication d'√©v√©nement Pub/Sub optimis√©e"""
        try:
            if OPTIMIZATION_LIBS_AVAILABLE:
                serialized_data = msgpack.packb(data)
            else:
                serialized_data = json.dumps(data).encode()
            
            await self._redis.publish(channel, serialized_data)
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur publication Redis: {e}")
            return False
    
    async def subscribe_events(self, channels: List[str]) -> AsyncGenerator[Dict[str, Any], None]:
        """Souscription aux √©v√©nements avec d√©codage automatique"""
        try:
            await self._pubsub.subscribe(*channels)
            
            async for message in self._pubsub.listen():
                if message['type'] == 'message':
                    try:
                        if OPTIMIZATION_LIBS_AVAILABLE:
                            data = msgpack.unpackb(message['data'])
                        else:
                            data = json.loads(message['data'].decode())
                        
                        yield {
                            'channel': message['channel'].decode(),
                            'data': data,
                            'timestamp': datetime.utcnow()
                        }
                    except Exception as e:
                        self.logger.error(f"Erreur d√©codage message: {e}")
                        
        except Exception as e:
            self.logger.error(f"‚ùå Erreur souscription Redis: {e}")
    
    async def get_metrics(self) -> StorageMetrics:
        """R√©cup√©ration des m√©triques Redis"""
        try:
            info = await self._redis.info()
            
            self._metrics.memory_usage_mb = info.get('used_memory', 0) / 1024 / 1024
            self._metrics.connections_active = info.get('connected_clients', 0)
            
            # Calcul du cache hit ratio
            total_access = sum(self._cache_stats.values())
            if total_access > 0:
                self._metrics.cache_hits = sum(
                    v for k, v in self._cache_stats.items() 
                    if k.startswith('hit:')
                )
                self._metrics.cache_misses = total_access - self._metrics.cache_hits
            
        except Exception as e:
            self.logger.error(f"Erreur r√©cup√©ration m√©triques Redis: {e}")
        
        self._metrics.last_updated = datetime.utcnow()
        return self._metrics
    
    async def shutdown(self):
        """Arr√™t propre du moteur Redis"""
        self.logger.info("üîÑ Arr√™t Redis Engine...")
        
        if self._pubsub:
            await self._pubsub.unsubscribe()
            await self._pubsub.close()
        
        if self._redis:
            await self._redis.close()
        
        self._lua_scripts.clear()
        self._cache_stats.clear()
        
        self.logger.info("‚úÖ Redis Engine arr√™t√©")

# =============================================================================
# GESTIONNAIRE MULTI-DATABASE ULTRA-AVANC√â
# =============================================================================

class MultiDatabaseManager:
    """
    üåê GESTIONNAIRE MULTI-DATABASE R√âVOLUTIONNAIRE
    
    Orchestrateur intelligent pour multiple bases de donn√©es :
    - Routage automatique selon le type de requ√™te
    - R√©plication et synchronisation
    - Load balancing adaptatif
    - Failover automatique
    - Optimisation cross-database
    """
    
    def __init__(self, configs: Optional[List[DatabaseConfig]] = None):
        self.configs = configs or []
        self.logger = logging.getLogger(f"{__name__}.MultiDatabaseManager")
        
        # Moteurs de base de donn√©es
        self._engines = {}
        self._primary_engine = None
        
        # Routage intelligent
        self._routing_rules = {}
        self._load_balancer = {}
        
        # M√©triques globales
        self._global_metrics = {
            "total_queries": 0,
            "avg_response_time": 0.0,
            "error_rate": 0.0,
            "databases_healthy": 0
        }
    
    async def initialize(self) -> bool:
        """Initialisation du gestionnaire multi-database"""
        try:
            self.logger.info("üåê Initialisation Multi-Database Manager...")
            
            # Initialisation des moteurs
            for config in self.configs:
                await self._initialize_engine(config)
            
            # Configuration du routage intelligent
            self._setup_routing_rules()
            
            # S√©lection du moteur primaire
            self._select_primary_engine()
            
            self.logger.info(f"‚úÖ {len(self._engines)} moteurs initialis√©s")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation multi-database: {e}")
            return False
    
    async def _initialize_engine(self, config: DatabaseConfig) -> bool:
        """Initialisation d'un moteur sp√©cifique"""
        try:
            if config.db_type == DatabaseType.POSTGRESQL:
                engine = PostgreSQLEngine(config)
            elif config.db_type == DatabaseType.REDIS:
                engine = RedisEngine(config)
            elif config.db_type == DatabaseType.CLICKHOUSE and CLICKHOUSE_AVAILABLE:
                engine = ClickHouseEngine(config)
            elif config.db_type == DatabaseType.MONGODB and MONGODB_AVAILABLE:
                engine = MongoDBEngine(config)
            else:
                self.logger.warning(f"Type de base non support√©: {config.db_type}")
                return False
            
            success = await engine.initialize()
            if success:
                self._engines[config.db_type] = engine
                self.logger.info(f"‚úÖ {config.db_type.name} initialis√©")
            
            return success
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation {config.db_type}: {e}")
            return False
    
    def _setup_routing_rules(self):
        """Configuration du routage intelligent"""
        self._routing_rules = {
            # M√©triques temps r√©el -> Redis
            QueryType.REAL_TIME: [DatabaseType.REDIS],
            
            # Analytics OLAP -> ClickHouse
            QueryType.ANALYTICS: [DatabaseType.CLICKHOUSE, DatabaseType.POSTGRESQL],
            
            # Agr√©gations -> ClickHouse puis PostgreSQL
            QueryType.AGGREGATION: [DatabaseType.CLICKHOUSE, DatabaseType.POSTGRESQL],
            
            # Requ√™tes de plage -> PostgreSQL puis ClickHouse
            QueryType.RANGE_QUERY: [DatabaseType.POSTGRESQL, DatabaseType.CLICKHOUSE],
            
            # Requ√™tes de point -> Redis puis PostgreSQL
            QueryType.POINT_QUERY: [DatabaseType.REDIS, DatabaseType.POSTGRESQL]
        }
    
    def _select_primary_engine(self):
        """S√©lection du moteur primaire pour √©criture"""
        # PostgreSQL comme primaire par d√©faut pour ACID
        if DatabaseType.POSTGRESQL in self._engines:
            self._primary_engine = self._engines[DatabaseType.POSTGRESQL]
        elif DatabaseType.CLICKHOUSE in self._engines:
            self._primary_engine = self._engines[DatabaseType.CLICKHOUSE]
        elif self._engines:
            self._primary_engine = list(self._engines.values())[0]
    
    async def route_query(
        self,
        query_type: QueryType,
        operation: str,
        *args,
        **kwargs
    ) -> Any:
        """Routage intelligent des requ√™tes"""
        # S√©lection des moteurs selon les r√®gles
        candidate_engines = self._routing_rules.get(query_type, [])
        
        # Filtrage des moteurs disponibles
        available_engines = [
            self._engines[db_type] for db_type in candidate_engines 
            if db_type in self._engines
        ]
        
        if not available_engines:
            # Fallback vers moteur primaire
            available_engines = [self._primary_engine] if self._primary_engine else []
        
        # Ex√©cution avec failover
        for engine in available_engines:
            try:
                method = getattr(engine, operation)
                result = await method(*args, **kwargs)
                
                # Mise √† jour m√©triques globales
                self._global_metrics["total_queries"] += 1
                
                return result
                
            except Exception as e:
                self.logger.warning(f"Erreur {engine.__class__.__name__}: {e}")
                continue
        
        raise Exception(f"Aucun moteur disponible pour {query_type}")
    
    async def store_metrics(self, metrics: List[Dict[str, Any]]) -> bool:
        """Stockage distribu√© avec r√©plication"""
        results = []
        
        # Stockage primaire
        if self._primary_engine:
            result = await self._primary_engine.store_metrics(metrics)
            results.append(result)
        
        # R√©plication vers Redis pour cache
        if DatabaseType.REDIS in self._engines and self._primary_engine != self._engines[DatabaseType.REDIS]:
            try:
                await self._engines[DatabaseType.REDIS].store_metrics(metrics)
            except Exception as e:
                self.logger.warning(f"Erreur r√©plication Redis: {e}")
        
        return any(results)
    
    async def query_metrics(
        self,
        metric_name: str,
        start_time: datetime,
        end_time: datetime,
        query_type: QueryType = QueryType.RANGE_QUERY,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """Requ√™te intelligente avec routage optimis√©"""
        return await self.route_query(
            query_type,
            "query_metrics",
            metric_name,
            start_time,
            end_time,
            **kwargs
        )
    
    async def get_system_health(self) -> Dict[str, Any]:
        """√âtat de sant√© du syst√®me multi-database"""
        health_status = {}
        healthy_count = 0
        
        for db_type, engine in self._engines.items():
            try:
                metrics = await engine.get_metrics()
                health_status[db_type.name] = {
                    "status": "healthy",
                    "query_count": metrics.query_count,
                    "avg_query_time_ms": metrics.avg_query_time_ms,
                    "error_rate": metrics.error_rate,
                    "cache_hit_ratio": metrics.cache_hit_ratio if hasattr(metrics, 'cache_hit_ratio') else 0.0
                }
                healthy_count += 1
            except Exception as e:
                health_status[db_type.name] = {
                    "status": "error",
                    "error": str(e)
                }
        
        self._global_metrics["databases_healthy"] = healthy_count
        
        return {
            "global_metrics": self._global_metrics,
            "databases": health_status,
            "primary_engine": self._primary_engine.__class__.__name__ if self._primary_engine else None,
            "routing_rules": {k.name: [v.name for v in vals] for k, vals in self._routing_rules.items()},
            "timestamp": datetime.utcnow().isoformat()
        }
    
    async def shutdown(self):
        """Arr√™t propre de tous les moteurs"""
        self.logger.info("üîÑ Arr√™t Multi-Database Manager...")
        
        for db_type, engine in self._engines.items():
            try:
                await engine.shutdown()
                self.logger.info(f"‚úÖ {db_type.name} arr√™t√©")
            except Exception as e:
                self.logger.error(f"‚ùå Erreur arr√™t {db_type.name}: {e}")
        
        self._engines.clear()
        self.logger.info("‚úÖ Multi-Database Manager arr√™t√©")

# =============================================================================
# UTILITAIRES ET EXPORTS
# =============================================================================

async def create_storage_manager(configs: List[DatabaseConfig]) -> MultiDatabaseManager:
    """Cr√©ation et initialisation du gestionnaire de stockage"""
    manager = MultiDatabaseManager(configs)
    await manager.initialize()
    return manager

__all__ = [
    # Classes principales
    "PostgreSQLEngine",
    "RedisEngine", 
    "MultiDatabaseManager",
    
    # Mod√®les
    "DatabaseConfig",
    "StorageMetrics",
    "QueryPlan",
    
    # Enums
    "DatabaseType",
    "StorageStrategy",
    "QueryType",
    "IndexType",
    
    # Utilitaires
    "create_storage_manager"
]
