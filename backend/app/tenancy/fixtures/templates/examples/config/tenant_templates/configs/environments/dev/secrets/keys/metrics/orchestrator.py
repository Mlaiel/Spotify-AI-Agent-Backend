#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enterprise Metrics System - Master Orchestrator
==============================================

Ultra-advanced master orchestrator for the complete enterprise metrics ecosystem.
This script provides a unified interface to deploy, test, benchmark, monitor,
and validate the entire metrics infrastructure with comprehensive automation.

Expert Development Team:
- Lead Dev + AI Architect
- Senior Backend Developer (Python/FastAPI/Django)
- ML Engineer (TensorFlow/PyTorch/Hugging Face)
- DBA & Data Engineer (PostgreSQL/Redis/MongoDB)
- Backend Security Specialist
- Microservices Architect

Project Lead: Fahed Mlaiel
"""

import asyncio
import json
import logging
import os
import sys
import time
import argparse
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
import tempfile
import shutil

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

# Ajout du chemin parent pour les imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from . import (
        EnterpriseMetricsSystem, MetricDataPoint, MetricType, 
        MetricCategory, MetricSeverity, get_metrics_system
    )
    from .collector import MetricsCollectionAgent, CollectorConfig
    from .monitor import AlertEngine, AlertRule, AlertPriority, HealthMonitor
    from .deploy import DeploymentOrchestrator, DeploymentConfig
    from .test_suite import MetricsSystemTestSuite
    from .benchmark import PerformanceBenchmark
    from .compliance import ComplianceValidator, ComplianceStandard
except ImportError as e:
    logger.error(f"Erreur d'import: {e}")
    logger.info("Tentative d'import relatif...")
    try:
        import __init__ as metrics_module
        from test_suite import MetricsSystemTestSuite
        from benchmark import PerformanceBenchmark
        from compliance import ComplianceValidator, ComplianceStandard
        from collector import MetricsCollectionAgent, CollectorConfig
        from monitor import AlertEngine
        from deploy import DeploymentOrchestrator, DeploymentConfig
    except ImportError as e2:
        logger.error(f"Erreur d'import relatif: {e2}")
        sys.exit(1)


class OrchestrationMode:
    """Modes d'orchestration disponibles."""
    FULL = "full"
    DEPLOY = "deploy"
    TEST = "test"
    BENCHMARK = "benchmark"
    MONITOR = "monitor"
    COMPLIANCE = "compliance"
    DEMO = "demo"
    INTERACTIVE = "interactive"


class MasterOrchestrator:
    """Orchestrateur principal du syst√®me de m√©triques d'entreprise."""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.temp_dir = None
        self.metrics_system = None
        self.start_time = None
        
        # Configuration par d√©faut
        self.default_config = {
            "storage_backend": "sqlite",
            "storage_config": {},
            "enable_collector": True,
            "enable_monitoring": True,
            "enable_analytics": True,
            "deployment_mode": "development",
            "auto_cleanup": True,
            "report_format": "both",  # json, markdown, both
            "verbose": False
        }
        
        # Fusion avec la configuration par d√©faut
        for key, value in self.default_config.items():
            if key not in self.config:
                self.config[key] = value
    
    async def run_orchestration(self, mode: str = OrchestrationMode.FULL, **kwargs) -> Dict[str, Any]:
        """Lance l'orchestration selon le mode sp√©cifi√©."""
        logger.info(f"üöÄ D√©marrage de l'orchestration en mode: {mode.upper()}")
        self.start_time = time.time()
        
        try:
            # Pr√©paration de l'environnement
            await self._setup_environment()
            
            # Ex√©cution selon le mode
            if mode == OrchestrationMode.FULL:
                results = await self._run_full_orchestration(**kwargs)
            elif mode == OrchestrationMode.DEPLOY:
                results = await self._run_deployment_only(**kwargs)
            elif mode == OrchestrationMode.TEST:
                results = await self._run_testing_only(**kwargs)
            elif mode == OrchestrationMode.BENCHMARK:
                results = await self._run_benchmark_only(**kwargs)
            elif mode == OrchestrationMode.MONITOR:
                results = await self._run_monitoring_only(**kwargs)
            elif mode == OrchestrationMode.COMPLIANCE:
                results = await self._run_compliance_only(**kwargs)
            elif mode == OrchestrationMode.DEMO:
                results = await self._run_demo_mode(**kwargs)
            elif mode == OrchestrationMode.INTERACTIVE:
                results = await self._run_interactive_mode(**kwargs)
            else:
                raise ValueError(f"Mode d'orchestration non support√©: {mode}")
            
            # Rapport final
            total_time = time.time() - self.start_time
            results["orchestration_summary"] = {
                "mode": mode,
                "total_execution_time": total_time,
                "success": True,
                "timestamp": datetime.now().isoformat()
            }
            
            # Sauvegarde des r√©sultats
            await self._save_orchestration_results(results, mode)
            
            logger.info(f"‚úÖ Orchestration termin√©e avec succ√®s en {total_time:.1f}s")
            
            return results
            
        except Exception as e:
            logger.error(f"üí• Erreur dans l'orchestration: {e}")
            import traceback
            traceback.print_exc()
            raise
        finally:
            # Nettoyage
            if self.config.get("auto_cleanup", True):
                await self._cleanup_environment()
    
    async def _setup_environment(self):
        """Configure l'environnement d'orchestration."""
        logger.info("üîß Configuration de l'environnement")
        
        # Cr√©ation du r√©pertoire temporaire
        self.temp_dir = tempfile.mkdtemp(prefix="metrics_orchestration_")
        logger.info(f"üìÅ R√©pertoire temporaire: {self.temp_dir}")
        
        # Configuration du stockage
        storage_config = self.config.get("storage_config", {})
        if self.config["storage_backend"] == "sqlite":
            storage_config["db_path"] = f"{self.temp_dir}/metrics.db"
        
        # Initialisation du syst√®me de m√©triques
        try:
            from . import get_metrics_system
            self.metrics_system = get_metrics_system(
                self.config["storage_backend"], 
                storage_config
            )
        except ImportError:
            # Fallback pour les imports relatifs
            import __init__ as metrics_module
            self.metrics_system = metrics_module.get_metrics_system(
                self.config["storage_backend"], 
                storage_config
            )
        
        await self.metrics_system.start()
        logger.info(f"‚úÖ Syst√®me de m√©triques initialis√© ({self.config['storage_backend']})")
    
    async def _cleanup_environment(self):
        """Nettoie l'environnement d'orchestration."""
        logger.info("üßπ Nettoyage de l'environnement")
        
        if self.metrics_system:
            try:
                await self.metrics_system.stop()
            except Exception as e:
                logger.warning(f"Erreur lors de l'arr√™t du syst√®me: {e}")
        
        if self.temp_dir and os.path.exists(self.temp_dir):
            try:
                shutil.rmtree(self.temp_dir)
                logger.info("‚úÖ R√©pertoire temporaire nettoy√©")
            except Exception as e:
                logger.warning(f"Erreur lors du nettoyage: {e}")
    
    async def _run_full_orchestration(self, **kwargs) -> Dict[str, Any]:
        """Ex√©cute l'orchestration compl√®te."""
        logger.info("üîÑ Orchestration compl√®te")
        
        results = {}
        
        # Phase 1: D√©ploiement
        logger.info("üì¶ Phase 1: D√©ploiement")
        try:
            deployment_results = await self._run_deployment()
            results["deployment"] = deployment_results
            logger.info("‚úÖ D√©ploiement termin√© avec succ√®s")
        except Exception as e:
            logger.error(f"‚ùå Erreur de d√©ploiement: {e}")
            results["deployment"] = {"error": str(e), "success": False}
        
        # Phase 2: Tests
        logger.info("üß™ Phase 2: Tests")
        try:
            test_results = await self._run_testing()
            results["testing"] = test_results
            logger.info("‚úÖ Tests termin√©s avec succ√®s")
        except Exception as e:
            logger.error(f"‚ùå Erreur de tests: {e}")
            results["testing"] = {"error": str(e), "success": False}
        
        # Phase 3: Benchmarks
        logger.info("‚ö° Phase 3: Benchmarks de performance")
        try:
            benchmark_results = await self._run_benchmark()
            results["benchmark"] = benchmark_results
            logger.info("‚úÖ Benchmarks termin√©s avec succ√®s")
        except Exception as e:
            logger.error(f"‚ùå Erreur de benchmark: {e}")
            results["benchmark"] = {"error": str(e), "success": False}
        
        # Phase 4: Validation de compliance
        logger.info("üîç Phase 4: Validation de compliance")
        try:
            compliance_results = await self._run_compliance()
            results["compliance"] = compliance_results
            logger.info("‚úÖ Validation de compliance termin√©e")
        except Exception as e:
            logger.error(f"‚ùå Erreur de compliance: {e}")
            results["compliance"] = {"error": str(e), "success": False}
        
        # Phase 5: Monitoring (optionnel - d√©marrage en arri√®re-plan)
        if self.config.get("enable_monitoring", False):
            logger.info("üìä Phase 5: D√©marrage du monitoring")
            try:
                monitoring_results = await self._start_monitoring()
                results["monitoring"] = monitoring_results
                logger.info("‚úÖ Monitoring d√©marr√©")
            except Exception as e:
                logger.error(f"‚ùå Erreur de monitoring: {e}")
                results["monitoring"] = {"error": str(e), "success": False}
        
        return results
    
    async def _run_deployment_only(self, **kwargs) -> Dict[str, Any]:
        """Ex√©cute uniquement le d√©ploiement."""
        logger.info("üì¶ Mode d√©ploiement uniquement")
        
        deployment_results = await self._run_deployment()
        
        return {
            "deployment": deployment_results,
            "mode": "deploy_only"
        }
    
    async def _run_testing_only(self, **kwargs) -> Dict[str, Any]:
        """Ex√©cute uniquement les tests."""
        logger.info("üß™ Mode tests uniquement")
        
        test_results = await self._run_testing()
        
        return {
            "testing": test_results,
            "mode": "test_only"
        }
    
    async def _run_benchmark_only(self, **kwargs) -> Dict[str, Any]:
        """Ex√©cute uniquement les benchmarks."""
        logger.info("‚ö° Mode benchmark uniquement")
        
        benchmark_results = await self._run_benchmark()
        
        return {
            "benchmark": benchmark_results,
            "mode": "benchmark_only"
        }
    
    async def _run_monitoring_only(self, duration: int = 60, **kwargs) -> Dict[str, Any]:
        """Ex√©cute uniquement le monitoring."""
        logger.info(f"üìä Mode monitoring uniquement ({duration}s)")
        
        monitoring_results = await self._run_monitoring_session(duration)
        
        return {
            "monitoring": monitoring_results,
            "mode": "monitor_only"
        }
    
    async def _run_compliance_only(self, **kwargs) -> Dict[str, Any]:
        """Ex√©cute uniquement la validation de compliance."""
        logger.info("üîç Mode compliance uniquement")
        
        compliance_results = await self._run_compliance()
        
        return {
            "compliance": compliance_results,
            "mode": "compliance_only"
        }
    
    async def _run_demo_mode(self, **kwargs) -> Dict[str, Any]:
        """Ex√©cute une d√©monstration compl√®te."""
        logger.info("üé≠ Mode d√©monstration")
        
        results = {}
        
        # D√©monstration rapide de chaque composant
        results["demo_deployment"] = await self._demo_deployment()
        results["demo_metrics"] = await self._demo_metrics_collection()
        results["demo_queries"] = await self._demo_queries()
        results["demo_alerts"] = await self._demo_alerts()
        results["demo_analytics"] = await self._demo_analytics()
        
        return {
            "demo": results,
            "mode": "demo"
        }
    
    async def _run_interactive_mode(self, **kwargs) -> Dict[str, Any]:
        """Ex√©cute en mode interactif."""
        logger.info("üîÑ Mode interactif")
        
        results = {"interactions": []}
        
        print("\n" + "=" * 60)
        print("üéØ MODE INTERACTIF - Syst√®me de M√©triques d'Entreprise")
        print("Expert Development Team - Dirig√© par Fahed Mlaiel")
        print("=" * 60)
        
        while True:
            print("\nüìã Options disponibles:")
            print("1. üì¶ D√©ployer le syst√®me")
            print("2. üß™ Ex√©cuter les tests")
            print("3. ‚ö° Benchmark de performance")
            print("4. üîç Validation de compliance")
            print("5. üìä Session de monitoring")
            print("6. üé≠ D√©monstration")
            print("7. üìà Injecter des m√©triques de test")
            print("8. üîç Requ√™ter les m√©triques")
            print("9. üìä Afficher les statistiques")
            print("0. üö™ Quitter")
            
            try:
                choice = input("\nüëâ Votre choix (0-9): ").strip()
                
                if choice == "0":
                    print("üëã Au revoir!")
                    break
                elif choice == "1":
                    result = await self._run_deployment()
                    results["interactions"].append({"action": "deployment", "result": result})
                    print("‚úÖ D√©ploiement termin√©!")
                elif choice == "2":
                    result = await self._run_testing()
                    results["interactions"].append({"action": "testing", "result": result})
                    print("‚úÖ Tests termin√©s!")
                elif choice == "3":
                    result = await self._run_benchmark()
                    results["interactions"].append({"action": "benchmark", "result": result})
                    print("‚úÖ Benchmark termin√©!")
                elif choice == "4":
                    result = await self._run_compliance()
                    results["interactions"].append({"action": "compliance", "result": result})
                    print("‚úÖ Validation termin√©e!")
                elif choice == "5":
                    duration = input("Dur√©e du monitoring (secondes, d√©faut 30): ").strip()
                    duration = int(duration) if duration.isdigit() else 30
                    result = await self._run_monitoring_session(duration)
                    results["interactions"].append({"action": "monitoring", "result": result})
                    print("‚úÖ Monitoring termin√©!")
                elif choice == "6":
                    result = await self._run_demo_mode()
                    results["interactions"].append({"action": "demo", "result": result})
                    print("‚úÖ D√©monstration termin√©e!")
                elif choice == "7":
                    count = input("Nombre de m√©triques √† injecter (d√©faut 100): ").strip()
                    count = int(count) if count.isdigit() else 100
                    result = await self._inject_test_metrics(count)
                    results["interactions"].append({"action": "inject_metrics", "result": result})
                    print(f"‚úÖ {count} m√©triques inject√©es!")
                elif choice == "8":
                    pattern = input("Pattern de recherche (d√©faut '*'): ").strip()
                    pattern = pattern if pattern else "*"
                    result = await self._query_metrics_interactive(pattern)
                    results["interactions"].append({"action": "query_metrics", "result": result})
                    print(f"‚úÖ Requ√™te termin√©e! {len(result.get('metrics', []))} m√©triques trouv√©es")
                elif choice == "9":
                    result = await self._show_system_stats()
                    results["interactions"].append({"action": "show_stats", "result": result})
                    print("‚úÖ Statistiques affich√©es!")
                else:
                    print("‚ùå Choix invalide, veuillez r√©essayer.")
                    
            except KeyboardInterrupt:
                print("\nüëã Interruption utilisateur, arr√™t...")
                break
            except Exception as e:
                print(f"‚ùå Erreur: {e}")
                logger.error(f"Erreur interactive: {e}")
        
        return {
            "interactive": results,
            "mode": "interactive"
        }
    
    async def _run_deployment(self) -> Dict[str, Any]:
        """Ex√©cute le d√©ploiement."""
        config = DeploymentConfig(
            deployment_name="orchestration_deployment",
            mode=self.config.get("deployment_mode", "development")
        )
        
        orchestrator = DeploymentOrchestrator(config)
        return await orchestrator.deploy_complete_system()
    
    async def _run_testing(self) -> Dict[str, Any]:
        """Ex√©cute la suite de tests."""
        test_suite = MetricsSystemTestSuite()
        return await test_suite.run_all_tests()
    
    async def _run_benchmark(self) -> Dict[str, Any]:
        """Ex√©cute les benchmarks de performance."""
        benchmark_config = {
            "warm_up_iterations": 3,
            "benchmark_iterations": 10,
            "concurrent_users": [1, 2, 5],
            "data_sizes": [100, 500, 1000]
        }
        
        benchmark = PerformanceBenchmark(benchmark_config)
        profile = await benchmark.run_comprehensive_benchmark()
        
        # Conversion en dictionnaire pour s√©rialisation
        return {
            "profile_name": profile.profile_name,
            "overall_score": profile.overall_score,
            "benchmark_count": len(profile.benchmark_results),
            "recommendations": profile.recommendations,
            "timestamp": profile.timestamp.isoformat()
        }
    
    async def _run_compliance(self) -> Dict[str, Any]:
        """Ex√©cute la validation de compliance."""
        compliance_config = {
            "standards": [
                ComplianceStandard.GDPR,
                ComplianceStandard.ISO_27001,
                ComplianceStandard.SOC2
            ]
        }
        
        validator = ComplianceValidator(compliance_config)
        report = await validator.run_comprehensive_validation(self.metrics_system)
        
        # Conversion en dictionnaire pour s√©rialisation
        return {
            "report_id": report.report_id,
            "compliance_score": report.compliance_score,
            "overall_status": report.overall_status.value,
            "vulnerabilities_count": len(report.security_vulnerabilities),
            "checks_count": len(report.compliance_checks),
            "recommendations": report.recommendations,
            "generated_at": report.generated_at.isoformat()
        }
    
    async def _start_monitoring(self) -> Dict[str, Any]:
        """D√©marre le syst√®me de monitoring."""
        # Configuration basique du monitoring
        alert_engine = AlertEngine(self.metrics_system)
        await alert_engine.start()
        
        # Note: Dans un vrai d√©ploiement, le monitoring continuerait
        # Ici on simule juste le d√©marrage
        await asyncio.sleep(1)
        await alert_engine.stop()
        
        return {
            "monitoring_started": True,
            "timestamp": datetime.now().isoformat()
        }
    
    async def _run_monitoring_session(self, duration: int) -> Dict[str, Any]:
        """Ex√©cute une session de monitoring."""
        logger.info(f"üìä Session de monitoring ({duration}s)")
        
        # D√©marrage des collecteurs
        collector_config = CollectorConfig(
            system_interval=5,
            security_interval=30,
            application_interval=10
        )
        
        collector = MetricsCollectionAgent(collector_config, self.metrics_system)
        await collector.start()
        
        # D√©marrage du moteur d'alertes
        alert_engine = AlertEngine(self.metrics_system)
        await alert_engine.start()
        
        start_time = time.time()
        collected_metrics = 0
        
        # Monitoring pendant la dur√©e sp√©cifi√©e
        while time.time() - start_time < duration:
            await asyncio.sleep(1)
            collected_metrics = collector.metrics_collected
            
            if collected_metrics % 10 == 0 and collected_metrics > 0:
                logger.info(f"üìà M√©triques collect√©es: {collected_metrics}")
        
        # Arr√™t des services
        await collector.stop()
        await alert_engine.stop()
        
        return {
            "duration": duration,
            "metrics_collected": collected_metrics,
            "collection_rate": collected_metrics / duration if duration > 0 else 0,
            "timestamp": datetime.now().isoformat()
        }
    
    async def _demo_deployment(self) -> Dict[str, Any]:
        """D√©monstration du d√©ploiement."""
        logger.info("üé≠ D√©mo: D√©ploiement")
        
        # Simulation d'un d√©ploiement rapide
        await asyncio.sleep(0.5)
        
        return {
            "demo": "deployment",
            "status": "success",
            "components_deployed": ["metrics_system", "storage", "collectors", "monitoring"]
        }
    
    async def _demo_metrics_collection(self) -> Dict[str, Any]:
        """D√©monstration de collecte de m√©triques."""
        logger.info("üé≠ D√©mo: Collecte de m√©triques")
        
        # Injection de quelques m√©triques de d√©monstration
        demo_metrics = []
        for i in range(10):
            metric = MetricDataPoint(
                metric_id=f"demo.metric.{i}",
                value=float(i * 10),
                metric_type=MetricType.GAUGE,
                category=MetricCategory.SYSTEM,
                tags={"demo": "true", "component": "orchestrator"}
            )
            await self.metrics_system.storage.store_metric(metric)
            demo_metrics.append(metric.metric_id)
        
        return {
            "demo": "metrics_collection",
            "metrics_created": len(demo_metrics),
            "metric_ids": demo_metrics
        }
    
    async def _demo_queries(self) -> Dict[str, Any]:
        """D√©monstration de requ√™tes."""
        logger.info("üé≠ D√©mo: Requ√™tes de m√©triques")
        
        # Requ√™te des m√©triques de d√©monstration
        results = await self.metrics_system.storage.query_metrics(
            metric_pattern="demo.*",
            start_time=datetime.now() - timedelta(minutes=5),
            end_time=datetime.now()
        )
        
        return {
            "demo": "queries",
            "query_pattern": "demo.*",
            "results_count": len(results),
            "sample_metrics": [r.metric_id for r in results[:3]]
        }
    
    async def _demo_alerts(self) -> Dict[str, Any]:
        """D√©monstration du syst√®me d'alertes."""
        logger.info("üé≠ D√©mo: Syst√®me d'alertes")
        
        # Simulation d'une alerte
        alert_engine = AlertEngine(self.metrics_system)
        
        return {
            "demo": "alerts",
            "alert_engine_status": "initialized",
            "sample_alert": "High CPU usage detected"
        }
    
    async def _demo_analytics(self) -> Dict[str, Any]:
        """D√©monstration des analytics."""
        logger.info("üé≠ D√©mo: Analytics")
        
        # Simulation d'analytics
        return {
            "demo": "analytics",
            "ml_models_loaded": ["anomaly_detection", "trend_analysis"],
            "analytics_ready": True
        }
    
    async def _inject_test_metrics(self, count: int) -> Dict[str, Any]:
        """Injecte des m√©triques de test."""
        logger.info(f"üíâ Injection de {count} m√©triques de test")
        
        import random
        
        metrics_created = []
        for i in range(count):
            metric = MetricDataPoint(
                metric_id=f"test.interactive.metric_{i}",
                value=random.uniform(0, 100),
                metric_type=random.choice([MetricType.GAUGE, MetricType.COUNTER]),
                category=random.choice([MetricCategory.SYSTEM, MetricCategory.PERFORMANCE, MetricCategory.BUSINESS]),
                tags={
                    "source": "interactive",
                    "batch": str(int(i / 10)),
                    "priority": random.choice(["low", "medium", "high"])
                }
            )
            await self.metrics_system.storage.store_metric(metric)
            metrics_created.append(metric.metric_id)
        
        return {
            "metrics_injected": count,
            "metric_ids": metrics_created[:5],  # Seulement les 5 premiers pour l'affichage
            "timestamp": datetime.now().isoformat()
        }
    
    async def _query_metrics_interactive(self, pattern: str) -> Dict[str, Any]:
        """Requ√™te interactive de m√©triques."""
        logger.info(f"üîç Requ√™te de m√©triques: {pattern}")
        
        results = await self.metrics_system.storage.query_metrics(
            metric_pattern=pattern,
            start_time=datetime.now() - timedelta(hours=1),
            end_time=datetime.now()
        )
        
        # Affichage des r√©sultats
        print(f"\nüìä R√©sultats de la requ√™te '{pattern}':")
        print(f"   Total: {len(results)} m√©triques trouv√©es")
        
        if results:
            print("   √âchantillon:")
            for metric in results[:5]:
                print(f"   ‚Ä¢ {metric.metric_id}: {metric.value} ({metric.metric_type.value})")
        
        return {
            "query_pattern": pattern,
            "results_count": len(results),
            "metrics": [
                {
                    "metric_id": m.metric_id,
                    "value": m.value,
                    "timestamp": m.timestamp.isoformat(),
                    "type": m.metric_type.value
                } for m in results[:10]  # Limiter pour √©viter les gros volumes
            ]
        }
    
    async def _show_system_stats(self) -> Dict[str, Any]:
        """Affiche les statistiques du syst√®me."""
        logger.info("üìä Affichage des statistiques syst√®me")
        
        # Requ√™te de toutes les m√©triques r√©centes
        all_metrics = await self.metrics_system.storage.query_metrics(
            start_time=datetime.now() - timedelta(hours=1),
            end_time=datetime.now()
        )
        
        # Calcul de statistiques
        total_metrics = len(all_metrics)
        
        metrics_by_type = {}
        metrics_by_category = {}
        
        for metric in all_metrics:
            # Par type
            type_name = metric.metric_type.value
            if type_name not in metrics_by_type:
                metrics_by_type[type_name] = 0
            metrics_by_type[type_name] += 1
            
            # Par cat√©gorie
            category_name = metric.category.value
            if category_name not in metrics_by_category:
                metrics_by_category[category_name] = 0
            metrics_by_category[category_name] += 1
        
        stats = {
            "total_metrics": total_metrics,
            "metrics_by_type": metrics_by_type,
            "metrics_by_category": metrics_by_category,
            "system_uptime": time.time() - self.start_time if self.start_time else 0,
            "timestamp": datetime.now().isoformat()
        }
        
        # Affichage format√©
        print(f"\nüìä Statistiques du Syst√®me:")
        print(f"   Total m√©triques: {stats['total_metrics']}")
        print(f"   Uptime: {stats['system_uptime']:.1f}s")
        
        if metrics_by_type:
            print("   Par type:")
            for type_name, count in metrics_by_type.items():
                print(f"     ‚Ä¢ {type_name}: {count}")
        
        if metrics_by_category:
            print("   Par cat√©gorie:")
            for category_name, count in metrics_by_category.items():
                print(f"     ‚Ä¢ {category_name}: {count}")
        
        return stats
    
    async def _save_orchestration_results(self, results: Dict[str, Any], mode: str):
        """Sauvegarde les r√©sultats d'orchestration."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Sauvegarde JSON
        if self.config.get("report_format") in ["json", "both"]:
            json_filename = f"orchestration_results_{mode}_{timestamp}.json"
            
            with open(json_filename, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            
            logger.info(f"üíæ R√©sultats sauvegard√©s: {json_filename}")
        
        # Sauvegarde Markdown
        if self.config.get("report_format") in ["markdown", "both"]:
            md_filename = f"orchestration_report_{mode}_{timestamp}.md"
            await self._generate_markdown_report(results, mode, md_filename)
            logger.info(f"üìù Rapport g√©n√©r√©: {md_filename}")
    
    async def _generate_markdown_report(self, results: Dict[str, Any], mode: str, filename: str):
        """G√©n√®re un rapport Markdown."""
        with open(filename, 'w') as f:
            f.write("# Rapport d'Orchestration - Syst√®me de M√©triques d'Entreprise\n\n")
            f.write(f"**Projet dirig√© par:** Fahed Mlaiel\n")
            f.write(f"**Mode d'ex√©cution:** {mode.upper()}\n")
            f.write(f"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # R√©sum√©
            if "orchestration_summary" in results:
                summary = results["orchestration_summary"]
                f.write("## üìä R√©sum√© Ex√©cutif\n\n")
                f.write(f"- **Statut:** {'‚úÖ SUCC√àS' if summary.get('success') else '‚ùå √âCHEC'}\n")
                f.write(f"- **Temps d'ex√©cution:** {summary.get('total_execution_time', 0):.1f}s\n")
                f.write(f"- **Mode:** {summary.get('mode', 'unknown')}\n\n")
            
            # D√©tails par composant
            components = {
                "deployment": "üì¶ D√©ploiement",
                "testing": "üß™ Tests",
                "benchmark": "‚ö° Benchmarks",
                "compliance": "üîç Compliance",
                "monitoring": "üìä Monitoring",
                "demo": "üé≠ D√©monstration",
                "interactive": "üîÑ Interactif"
            }
            
            for comp_key, comp_title in components.items():
                if comp_key in results:
                    f.write(f"## {comp_title}\n\n")
                    comp_data = results[comp_key]
                    
                    if isinstance(comp_data, dict):
                        if "error" in comp_data:
                            f.write(f"‚ùå **Erreur:** {comp_data['error']}\n\n")
                        else:
                            # Affichage des donn√©es principales
                            for key, value in comp_data.items():
                                if key not in ["error", "success"]:
                                    f.write(f"- **{key}:** {value}\n")
                            f.write("\n")
                    else:
                        f.write(f"```json\n{json.dumps(comp_data, indent=2, default=str)}\n```\n\n")
            
            f.write("---\n")
            f.write("*Rapport g√©n√©r√© par le Master Orchestrator*\n")
            f.write("*Expert Development Team - Dirig√© par Fahed Mlaiel*\n")


def create_argument_parser():
    """Cr√©e le parser d'arguments."""
    parser = argparse.ArgumentParser(
        description="Master Orchestrator - Syst√®me de M√©triques d'Entreprise",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
  python orchestrator.py --mode full                    # Orchestration compl√®te
  python orchestrator.py --mode test                    # Tests uniquement
  python orchestrator.py --mode benchmark               # Benchmarks uniquement
  python orchestrator.py --mode demo                    # D√©monstration
  python orchestrator.py --mode interactive             # Mode interactif
  python orchestrator.py --mode monitor --duration 60   # Monitoring 60s
  
D√©velopp√© par l'Expert Development Team - Dirig√© par Fahed Mlaiel
        """
    )
    
    parser.add_argument(
        "--mode", "-m",
        choices=[
            OrchestrationMode.FULL,
            OrchestrationMode.DEPLOY,
            OrchestrationMode.TEST,
            OrchestrationMode.BENCHMARK,
            OrchestrationMode.MONITOR,
            OrchestrationMode.COMPLIANCE,
            OrchestrationMode.DEMO,
            OrchestrationMode.INTERACTIVE
        ],
        default=OrchestrationMode.FULL,
        help="Mode d'orchestration (d√©faut: full)"
    )
    
    parser.add_argument(
        "--storage",
        choices=["sqlite", "redis", "postgresql"],
        default="sqlite",
        help="Backend de stockage (d√©faut: sqlite)"
    )
    
    parser.add_argument(
        "--duration",
        type=int,
        default=60,
        help="Dur√©e pour le mode monitor en secondes (d√©faut: 60)"
    )
    
    parser.add_argument(
        "--deployment-mode",
        choices=["development", "staging", "production"],
        default="development",
        help="Mode de d√©ploiement (d√©faut: development)"
    )
    
    parser.add_argument(
        "--report-format",
        choices=["json", "markdown", "both"],
        default="both",
        help="Format de rapport (d√©faut: both)"
    )
    
    parser.add_argument(
        "--no-cleanup",
        action="store_true",
        help="D√©sactiver le nettoyage automatique"
    )
    
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Mode verbeux"
    )
    
    return parser


async def main():
    """Fonction principale."""
    print("üöÄ Master Orchestrator - Syst√®me de M√©triques d'Entreprise")
    print("=" * 60)
    print("Expert Development Team - Projet dirig√© par Fahed Mlaiel")
    print("=" * 60)
    
    # Parse des arguments
    parser = create_argument_parser()
    args = parser.parse_args()
    
    # Configuration du logging
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Configuration de l'orchestrateur
    config = {
        "storage_backend": args.storage,
        "deployment_mode": args.deployment_mode,
        "auto_cleanup": not args.no_cleanup,
        "report_format": args.report_format,
        "verbose": args.verbose
    }
    
    # Cr√©ation et ex√©cution de l'orchestrateur
    orchestrator = MasterOrchestrator(config)
    
    try:
        # Arguments sp√©cifiques au mode
        kwargs = {}
        if args.mode == OrchestrationMode.MONITOR:
            kwargs["duration"] = args.duration
        
        # Ex√©cution
        results = await orchestrator.run_orchestration(args.mode, **kwargs)
        
        print("\n" + "=" * 60)
        print("üéâ ORCHESTRATION TERMIN√âE AVEC SUCC√àS!")
        print("=" * 60)
        
        # Affichage du r√©sum√©
        if "orchestration_summary" in results:
            summary = results["orchestration_summary"]
            print(f"üìä Mode: {summary.get('mode', 'unknown').upper()}")
            print(f"‚è±Ô∏è  Dur√©e: {summary.get('total_execution_time', 0):.1f}s")
            print(f"‚úÖ Statut: {'SUCC√àS' if summary.get('success') else '√âCHEC'}")
        
        # Statistiques par composant
        components_stats = []
        for comp in ["deployment", "testing", "benchmark", "compliance", "monitoring"]:
            if comp in results:
                comp_data = results[comp]
                if isinstance(comp_data, dict) and "error" not in comp_data:
                    components_stats.append(f"‚úÖ {comp}")
                else:
                    components_stats.append(f"‚ùå {comp}")
        
        if components_stats:
            print(f"üìã Composants: {', '.join(components_stats)}")
        
        print("\n" + "=" * 60)
        print("Merci d'avoir utilis√© le Master Orchestrator!")
        print("D√©velopp√© par l'√âquipe d'Experts - Dirig√© par Fahed Mlaiel")
        print("=" * 60)
        
        return 0
        
    except KeyboardInterrupt:
        print("\nüëã Interruption utilisateur")
        return 1
    except Exception as e:
        print(f"\nüí• ERREUR FATALE: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    # Ex√©cution de l'orchestrateur principal
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
