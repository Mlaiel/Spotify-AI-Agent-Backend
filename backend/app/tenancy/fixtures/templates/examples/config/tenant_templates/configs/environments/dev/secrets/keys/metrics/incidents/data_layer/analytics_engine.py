#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
üß† ANALYTICS ENGINE ULTRA-AVANC√â - INTELLIGENCE ARTIFICIELLE POUR DONN√âES
Moteur d'analytics r√©volutionnaire avec ML et IA pour insights ultra-pr√©cis

Architecture ML/AI Enterprise :
‚îú‚îÄ‚îÄ ü§ñ Machine Learning Pipeline (AutoML + Custom Models)
‚îú‚îÄ‚îÄ üìä Time Series Analytics (Prophet, ARIMA, LSTM)
‚îú‚îÄ‚îÄ üîç Anomaly Detection (Isolation Forest, DBSCAN, VAE)
‚îú‚îÄ‚îÄ üìà Predictive Analytics (XGBoost, Neural Networks)
‚îú‚îÄ‚îÄ üéØ Real-time Scoring (Sub-millisecond predictions)
‚îú‚îÄ‚îÄ üß¨ Feature Engineering (Automated feature discovery)
‚îú‚îÄ‚îÄ üìö Model Registry (MLflow + versioning)
‚îú‚îÄ‚îÄ üîÑ Auto-retraining (Drift detection + adaptive)
‚îú‚îÄ‚îÄ üìã Statistical Analysis (Distributions + tests)
‚îî‚îÄ‚îÄ ‚ö° GPU Acceleration (CUDA optimized)

D√©velopp√© par l'√©quipe d'experts Achiri avec IA de niveau industriel
Version: 3.0.0 - Production Ready Enterprise
"""

__version__ = "3.0.0"
__author__ = "Achiri Expert Team - AI Analytics Division"
__license__ = "Enterprise Commercial"

import asyncio
import logging
import sys
import time
import json
import uuid
import pickle
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import (
    Dict, List, Optional, Any, Union, Tuple, Set, 
    AsyncGenerator, Callable, TypeVar, Generic
)
from dataclasses import dataclass, field, asdict
from enum import Enum, auto
from abc import ABC, abstractmethod
from contextlib import asynccontextmanager
from concurrent.futures import ThreadPoolExecutor
import threading
from collections import defaultdict, deque
import warnings
warnings.filterwarnings('ignore')

# Machine Learning et IA
try:
    import scikit_learn
    from sklearn.ensemble import (
        IsolationForest, RandomForestRegressor, GradientBoostingRegressor
    )
    from sklearn.cluster import DBSCAN, KMeans
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import (
        mean_squared_error, mean_absolute_error, r2_score,
        precision_score, recall_score, f1_score
    )
    from sklearn.decomposition import PCA
    from sklearn.manifold import TSNE
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

try:
    from prophet import Prophet
    PROPHET_AVAILABLE = True
except ImportError:
    PROPHET_AVAILABLE = False

try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.utils.data import DataLoader, TensorDataset
    PYTORCH_AVAILABLE = True
except ImportError:
    PYTORCH_AVAILABLE = False

# Analytics et statistiques avanc√©es
try:
    from scipy import stats, signal
    from scipy.fft import fft, ifft
    from statsmodels.tsa.arima.model import ARIMA
    from statsmodels.tsa.seasonal import seasonal_decompose
    from statsmodels.stats.stattools import durbin_watson
    SCIPY_STATSMODELS_AVAILABLE = True
except ImportError:
    SCIPY_STATSMODELS_AVAILABLE = False

# Traitement parall√®le et optimisations
try:
    import joblib
    from joblib import Parallel, delayed
    JOBLIB_AVAILABLE = True
except ImportError:
    JOBLIB_AVAILABLE = False

try:
    import dask.dataframe as dd
    from dask.distributed import Client
    DASK_AVAILABLE = True
except ImportError:
    DASK_AVAILABLE = False

# Model Registry et MLOps
try:
    import mlflow
    import mlflow.sklearn
    import mlflow.tensorflow
    import mlflow.pytorch
    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False

# Monitoring et m√©triques
try:
    from prometheus_client import Counter, Histogram, Gauge
    MONITORING_AVAILABLE = True
except ImportError:
    MONITORING_AVAILABLE = False

logger = logging.getLogger(__name__)

# =============================================================================
# √âNUM√âRATIONS ET TYPES
# =============================================================================

class AnalyticsType(Enum):
    """Types d'analytics support√©s"""
    TIME_SERIES = auto()        # Analyse de s√©ries temporelles
    ANOMALY_DETECTION = auto()  # D√©tection d'anomalies
    PREDICTIVE = auto()         # Analytics pr√©dictive
    CLUSTERING = auto()         # Clustering et segmentation
    CLASSIFICATION = auto()     # Classification
    REGRESSION = auto()         # R√©gression
    STATISTICAL = auto()        # Analyse statistique
    REAL_TIME = auto()         # Analytics temps r√©el

class ModelType(Enum):
    """Types de mod√®les ML"""
    LINEAR_REGRESSION = auto()
    RANDOM_FOREST = auto()
    GRADIENT_BOOSTING = auto()
    XGBOOST = auto()
    NEURAL_NETWORK = auto()
    LSTM = auto()
    TRANSFORMER = auto()
    PROPHET = auto()
    ARIMA = auto()
    ISOLATION_FOREST = auto()
    DBSCAN = auto()
    KMEANS = auto()
    VAE = auto()  # Variational Autoencoder

class FeatureType(Enum):
    """Types de features"""
    NUMERICAL = auto()
    CATEGORICAL = auto()
    TEMPORAL = auto()
    TEXT = auto()
    IMAGE = auto()
    EMBEDDING = auto()

class ModelStatus(Enum):
    """√âtats des mod√®les"""
    TRAINING = auto()
    TRAINED = auto()
    DEPLOYED = auto()
    DEPRECATED = auto()
    FAILED = auto()

# =============================================================================
# MOD√àLES DE DONN√âES
# =============================================================================

@dataclass
class AnalyticsConfig:
    """Configuration pour analytics"""
    analytics_type: AnalyticsType
    model_type: ModelType
    
    # Param√®tres g√©n√©raux
    target_column: str
    feature_columns: List[str]
    time_column: Optional[str] = None
    
    # Param√®tres ML
    test_size: float = 0.2
    validation_size: float = 0.1
    random_state: int = 42
    
    # Hyperparam√®tres
    hyperparameters: Dict[str, Any] = field(default_factory=dict)
    
    # Optimisations
    auto_feature_engineering: bool = True
    feature_selection: bool = True
    hyperparameter_tuning: bool = True
    
    # Production
    model_versioning: bool = True
    auto_retraining: bool = True
    drift_detection: bool = True
    
    # Performance
    enable_gpu: bool = False
    parallel_jobs: int = -1
    batch_size: int = 1000

@dataclass
class ModelMetrics:
    """M√©triques de performance des mod√®les"""
    model_id: str
    model_type: ModelType
    analytics_type: AnalyticsType
    
    # M√©triques de performance
    mse: Optional[float] = None
    mae: Optional[float] = None
    r2_score: Optional[float] = None
    accuracy: Optional[float] = None
    precision: Optional[float] = None
    recall: Optional[float] = None
    f1_score: Optional[float] = None
    
    # M√©triques time series
    mape: Optional[float] = None  # Mean Absolute Percentage Error
    smape: Optional[float] = None  # Symmetric Mean Absolute Percentage Error
    
    # M√©triques business
    prediction_latency_ms: float = 0.0
    training_time_seconds: float = 0.0
    model_size_mb: float = 0.0
    
    # Drift et qualit√©
    data_drift_score: Optional[float] = None
    model_drift_score: Optional[float] = None
    feature_importance: Optional[Dict[str, float]] = None
    
    # Timestamps
    created_at: datetime = field(default_factory=datetime.utcnow)
    last_updated: datetime = field(default_factory=datetime.utcnow)

@dataclass
class PredictionRequest:
    """Requ√™te de pr√©diction"""
    model_id: str
    features: Dict[str, Any]
    
    # Options
    include_confidence: bool = True
    include_explanation: bool = False
    batch_prediction: bool = False
    
    # M√©tadonn√©es
    request_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: datetime = field(default_factory=datetime.utcnow)

@dataclass
class PredictionResponse:
    """R√©ponse de pr√©diction"""
    request_id: str
    model_id: str
    
    # R√©sultats
    prediction: Union[float, int, str, List[Any]]
    confidence: Optional[float] = None
    
    # Explications
    feature_importance: Optional[Dict[str, float]] = None
    shap_values: Optional[List[float]] = None
    
    # M√©triques
    prediction_time_ms: float = 0.0
    model_version: str = "1.0.0"
    
    # Timestamps
    timestamp: datetime = field(default_factory=datetime.utcnow)

# =============================================================================
# MOTEURS D'ANALYTICS SP√âCIALIS√âS
# =============================================================================

class TimeSeriesAnalyzer:
    """
    üìä ANALYSEUR TIME SERIES ULTRA-AVANC√â
    
    Moteur d'analyse de s√©ries temporelles avec ML :
    - Prophet pour tendances et saisonnalit√©
    - ARIMA pour mod√©lisation classique
    - LSTM pour patterns complexes
    - D√©tection d'anomalies temporelles
    - Pr√©dictions multi-horizon
    """
    
    def __init__(self, config: AnalyticsConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.TimeSeriesAnalyzer")
        
        # Mod√®les disponibles
        self._models = {}
        self._scalers = {}
        
        # Cache des pr√©dictions
        self._prediction_cache = {}
        
        # M√©triques
        self._metrics = {}
    
    async def train_prophet_model(
        self,
        data: pd.DataFrame,
        model_id: str = None
    ) -> str:
        """Entra√Ænement mod√®le Prophet pour time series"""
        if not PROPHET_AVAILABLE:
            raise ValueError("Prophet non disponible")
        
        model_id = model_id or f"prophet_{int(time.time())}"
        start_time = time.time()
        
        try:
            self.logger.info(f"üîÆ Entra√Ænement Prophet pour {model_id}...")
            
            # Pr√©paration des donn√©es pour Prophet
            prophet_data = data[[self.config.time_column, self.config.target_column]].copy()
            prophet_data.columns = ['ds', 'y']
            prophet_data['ds'] = pd.to_datetime(prophet_data['ds'])
            
            # Configuration Prophet avec optimisations
            model = Prophet(
                yearly_seasonality=True,
                weekly_seasonality=True,
                daily_seasonality=True,
                seasonality_mode='multiplicative',
                changepoint_prior_scale=0.05,
                seasonality_prior_scale=10.0,
                holidays_prior_scale=10.0,
                mcmc_samples=300 if len(prophet_data) > 1000 else 0,
                interval_width=0.95,
                uncertainty_samples=1000
            )
            
            # Ajout de r√©gresseurs si sp√©cifi√©s
            if len(self.config.feature_columns) > 1:
                for col in self.config.feature_columns:
                    if col != self.config.target_column and col in data.columns:
                        model.add_regressor(col)
                        prophet_data[col] = data[col]
            
            # Entra√Ænement
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                model.fit(prophet_data)
            
            # Stockage du mod√®le
            self._models[model_id] = {
                'model': model,
                'type': ModelType.PROPHET,
                'status': ModelStatus.TRAINED,
                'training_data_size': len(prophet_data),
                'created_at': datetime.utcnow()
            }
            
            # M√©triques d'entra√Ænement
            training_time = time.time() - start_time
            
            # Validation crois√©e pour √©valuation
            cv_results = await self._cross_validate_prophet(model, prophet_data)
            
            self._metrics[model_id] = ModelMetrics(
                model_id=model_id,
                model_type=ModelType.PROPHET,
                analytics_type=AnalyticsType.TIME_SERIES,
                training_time_seconds=training_time,
                mape=cv_results.get('mape', 0.0),
                smape=cv_results.get('smape', 0.0)
            )
            
            self.logger.info(f"‚úÖ Prophet {model_id} entra√Æn√© (MAPE: {cv_results.get('mape', 0):.2f}%)")
            return model_id
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur entra√Ænement Prophet: {e}")
            raise
    
    async def _cross_validate_prophet(
        self,
        model: 'Prophet',
        data: pd.DataFrame
    ) -> Dict[str, float]:
        """Validation crois√©e pour Prophet"""
        try:
            from prophet.diagnostics import cross_validation, performance_metrics
            
            # Validation crois√©e
            initial_size = int(len(data) * 0.7)
            period = max(1, len(data) // 10)
            horizon = max(1, len(data) // 20)
            
            cv_results = cross_validation(
                model,
                initial=f'{initial_size} days',
                period=f'{period} days',
                horizon=f'{horizon} days',
                parallel='processes' if JOBLIB_AVAILABLE else None
            )
            
            # Calcul des m√©triques
            metrics = performance_metrics(cv_results)
            
            return {
                'mape': metrics['mape'].mean() * 100,
                'smape': metrics['smape'].mean() * 100,
                'mae': metrics['mae'].mean(),
                'rmse': metrics['rmse'].mean()
            }
            
        except Exception as e:
            self.logger.warning(f"Erreur validation crois√©e: {e}")
            return {}
    
    async def train_lstm_model(
        self,
        data: pd.DataFrame,
        sequence_length: int = 60,
        model_id: str = None
    ) -> str:
        """Entra√Ænement mod√®le LSTM pour time series complexes"""
        if not TENSORFLOW_AVAILABLE:
            raise ValueError("TensorFlow non disponible")
        
        model_id = model_id or f"lstm_{int(time.time())}"
        start_time = time.time()
        
        try:
            self.logger.info(f"üß† Entra√Ænement LSTM pour {model_id}...")
            
            # Pr√©paration des donn√©es
            features = data[self.config.feature_columns].values
            target = data[self.config.target_column].values
            
            # Normalisation
            feature_scaler = MinMaxScaler()
            target_scaler = MinMaxScaler()
            
            features_scaled = feature_scaler.fit_transform(features)
            target_scaled = target_scaler.fit_transform(target.reshape(-1, 1)).flatten()
            
            # Cr√©ation des s√©quences
            X, y = self._create_sequences(features_scaled, target_scaled, sequence_length)
            
            # Division train/test
            split_idx = int(len(X) * (1 - self.config.test_size))
            X_train, X_test = X[:split_idx], X[split_idx:]
            y_train, y_test = y[:split_idx], y[split_idx:]
            
            # Architecture LSTM optimis√©e
            model = keras.Sequential([
                layers.LSTM(128, return_sequences=True, input_shape=(sequence_length, len(self.config.feature_columns))),
                layers.Dropout(0.2),
                layers.LSTM(64, return_sequences=True),
                layers.Dropout(0.2),
                layers.LSTM(32),
                layers.Dropout(0.2),
                layers.Dense(16, activation='relu'),
                layers.Dense(1)
            ])
            
            # Compilation avec optimiseur adaptatif
            model.compile(
                optimizer=keras.optimizers.Adam(learning_rate=0.001),
                loss='mse',
                metrics=['mae', 'mape']
            )
            
            # Callbacks pour optimisation
            callbacks = [
                keras.callbacks.EarlyStopping(
                    monitor='val_loss',
                    patience=10,
                    restore_best_weights=True
                ),
                keras.callbacks.ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=0.5,
                    patience=5,
                    min_lr=0.0001
                )
            ]
            
            # Entra√Ænement
            history = model.fit(
                X_train, y_train,
                epochs=100,
                batch_size=self.config.batch_size,
                validation_data=(X_test, y_test),
                callbacks=callbacks,
                verbose=0
            )
            
            # √âvaluation
            y_pred = model.predict(X_test)
            y_pred_rescaled = target_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()
            y_test_rescaled = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()
            
            mse = mean_squared_error(y_test_rescaled, y_pred_rescaled)
            mae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)
            r2 = r2_score(y_test_rescaled, y_pred_rescaled)
            
            # Stockage du mod√®le
            self._models[model_id] = {
                'model': model,
                'feature_scaler': feature_scaler,
                'target_scaler': target_scaler,
                'sequence_length': sequence_length,
                'type': ModelType.LSTM,
                'status': ModelStatus.TRAINED,
                'created_at': datetime.utcnow()
            }
            
            self._scalers[model_id] = {
                'feature_scaler': feature_scaler,
                'target_scaler': target_scaler
            }
            
            # M√©triques
            training_time = time.time() - start_time
            self._metrics[model_id] = ModelMetrics(
                model_id=model_id,
                model_type=ModelType.LSTM,
                analytics_type=AnalyticsType.TIME_SERIES,
                mse=mse,
                mae=mae,
                r2_score=r2,
                training_time_seconds=training_time
            )
            
            self.logger.info(f"‚úÖ LSTM {model_id} entra√Æn√© (R¬≤: {r2:.4f}, MAE: {mae:.4f})")
            return model_id
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur entra√Ænement LSTM: {e}")
            raise
    
    def _create_sequences(
        self,
        features: np.ndarray,
        target: np.ndarray,
        sequence_length: int
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Cr√©ation de s√©quences pour LSTM"""
        X, y = [], []
        
        for i in range(sequence_length, len(features)):
            X.append(features[i-sequence_length:i])
            y.append(target[i])
        
        return np.array(X), np.array(y)
    
    async def predict(
        self,
        model_id: str,
        data: pd.DataFrame,
        periods: int = 30
    ) -> Dict[str, Any]:
        """Pr√©diction avec mod√®le time series"""
        start_time = time.time()
        
        if model_id not in self._models:
            raise ValueError(f"Mod√®le {model_id} non trouv√©")
        
        model_info = self._models[model_id]
        model = model_info['model']
        model_type = model_info['type']
        
        try:
            if model_type == ModelType.PROPHET:
                return await self._predict_prophet(model, data, periods)
            elif model_type == ModelType.LSTM:
                return await self._predict_lstm(model_id, data, periods)
            else:
                raise ValueError(f"Type de mod√®le {model_type} non support√©")
                
        except Exception as e:
            self.logger.error(f"‚ùå Erreur pr√©diction {model_id}: {e}")
            raise
    
    async def _predict_prophet(
        self,
        model: 'Prophet',
        data: pd.DataFrame,
        periods: int
    ) -> Dict[str, Any]:
        """Pr√©diction avec Prophet"""
        # Cr√©ation du dataframe futur
        future = model.make_future_dataframe(periods=periods, freq='D')
        
        # Ajout des r√©gresseurs si n√©cessaire
        if len(self.config.feature_columns) > 1:
            for col in self.config.feature_columns:
                if col != self.config.target_column and col in data.columns:
                    # Extension des valeurs (moyenne mobile ou derni√®re valeur)
                    last_values = data[col].tail(periods).values
                    extended_values = np.tile(last_values, (len(future) // len(last_values)) + 1)[:len(future)]
                    future[col] = extended_values
        
        # Pr√©diction
        forecast = model.predict(future)
        
        return {
            'predictions': forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(periods).to_dict('records'),
            'components': {
                'trend': forecast['trend'].tail(periods).tolist(),
                'seasonal': forecast.get('yearly', [0] * periods)[-periods:] if 'yearly' in forecast.columns else [],
                'weekly': forecast.get('weekly', [0] * periods)[-periods:] if 'weekly' in forecast.columns else []
            },
            'confidence_intervals': True,
            'model_type': 'Prophet'
        }
    
    async def _predict_lstm(
        self,
        model_id: str,
        data: pd.DataFrame,
        periods: int
    ) -> Dict[str, Any]:
        """Pr√©diction avec LSTM"""
        model_info = self._models[model_id]
        model = model_info['model']
        feature_scaler = model_info['feature_scaler']
        target_scaler = model_info['target_scaler']
        sequence_length = model_info['sequence_length']
        
        # Pr√©paration des donn√©es
        features = data[self.config.feature_columns].values
        features_scaled = feature_scaler.transform(features)
        
        # Pr√©dictions r√©cursives
        predictions = []
        current_sequence = features_scaled[-sequence_length:].copy()
        
        for _ in range(periods):
            # Pr√©diction pour le point suivant
            pred_input = current_sequence.reshape(1, sequence_length, -1)
            pred = model.predict(pred_input, verbose=0)[0, 0]
            
            # D√©scaling
            pred_rescaled = target_scaler.inverse_transform([[pred]])[0, 0]
            predictions.append(pred_rescaled)
            
            # Mise √† jour de la s√©quence (roll forward)
            # Note: Ici on utilise la pr√©diction pour la feature target
            new_row = current_sequence[-1].copy()
            if self.config.target_column in self.config.feature_columns:
                target_idx = self.config.feature_columns.index(self.config.target_column)
                new_row[target_idx] = pred
            
            current_sequence = np.roll(current_sequence, -1, axis=0)
            current_sequence[-1] = new_row
        
        return {
            'predictions': predictions,
            'confidence_intervals': False,
            'model_type': 'LSTM'
        }

class AnomalyDetector:
    """
    üîç D√âTECTEUR D'ANOMALIES ULTRA-AVANC√â
    
    D√©tection d'anomalies multi-algorithmes :
    - Isolation Forest pour anomalies g√©n√©rales
    - DBSCAN pour anomalies de clustering
    - Autoencoders pour anomalies complexes
    - D√©tection statistique (Z-score, IQR)
    - D√©tection temporelle (changements de distribution)
    """
    
    def __init__(self, config: AnalyticsConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.AnomalyDetector")
        
        # Mod√®les d'anomalie
        self._detectors = {}
        self._thresholds = {}
        
        # Historique pour d√©tection contextuelle
        self._baseline_stats = {}
    
    async def train_isolation_forest(
        self,
        data: pd.DataFrame,
        contamination: float = 0.1,
        model_id: str = None
    ) -> str:
        """Entra√Ænement Isolation Forest pour anomalies"""
        if not SKLEARN_AVAILABLE:
            raise ValueError("Scikit-learn non disponible")
        
        model_id = model_id or f"isolation_forest_{int(time.time())}"
        start_time = time.time()
        
        try:
            self.logger.info(f"üå≤ Entra√Ænement Isolation Forest pour {model_id}...")
            
            # Pr√©paration des donn√©es
            features = data[self.config.feature_columns].values
            
            # Normalisation
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(features)
            
            # Mod√®le Isolation Forest optimis√©
            model = IsolationForest(
                contamination=contamination,
                random_state=self.config.random_state,
                n_estimators=200,
                max_samples='auto',
                bootstrap=True,
                n_jobs=self.config.parallel_jobs
            )
            
            # Entra√Ænement
            model.fit(features_scaled)
            
            # Calcul des scores d'anomalie
            anomaly_scores = model.decision_function(features_scaled)
            predictions = model.predict(features_scaled)
            
            # Calcul du seuil optimal
            threshold = np.percentile(anomaly_scores, contamination * 100)
            
            # Stockage
            self._detectors[model_id] = {
                'model': model,
                'scaler': scaler,
                'threshold': threshold,
                'type': ModelType.ISOLATION_FOREST,
                'contamination': contamination,
                'status': ModelStatus.TRAINED,
                'created_at': datetime.utcnow()
            }
            
            self._thresholds[model_id] = threshold
            
            # M√©triques
            training_time = time.time() - start_time
            anomaly_count = np.sum(predictions == -1)
            
            self._metrics[model_id] = ModelMetrics(
                model_id=model_id,
                model_type=ModelType.ISOLATION_FOREST,
                analytics_type=AnalyticsType.ANOMALY_DETECTION,
                training_time_seconds=training_time
            )
            
            self.logger.info(f"‚úÖ Isolation Forest {model_id} entra√Æn√© ({anomaly_count} anomalies d√©tect√©es)")
            return model_id
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur entra√Ænement Isolation Forest: {e}")
            raise
    
    async def train_autoencoder(
        self,
        data: pd.DataFrame,
        encoding_dim: int = None,
        model_id: str = None
    ) -> str:
        """Entra√Ænement Autoencoder pour anomalies complexes"""
        if not TENSORFLOW_AVAILABLE:
            raise ValueError("TensorFlow non disponible")
        
        model_id = model_id or f"autoencoder_{int(time.time())}"
        start_time = time.time()
        
        try:
            self.logger.info(f"üß† Entra√Ænement Autoencoder pour {model_id}...")
            
            # Pr√©paration des donn√©es
            features = data[self.config.feature_columns].values
            
            # Normalisation
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(features)
            
            input_dim = features_scaled.shape[1]
            encoding_dim = encoding_dim or max(2, input_dim // 4)
            
            # Architecture Autoencoder
            input_layer = keras.layers.Input(shape=(input_dim,))
            
            # Encoder
            encoded = keras.layers.Dense(encoding_dim * 2, activation='relu')(input_layer)
            encoded = keras.layers.Dropout(0.2)(encoded)
            encoded = keras.layers.Dense(encoding_dim, activation='relu')(encoded)
            
            # Decoder
            decoded = keras.layers.Dense(encoding_dim * 2, activation='relu')(encoded)
            decoded = keras.layers.Dropout(0.2)(decoded)
            decoded = keras.layers.Dense(input_dim, activation='linear')(decoded)
            
            # Mod√®le complet
            autoencoder = keras.Model(input_layer, decoded)
            autoencoder.compile(
                optimizer='adam',
                loss='mse',
                metrics=['mae']
            )
            
            # Entra√Ænement
            history = autoencoder.fit(
                features_scaled, features_scaled,
                epochs=100,
                batch_size=self.config.batch_size,
                validation_split=0.1,
                callbacks=[
                    keras.callbacks.EarlyStopping(
                        monitor='val_loss',
                        patience=10,
                        restore_best_weights=True
                    )
                ],
                verbose=0
            )
            
            # Calcul des erreurs de reconstruction
            reconstructed = autoencoder.predict(features_scaled, verbose=0)
            reconstruction_errors = np.mean(np.square(features_scaled - reconstructed), axis=1)
            
            # Seuil d'anomalie (95e percentile)
            threshold = np.percentile(reconstruction_errors, 95)
            
            # Stockage
            self._detectors[model_id] = {
                'model': autoencoder,
                'scaler': scaler,
                'threshold': threshold,
                'type': ModelType.VAE,
                'encoding_dim': encoding_dim,
                'status': ModelStatus.TRAINED,
                'created_at': datetime.utcnow()
            }
            
            self._thresholds[model_id] = threshold
            
            # M√©triques
            training_time = time.time() - start_time
            anomaly_count = np.sum(reconstruction_errors > threshold)
            
            self._metrics[model_id] = ModelMetrics(
                model_id=model_id,
                model_type=ModelType.VAE,
                analytics_type=AnalyticsType.ANOMALY_DETECTION,
                training_time_seconds=training_time
            )
            
            self.logger.info(f"‚úÖ Autoencoder {model_id} entra√Æn√© ({anomaly_count} anomalies d√©tect√©es)")
            return model_id
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur entra√Ænement Autoencoder: {e}")
            raise
    
    async def detect_anomalies(
        self,
        model_id: str,
        data: pd.DataFrame
    ) -> Dict[str, Any]:
        """D√©tection d'anomalies avec mod√®le sp√©cifi√©"""
        if model_id not in self._detectors:
            raise ValueError(f"D√©tecteur {model_id} non trouv√©")
        
        detector_info = self._detectors[model_id]
        model = detector_info['model']
        scaler = detector_info['scaler']
        threshold = detector_info['threshold']
        model_type = detector_info['type']
        
        try:
            # Pr√©paration des donn√©es
            features = data[self.config.feature_columns].values
            features_scaled = scaler.transform(features)
            
            if model_type == ModelType.ISOLATION_FOREST:
                return await self._detect_isolation_forest(model, features_scaled, threshold, data)
            elif model_type == ModelType.VAE:
                return await self._detect_autoencoder(model, features_scaled, threshold, data)
            else:
                raise ValueError(f"Type de d√©tecteur {model_type} non support√©")
                
        except Exception as e:
            self.logger.error(f"‚ùå Erreur d√©tection anomalies: {e}")
            raise
    
    async def _detect_isolation_forest(
        self,
        model,
        features_scaled: np.ndarray,
        threshold: float,
        original_data: pd.DataFrame
    ) -> Dict[str, Any]:
        """D√©tection avec Isolation Forest"""
        # Scores d'anomalie
        anomaly_scores = model.decision_function(features_scaled)
        predictions = model.predict(features_scaled)
        
        # Identification des anomalies
        anomalies_idx = np.where(predictions == -1)[0]
        
        # D√©tails des anomalies
        anomalies = []
        for idx in anomalies_idx:
            anomalies.append({
                'index': int(idx),
                'score': float(anomaly_scores[idx]),
                'severity': 'high' if anomaly_scores[idx] < threshold * 0.5 else 'medium',
                'features': original_data.iloc[idx][self.config.feature_columns].to_dict(),
                'timestamp': original_data.iloc[idx][self.config.time_column] if self.config.time_column else None
            })
        
        return {
            'total_points': len(features_scaled),
            'anomalies_count': len(anomalies),
            'anomaly_rate': len(anomalies) / len(features_scaled) * 100,
            'anomalies': anomalies,
            'scores': anomaly_scores.tolist(),
            'threshold': threshold,
            'model_type': 'Isolation Forest'
        }
    
    async def _detect_autoencoder(
        self,
        model,
        features_scaled: np.ndarray,
        threshold: float,
        original_data: pd.DataFrame
    ) -> Dict[str, Any]:
        """D√©tection avec Autoencoder"""
        # Reconstruction et erreurs
        reconstructed = model.predict(features_scaled, verbose=0)
        reconstruction_errors = np.mean(np.square(features_scaled - reconstructed), axis=1)
        
        # Identification des anomalies
        anomalies_idx = np.where(reconstruction_errors > threshold)[0]
        
        # D√©tails des anomalies
        anomalies = []
        for idx in anomalies_idx:
            error = reconstruction_errors[idx]
            severity = 'high' if error > threshold * 2 else 'medium'
            
            anomalies.append({
                'index': int(idx),
                'reconstruction_error': float(error),
                'severity': severity,
                'features': original_data.iloc[idx][self.config.feature_columns].to_dict(),
                'timestamp': original_data.iloc[idx][self.config.time_column] if self.config.time_column else None
            })
        
        return {
            'total_points': len(features_scaled),
            'anomalies_count': len(anomalies),
            'anomaly_rate': len(anomalies) / len(features_scaled) * 100,
            'anomalies': anomalies,
            'reconstruction_errors': reconstruction_errors.tolist(),
            'threshold': threshold,
            'model_type': 'Autoencoder'
        }

# =============================================================================
# GESTIONNAIRE ANALYTICS PRINCIPAL
# =============================================================================

class AnalyticsEngine:
    """
    üß† MOTEUR D'ANALYTICS PRINCIPAL ULTRA-AVANC√â
    
    Orchestrateur ML/AI pour analytics enterprise :
    - Gestion multi-mod√®les
    - Pipeline automatis√©
    - Optimisation de performance
    - Model registry int√©gr√©
    - Monitoring en temps r√©el
    """
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.AnalyticsEngine")
        
        # Analyseurs sp√©cialis√©s
        self._time_series_analyzer = None
        self._anomaly_detector = None
        
        # Registre des mod√®les
        self._model_registry = {}
        self._active_models = {}
        
        # M√©triques globales
        self._global_metrics = {
            "total_models": 0,
            "active_predictions": 0,
            "avg_prediction_time_ms": 0.0,
            "model_accuracy_avg": 0.0
        }
        
        # Performance monitoring
        if MONITORING_AVAILABLE:
            self._prediction_counter = Counter('ml_predictions_total', 'Total ML predictions')
            self._prediction_latency = Histogram('ml_prediction_duration_seconds', 'ML prediction latency')
            self._model_accuracy = Gauge('ml_model_accuracy', 'Model accuracy score')
    
    async def initialize(self) -> bool:
        """Initialisation du moteur analytics"""
        try:
            self.logger.info("üß† Initialisation Analytics Engine Ultra-Avanc√©...")
            
            # V√©rification des d√©pendances
            dependencies = self._check_dependencies()
            self.logger.info(f"üì¶ D√©pendances disponibles: {dependencies}")
            
            # Initialisation MLflow si disponible
            if MLFLOW_AVAILABLE:
                try:
                    mlflow.set_tracking_uri("sqlite:///mlflow.db")
                    mlflow.set_experiment("data_layer_analytics")
                    self.logger.info("‚úÖ MLflow initialis√©")
                except Exception as e:
                    self.logger.warning(f"MLflow non initialis√©: {e}")
            
            self.logger.info("‚úÖ Analytics Engine initialis√© avec succ√®s")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation Analytics Engine: {e}")
            return False
    
    def _check_dependencies(self) -> Dict[str, bool]:
        """V√©rification des d√©pendances ML/AI"""
        return {
            "scikit_learn": SKLEARN_AVAILABLE,
            "xgboost": XGBOOST_AVAILABLE,
            "prophet": PROPHET_AVAILABLE,
            "tensorflow": TENSORFLOW_AVAILABLE,
            "pytorch": PYTORCH_AVAILABLE,
            "scipy_statsmodels": SCIPY_STATSMODELS_AVAILABLE,
            "joblib": JOBLIB_AVAILABLE,
            "dask": DASK_AVAILABLE,
            "mlflow": MLFLOW_AVAILABLE
        }
    
    async def create_analyzer(
        self,
        config: AnalyticsConfig
    ) -> Union[TimeSeriesAnalyzer, AnomalyDetector]:
        """Cr√©ation d'un analyseur sp√©cialis√©"""
        try:
            if config.analytics_type == AnalyticsType.TIME_SERIES:
                analyzer = TimeSeriesAnalyzer(config)
                self._time_series_analyzer = analyzer
                return analyzer
            elif config.analytics_type == AnalyticsType.ANOMALY_DETECTION:
                detector = AnomalyDetector(config)
                self._anomaly_detector = detector
                return detector
            else:
                raise ValueError(f"Type d'analytics {config.analytics_type} non support√©")
                
        except Exception as e:
            self.logger.error(f"‚ùå Erreur cr√©ation analyseur: {e}")
            raise
    
    async def train_model(
        self,
        config: AnalyticsConfig,
        data: pd.DataFrame,
        model_name: str = None
    ) -> str:
        """Entra√Ænement de mod√®le avec configuration"""
        try:
            # Cr√©ation de l'analyseur appropri√©
            analyzer = await self.create_analyzer(config)
            
            # Entra√Ænement selon le type de mod√®le
            if config.model_type == ModelType.PROPHET:
                model_id = await analyzer.train_prophet_model(data, model_name)
            elif config.model_type == ModelType.LSTM:
                model_id = await analyzer.train_lstm_model(data, model_id=model_name)
            elif config.model_type == ModelType.ISOLATION_FOREST:
                model_id = await analyzer.train_isolation_forest(data, model_id=model_name)
            elif config.model_type == ModelType.VAE:
                model_id = await analyzer.train_autoencoder(data, model_id=model_name)
            else:
                raise ValueError(f"Type de mod√®le {config.model_type} non support√©")
            
            # Enregistrement dans le registre
            self._model_registry[model_id] = {
                'config': config,
                'analyzer': analyzer,
                'status': ModelStatus.TRAINED,
                'created_at': datetime.utcnow()
            }
            
            # Mise √† jour m√©triques globales
            self._global_metrics["total_models"] += 1
            
            # Enregistrement MLflow
            if MLFLOW_AVAILABLE:
                await self._log_to_mlflow(model_id, config, analyzer)
            
            self.logger.info(f"‚úÖ Mod√®le {model_id} entra√Æn√© et enregistr√©")
            return model_id
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur entra√Ænement mod√®le: {e}")
            raise
    
    async def _log_to_mlflow(
        self,
        model_id: str,
        config: AnalyticsConfig,
        analyzer: Union[TimeSeriesAnalyzer, AnomalyDetector]
    ):
        """Enregistrement dans MLflow"""
        try:
            with mlflow.start_run(run_name=model_id):
                # Log des param√®tres
                mlflow.log_params({
                    "model_type": config.model_type.name,
                    "analytics_type": config.analytics_type.name,
                    "target_column": config.target_column,
                    "feature_columns": str(config.feature_columns),
                    "test_size": config.test_size,
                    "random_state": config.random_state
                })
                
                # Log des hyperparam√®tres
                for key, value in config.hyperparameters.items():
                    mlflow.log_param(f"hp_{key}", value)
                
                # Log des m√©triques si disponibles
                if hasattr(analyzer, '_metrics') and model_id in analyzer._metrics:
                    metrics = analyzer._metrics[model_id]
                    if metrics.mse:
                        mlflow.log_metric("mse", metrics.mse)
                    if metrics.mae:
                        mlflow.log_metric("mae", metrics.mae)
                    if metrics.r2_score:
                        mlflow.log_metric("r2_score", metrics.r2_score)
                    if metrics.training_time_seconds:
                        mlflow.log_metric("training_time_seconds", metrics.training_time_seconds)
                
                # Log du mod√®le
                if hasattr(analyzer, '_models') and model_id in analyzer._models:
                    model_info = analyzer._models[model_id]
                    if config.model_type in [ModelType.LSTM, ModelType.VAE]:
                        mlflow.tensorflow.log_model(model_info['model'], "model")
                    elif config.model_type in [ModelType.ISOLATION_FOREST]:
                        mlflow.sklearn.log_model(model_info['model'], "model")
                
        except Exception as e:
            self.logger.warning(f"Erreur logging MLflow: {e}")
    
    async def predict(
        self,
        model_id: str,
        data: pd.DataFrame,
        **kwargs
    ) -> PredictionResponse:
        """Pr√©diction avec mod√®le enregistr√©"""
        start_time = time.time()
        request_id = str(uuid.uuid4())
        
        try:
            if model_id not in self._model_registry:
                raise ValueError(f"Mod√®le {model_id} non trouv√© dans le registre")
            
            registry_entry = self._model_registry[model_id]
            analyzer = registry_entry['analyzer']
            config = registry_entry['config']
            
            # Pr√©diction selon le type d'analytics
            if config.analytics_type == AnalyticsType.TIME_SERIES:
                result = await analyzer.predict(model_id, data, **kwargs)
                prediction = result['predictions']
            elif config.analytics_type == AnalyticsType.ANOMALY_DETECTION:
                result = await analyzer.detect_anomalies(model_id, data)
                prediction = result['anomalies']
            else:
                raise ValueError(f"Type d'analytics {config.analytics_type} non support√© pour pr√©diction")
            
            # Temps de pr√©diction
            prediction_time = (time.time() - start_time) * 1000
            
            # Mise √† jour m√©triques
            self._global_metrics["active_predictions"] += 1
            self._global_metrics["avg_prediction_time_ms"] = (
                (self._global_metrics["avg_prediction_time_ms"] * (self._global_metrics["active_predictions"] - 1) + prediction_time) /
                self._global_metrics["active_predictions"]
            )
            
            if MONITORING_AVAILABLE:
                self._prediction_counter.inc()
                self._prediction_latency.observe(prediction_time / 1000)
            
            return PredictionResponse(
                request_id=request_id,
                model_id=model_id,
                prediction=prediction,
                prediction_time_ms=prediction_time,
                timestamp=datetime.utcnow()
            )
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur pr√©diction {model_id}: {e}")
            raise
    
    async def get_model_metrics(self, model_id: str) -> Optional[ModelMetrics]:
        """R√©cup√©ration des m√©triques d'un mod√®le"""
        if model_id not in self._model_registry:
            return None
        
        registry_entry = self._model_registry[model_id]
        analyzer = registry_entry['analyzer']
        
        if hasattr(analyzer, '_metrics') and model_id in analyzer._metrics:
            return analyzer._metrics[model_id]
        
        return None
    
    async def get_system_status(self) -> Dict[str, Any]:
        """√âtat global du syst√®me analytics"""
        dependencies = self._check_dependencies()
        
        # Calcul de la pr√©cision moyenne
        total_accuracy = 0.0
        model_count = 0
        
        for model_id in self._model_registry:
            metrics = await self.get_model_metrics(model_id)
            if metrics and metrics.r2_score:
                total_accuracy += metrics.r2_score
                model_count += 1
        
        if model_count > 0:
            self._global_metrics["model_accuracy_avg"] = total_accuracy / model_count
        
        return {
            "dependencies": dependencies,
            "global_metrics": self._global_metrics,
            "registered_models": len(self._model_registry),
            "model_types": list(set(
                entry['config'].model_type.name 
                for entry in self._model_registry.values()
            )),
            "analytics_types": list(set(
                entry['config'].analytics_type.name 
                for entry in self._model_registry.values()
            )),
            "timestamp": datetime.utcnow().isoformat()
        }
    
    async def shutdown(self):
        """Arr√™t propre du moteur analytics"""
        self.logger.info("üîÑ Arr√™t Analytics Engine...")
        
        # Nettoyage des analyseurs
        self._time_series_analyzer = None
        self._anomaly_detector = None
        
        # Nettoyage du registre
        self._model_registry.clear()
        self._active_models.clear()
        
        self.logger.info("‚úÖ Analytics Engine arr√™t√©")

# =============================================================================
# UTILITAIRES ET EXPORTS
# =============================================================================

async def create_analytics_engine() -> AnalyticsEngine:
    """Cr√©ation et initialisation du moteur analytics"""
    engine = AnalyticsEngine()
    await engine.initialize()
    return engine

__all__ = [
    # Classes principales
    "AnalyticsEngine",
    "TimeSeriesAnalyzer",
    "AnomalyDetector",
    
    # Mod√®les
    "AnalyticsConfig",
    "ModelMetrics",
    "PredictionRequest",
    "PredictionResponse",
    
    # Enums
    "AnalyticsType",
    "ModelType",
    "FeatureType",
    "ModelStatus",
    
    # Utilitaires
    "create_analytics_engine"
]
