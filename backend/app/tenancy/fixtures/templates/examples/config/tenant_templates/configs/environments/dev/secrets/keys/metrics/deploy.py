#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Automated Metrics System Configuration & Deployment
=================================================

Ultra-advanced automated configuration and deployment system for enterprise
metrics infrastructure with intelligent auto-configuration, health checks,
and production-ready deployment orchestration.

Expert Development Team:
- Lead Dev + AI Architect
- Senior Backend Developer (Python/FastAPI/Django)
- ML Engineer (TensorFlow/PyTorch/Hugging Face)
- DBA & Data Engineer (PostgreSQL/Redis/MongoDB)
- Backend Security Specialist
- Microservices Architect
"""

import asyncio
import json
import logging
import os
import sys
import time
import shutil
import tempfile
import subprocess
import socket
import platform
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass, field, asdict
from enum import Enum
import argparse
import yaml
import hashlib

# Ajout du chemin parent pour les imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from . import EnterpriseMetricsSystem, MetricDataPoint, get_metrics_system
from .collector import CollectorConfig, MetricsCollectionAgent

try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False

try:
    import docker
    HAS_DOCKER = True
except ImportError:
    HAS_DOCKER = False

try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False


class DeploymentMode(Enum):
    """Modes de d√©ploiement."""
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION = "production"
    TESTING = "testing"
    DEMO = "demo"


class InfrastructureType(Enum):
    """Types d'infrastructure."""
    STANDALONE = "standalone"
    DOCKER = "docker"
    KUBERNETES = "kubernetes"
    CLOUD_AWS = "cloud_aws"
    CLOUD_GCP = "cloud_gcp"
    CLOUD_AZURE = "cloud_azure"


@dataclass
class DeploymentConfig:
    """Configuration de d√©ploiement."""
    
    # Identification
    deployment_id: str = field(default_factory=lambda: f"metrics-{int(time.time())}")
    deployment_name: str = "enterprise-metrics-system"
    version: str = "1.0.0"
    
    # Mode et infrastructure
    mode: DeploymentMode = DeploymentMode.DEVELOPMENT
    infrastructure: InfrastructureType = InfrastructureType.STANDALONE
    
    # Chemins et r√©pertoires
    base_dir: str = "/opt/metrics-system"
    config_dir: str = "/etc/metrics-system"
    data_dir: str = "/var/lib/metrics-system"
    log_dir: str = "/var/log/metrics-system"
    pid_dir: str = "/run/metrics-system"
    
    # Configuration r√©seau
    bind_host: str = "0.0.0.0"
    bind_port: int = 8080
    metrics_port: int = 9090
    health_port: int = 8081
    
    # Base de donn√©es
    database_type: str = "sqlite"  # sqlite, postgresql, redis
    database_url: str = ""
    database_pool_size: int = 10
    
    # S√©curit√©
    enable_tls: bool = False
    cert_file: str = ""
    key_file: str = ""
    ca_file: str = ""
    
    # Performance
    worker_processes: int = 1
    max_connections: int = 1000
    memory_limit_mb: int = 1024
    cpu_limit_cores: float = 2.0
    
    # Monitoring
    enable_monitoring: bool = True
    metrics_retention_days: int = 30
    backup_enabled: bool = True
    backup_interval_hours: int = 24
    
    # Alerting
    enable_alerting: bool = True
    smtp_server: str = ""
    smtp_port: int = 587
    smtp_username: str = ""
    smtp_password: str = ""
    alert_recipients: List[str] = field(default_factory=list)
    
    # Int√©grations
    enable_prometheus: bool = False
    enable_grafana: bool = False
    enable_elasticsearch: bool = False
    
    # Auto-configuration
    auto_detect_hardware: bool = True
    auto_tune_performance: bool = True
    auto_setup_systemd: bool = True
    auto_setup_nginx: bool = False


class SystemRequirements:
    """V√©rification et validation des pr√©requis syst√®me."""
    
    @staticmethod
    def check_python_version() -> Tuple[bool, str]:
        """V√©rifie la version Python."""
        required_version = (3, 8)
        current_version = sys.version_info[:2]
        
        if current_version >= required_version:
            return True, f"Python {sys.version} ‚úì"
        else:
            return False, f"Python {current_version} requis >= {required_version}"
    
    @staticmethod
    def check_system_resources() -> Dict[str, Any]:
        """V√©rifie les ressources syst√®me."""
        resources = {
            "cpu_count": 1,
            "memory_gb": 1.0,
            "disk_space_gb": 10.0,
            "load_average": 0.0
        }
        
        try:
            if HAS_PSUTIL:
                resources.update({
                    "cpu_count": psutil.cpu_count(),
                    "memory_gb": psutil.virtual_memory().total / (1024**3),
                    "disk_space_gb": psutil.disk_usage('/').free / (1024**3),
                    "load_average": os.getloadavg()[0] if hasattr(os, 'getloadavg') else 0.0
                })
        except Exception as e:
            logging.warning(f"Erreur v√©rification ressources: {e}")
        
        return resources
    
    @staticmethod
    def check_network_ports(ports: List[int]) -> Dict[int, bool]:
        """V√©rifie la disponibilit√© des ports r√©seau."""
        results = {}
        
        for port in ports:
            try:
                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                    sock.settimeout(1)
                    result = sock.connect_ex(('127.0.0.1', port))
                    results[port] = result != 0  # Port libre si connexion √©choue
            except Exception:
                results[port] = False
        
        return results
    
    @staticmethod
    def check_permissions(directories: List[str]) -> Dict[str, bool]:
        """V√©rifie les permissions d'√©criture."""
        results = {}
        
        for directory in directories:
            try:
                # Teste la cr√©ation d'un fichier temporaire
                test_file = os.path.join(directory, f".test_{int(time.time())}")
                with open(test_file, 'w') as f:
                    f.write("test")
                os.remove(test_file)
                results[directory] = True
            except Exception:
                results[directory] = False
        
        return results
    
    @staticmethod
    def check_dependencies() -> Dict[str, bool]:
        """V√©rifie les d√©pendances Python."""
        dependencies = {
            "psutil": HAS_PSUTIL,
            "requests": HAS_REQUESTS,
            "docker": HAS_DOCKER,
            "asyncio": True,  # Int√©gr√© √† Python 3.7+
            "json": True,     # Int√©gr√©
            "sqlite3": True   # Int√©gr√©
        }
        
        # V√©rification de modules optionnels
        try:
            import redis
            dependencies["redis"] = True
        except ImportError:
            dependencies["redis"] = False
        
        try:
            import postgresql
            dependencies["postgresql"] = True
        except ImportError:
            dependencies["postgresql"] = False
        
        try:
            import prometheus_client
            dependencies["prometheus_client"] = True
        except ImportError:
            dependencies["prometheus_client"] = False
        
        return dependencies


class ConfigurationGenerator:
    """G√©n√©rateur de configuration automatique."""
    
    def __init__(self, deployment_config: DeploymentConfig):
        self.deployment_config = deployment_config
        
    def generate_base_config(self) -> Dict[str, Any]:
        """G√©n√®re la configuration de base."""
        resources = SystemRequirements.check_system_resources()
        
        # Auto-tuning bas√© sur les ressources
        if self.deployment_config.auto_tune_performance:
            self._auto_tune_performance(resources)
        
        config = {
            "deployment": asdict(self.deployment_config),
            "system": {
                "resources": resources,
                "platform": platform.platform(),
                "hostname": socket.gethostname(),
                "python_version": sys.version
            },
            "collector": self._generate_collector_config(resources),
            "storage": self._generate_storage_config(),
            "monitoring": self._generate_monitoring_config(),
            "security": self._generate_security_config(),
            "networking": self._generate_networking_config()
        }
        
        return config
    
    def _auto_tune_performance(self, resources: Dict[str, Any]):
        """Auto-tuning des performances bas√© sur les ressources."""
        cpu_count = resources.get("cpu_count", 1)
        memory_gb = resources.get("memory_gb", 1.0)
        
        # Ajustement du nombre de workers
        if cpu_count >= 8:
            self.deployment_config.worker_processes = min(cpu_count - 2, 16)
        elif cpu_count >= 4:
            self.deployment_config.worker_processes = cpu_count - 1
        else:
            self.deployment_config.worker_processes = 1
        
        # Ajustement de la m√©moire
        if memory_gb >= 16:
            self.deployment_config.memory_limit_mb = 4096
        elif memory_gb >= 8:
            self.deployment_config.memory_limit_mb = 2048
        elif memory_gb >= 4:
            self.deployment_config.memory_limit_mb = 1024
        else:
            self.deployment_config.memory_limit_mb = 512
        
        # Ajustement des limites CPU
        self.deployment_config.cpu_limit_cores = min(cpu_count * 0.8, 8.0)
        
        # Ajustement des connexions
        if memory_gb >= 8:
            self.deployment_config.max_connections = 2000
        elif memory_gb >= 4:
            self.deployment_config.max_connections = 1000
        else:
            self.deployment_config.max_connections = 500
    
    def _generate_collector_config(self, resources: Dict[str, Any]) -> Dict[str, Any]:
        """G√©n√®re la configuration du collecteur."""
        cpu_count = resources.get("cpu_count", 1)
        
        # Intervalles adaptatifs bas√©s sur les ressources
        if cpu_count >= 8:
            system_interval = 30
            performance_interval = 15
        elif cpu_count >= 4:
            system_interval = 45
            performance_interval = 20
        else:
            system_interval = 60
            performance_interval = 30
        
        return {
            "system_interval": system_interval,
            "security_interval": 300,
            "performance_interval": performance_interval,
            "network_interval": 120,
            "storage_interval": 180,
            "max_concurrent_collectors": min(cpu_count, 10),
            "adaptive_sampling": True,
            "intelligent_batching": True,
            "compression_enabled": True
        }
    
    def _generate_storage_config(self) -> Dict[str, Any]:
        """G√©n√®re la configuration de stockage."""
        config = {
            "type": self.deployment_config.database_type,
            "retention_days": self.deployment_config.metrics_retention_days,
            "backup_enabled": self.deployment_config.backup_enabled,
            "backup_interval_hours": self.deployment_config.backup_interval_hours
        }
        
        if self.deployment_config.database_type == "sqlite":
            config.update({
                "db_path": os.path.join(self.deployment_config.data_dir, "metrics.db"),
                "connection_pool_size": 5,
                "wal_mode": True,
                "synchronous": "NORMAL"
            })
        elif self.deployment_config.database_type == "redis":
            config.update({
                "redis_url": self.deployment_config.database_url or "redis://localhost:6379/0",
                "connection_pool_size": self.deployment_config.database_pool_size,
                "max_connections": 100
            })
        elif self.deployment_config.database_type == "postgresql":
            config.update({
                "postgresql_url": self.deployment_config.database_url,
                "connection_pool_size": self.deployment_config.database_pool_size,
                "max_connections": self.deployment_config.max_connections
            })
        
        return config
    
    def _generate_monitoring_config(self) -> Dict[str, Any]:
        """G√©n√®re la configuration de monitoring."""
        return {
            "enabled": self.deployment_config.enable_monitoring,
            "health_check_port": self.deployment_config.health_port,
            "metrics_port": self.deployment_config.metrics_port,
            "prometheus_enabled": self.deployment_config.enable_prometheus,
            "grafana_enabled": self.deployment_config.enable_grafana,
            "elasticsearch_enabled": self.deployment_config.enable_elasticsearch,
            "health_check_interval": 30,
            "metrics_export_interval": 60
        }
    
    def _generate_security_config(self) -> Dict[str, Any]:
        """G√©n√®re la configuration de s√©curit√©."""
        return {
            "tls_enabled": self.deployment_config.enable_tls,
            "cert_file": self.deployment_config.cert_file,
            "key_file": self.deployment_config.key_file,
            "ca_file": self.deployment_config.ca_file,
            "auth_required": False,  # Configurable selon les besoins
            "api_key_required": False,
            "rate_limiting": {
                "enabled": True,
                "requests_per_minute": 1000,
                "burst_size": 100
            }
        }
    
    def _generate_networking_config(self) -> Dict[str, Any]:
        """G√©n√®re la configuration r√©seau."""
        return {
            "bind_host": self.deployment_config.bind_host,
            "bind_port": self.deployment_config.bind_port,
            "max_connections": self.deployment_config.max_connections,
            "keep_alive_timeout": 30,
            "request_timeout": 60,
            "cors_enabled": True,
            "cors_origins": ["*"]
        }


class DeploymentOrchestrator:
    """Orchestrateur de d√©ploiement automatis√©."""
    
    def __init__(self, deployment_config: DeploymentConfig):
        self.deployment_config = deployment_config
        self.config_generator = ConfigurationGenerator(deployment_config)
        
    async def deploy(self) -> bool:
        """D√©ploie le syst√®me de m√©triques."""
        try:
            logging.info("üöÄ D√©marrage du d√©ploiement du syst√®me de m√©triques")
            
            # 1. V√©rification des pr√©requis
            if not await self._check_prerequisites():
                return False
            
            # 2. Cr√©ation de la structure de r√©pertoires
            if not await self._create_directory_structure():
                return False
            
            # 3. G√©n√©ration des configurations
            if not await self._generate_configurations():
                return False
            
            # 4. Installation des d√©pendances
            if not await self._install_dependencies():
                return False
            
            # 5. Configuration des services syst√®me
            if not await self._setup_system_services():
                return False
            
            # 6. D√©ploiement des composants
            if not await self._deploy_components():
                return False
            
            # 7. Tests de sant√©
            if not await self._run_health_checks():
                return False
            
            # 8. Finalisation
            await self._finalize_deployment()
            
            logging.info("‚úÖ D√©ploiement termin√© avec succ√®s")
            return True
            
        except Exception as e:
            logging.error(f"‚ùå Erreur lors du d√©ploiement: {e}")
            await self._rollback_deployment()
            return False
    
    async def _check_prerequisites(self) -> bool:
        """V√©rifie les pr√©requis syst√®me."""
        logging.info("üìã V√©rification des pr√©requis...")
        
        # Version Python
        python_ok, python_msg = SystemRequirements.check_python_version()
        logging.info(f"Python: {python_msg}")
        if not python_ok:
            return False
        
        # Ressources syst√®me
        resources = SystemRequirements.check_system_resources()
        logging.info(f"CPU: {resources['cpu_count']} cores")
        logging.info(f"M√©moire: {resources['memory_gb']:.1f} GB")
        logging.info(f"Espace disque: {resources['disk_space_gb']:.1f} GB")
        
        # V√©rifications minimales
        if resources['memory_gb'] < 1.0:
            logging.error("‚ùå M√©moire insuffisante (minimum 1GB)")
            return False
        
        if resources['disk_space_gb'] < 5.0:
            logging.error("‚ùå Espace disque insuffisant (minimum 5GB)")
            return False
        
        # Ports r√©seau
        required_ports = [
            self.deployment_config.bind_port,
            self.deployment_config.metrics_port,
            self.deployment_config.health_port
        ]
        
        port_availability = SystemRequirements.check_network_ports(required_ports)
        for port, available in port_availability.items():
            if not available:
                logging.error(f"‚ùå Port {port} non disponible")
                return False
            logging.info(f"Port {port}: ‚úì")
        
        # D√©pendances
        dependencies = SystemRequirements.check_dependencies()
        missing_deps = [dep for dep, available in dependencies.items() if not available]
        
        if missing_deps:
            logging.warning(f"‚ö†Ô∏è  D√©pendances manquantes: {missing_deps}")
            # Certaines d√©pendances sont optionnelles
        
        logging.info("‚úÖ Pr√©requis valid√©s")
        return True
    
    async def _create_directory_structure(self) -> bool:
        """Cr√©e la structure de r√©pertoires."""
        logging.info("üìÅ Cr√©ation de la structure de r√©pertoires...")
        
        directories = [
            self.deployment_config.base_dir,
            self.deployment_config.config_dir,
            self.deployment_config.data_dir,
            self.deployment_config.log_dir,
            self.deployment_config.pid_dir,
            os.path.join(self.deployment_config.config_dir, "ssl"),
            os.path.join(self.deployment_config.data_dir, "backups"),
            os.path.join(self.deployment_config.log_dir, "archive")
        ]
        
        for directory in directories:
            try:
                os.makedirs(directory, mode=0o755, exist_ok=True)
                logging.debug(f"R√©pertoire cr√©√©: {directory}")
            except Exception as e:
                logging.error(f"‚ùå Erreur cr√©ation r√©pertoire {directory}: {e}")
                return False
        
        logging.info("‚úÖ Structure de r√©pertoires cr√©√©e")
        return True
    
    async def _generate_configurations(self) -> bool:
        """G√©n√®re les fichiers de configuration."""
        logging.info("‚öôÔ∏è  G√©n√©ration des configurations...")
        
        try:
            # Configuration principale
            main_config = self.config_generator.generate_base_config()
            
            # Sauvegarde des configurations
            config_files = {
                "main.json": main_config,
                "collector.json": main_config["collector"],
                "storage.json": main_config["storage"],
                "monitoring.json": main_config["monitoring"],
                "security.json": main_config["security"]
            }
            
            for filename, config_data in config_files.items():
                config_path = os.path.join(self.deployment_config.config_dir, filename)
                
                with open(config_path, 'w') as f:
                    json.dump(config_data, f, indent=2, default=str)
                
                # Permissions restrictives pour les fichiers de config
                os.chmod(config_path, 0o640)
                logging.debug(f"Configuration sauv√©e: {config_path}")
            
            # Configuration YAML pour Docker/Kubernetes
            if self.deployment_config.infrastructure in [InfrastructureType.DOCKER, InfrastructureType.KUBERNETES]:
                await self._generate_container_configs(main_config)
            
            logging.info("‚úÖ Configurations g√©n√©r√©es")
            return True
            
        except Exception as e:
            logging.error(f"‚ùå Erreur g√©n√©ration configurations: {e}")
            return False
    
    async def _generate_container_configs(self, main_config: Dict[str, Any]):
        """G√©n√®re les configurations pour conteneurs."""
        
        # Docker Compose
        if self.deployment_config.infrastructure == InfrastructureType.DOCKER:
            docker_compose = {
                "version": "3.8",
                "services": {
                    "metrics-system": {
                        "image": "metrics-system:latest",
                        "ports": [
                            f"{self.deployment_config.bind_port}:8080",
                            f"{self.deployment_config.metrics_port}:9090",
                            f"{self.deployment_config.health_port}:8081"
                        ],
                        "volumes": [
                            f"{self.deployment_config.data_dir}:/var/lib/metrics-system",
                            f"{self.deployment_config.config_dir}:/etc/metrics-system:ro"
                        ],
                        "environment": {
                            "METRICS_CONFIG_DIR": "/etc/metrics-system",
                            "METRICS_DATA_DIR": "/var/lib/metrics-system"
                        },
                        "restart": "unless-stopped",
                        "healthcheck": {
                            "test": f"curl -f http://localhost:{self.deployment_config.health_port}/health || exit 1",
                            "interval": "30s",
                            "timeout": "10s",
                            "retries": 3
                        }
                    }
                }
            }
            
            # Ajout de Redis si configur√©
            if self.deployment_config.database_type == "redis":
                docker_compose["services"]["redis"] = {
                    "image": "redis:7-alpine",
                    "ports": ["6379:6379"],
                    "volumes": [f"{self.deployment_config.data_dir}/redis:/data"],
                    "restart": "unless-stopped"
                }
            
            # Sauvegarde du Docker Compose
            compose_path = os.path.join(self.deployment_config.config_dir, "docker-compose.yml")
            with open(compose_path, 'w') as f:
                yaml.dump(docker_compose, f, default_flow_style=False)
        
        # Configuration Kubernetes
        elif self.deployment_config.infrastructure == InfrastructureType.KUBERNETES:
            k8s_configs = self._generate_kubernetes_configs(main_config)
            
            for config_name, config_data in k8s_configs.items():
                config_path = os.path.join(self.deployment_config.config_dir, f"k8s-{config_name}.yaml")
                with open(config_path, 'w') as f:
                    yaml.dump(config_data, f, default_flow_style=False)
    
    def _generate_kubernetes_configs(self, main_config: Dict[str, Any]) -> Dict[str, Dict]:
        """G√©n√®re les configurations Kubernetes."""
        configs = {}
        
        # ConfigMap
        configs["configmap"] = {
            "apiVersion": "v1",
            "kind": "ConfigMap",
            "metadata": {
                "name": "metrics-system-config",
                "namespace": "default"
            },
            "data": {
                "main.json": json.dumps(main_config, indent=2, default=str)
            }
        }
        
        # Deployment
        configs["deployment"] = {
            "apiVersion": "apps/v1",
            "kind": "Deployment",
            "metadata": {
                "name": "metrics-system",
                "namespace": "default"
            },
            "spec": {
                "replicas": 1,
                "selector": {
                    "matchLabels": {"app": "metrics-system"}
                },
                "template": {
                    "metadata": {
                        "labels": {"app": "metrics-system"}
                    },
                    "spec": {
                        "containers": [{
                            "name": "metrics-system",
                            "image": "metrics-system:latest",
                            "ports": [
                                {"containerPort": 8080, "name": "api"},
                                {"containerPort": 9090, "name": "metrics"},
                                {"containerPort": 8081, "name": "health"}
                            ],
                            "volumeMounts": [{
                                "name": "config",
                                "mountPath": "/etc/metrics-system"
                            }],
                            "livenessProbe": {
                                "httpGet": {
                                    "path": "/health",
                                    "port": 8081
                                },
                                "initialDelaySeconds": 30,
                                "periodSeconds": 10
                            },
                            "readinessProbe": {
                                "httpGet": {
                                    "path": "/ready",
                                    "port": 8081
                                },
                                "initialDelaySeconds": 5,
                                "periodSeconds": 5
                            }
                        }],
                        "volumes": [{
                            "name": "config",
                            "configMap": {"name": "metrics-system-config"}
                        }]
                    }
                }
            }
        }
        
        # Service
        configs["service"] = {
            "apiVersion": "v1",
            "kind": "Service",
            "metadata": {
                "name": "metrics-system-service",
                "namespace": "default"
            },
            "spec": {
                "selector": {"app": "metrics-system"},
                "ports": [
                    {"port": 8080, "targetPort": 8080, "name": "api"},
                    {"port": 9090, "targetPort": 9090, "name": "metrics"},
                    {"port": 8081, "targetPort": 8081, "name": "health"}
                ],
                "type": "ClusterIP"
            }
        }
        
        return configs
    
    async def _install_dependencies(self) -> bool:
        """Installe les d√©pendances."""
        logging.info("üì¶ Installation des d√©pendances...")
        
        try:
            # Installation via pip des d√©pendances optionnelles
            optional_deps = []
            
            if self.deployment_config.database_type == "redis":
                optional_deps.append("redis")
            
            if self.deployment_config.database_type == "postgresql":
                optional_deps.extend(["psycopg2-binary", "sqlalchemy"])
            
            if self.deployment_config.enable_prometheus:
                optional_deps.append("prometheus-client")
            
            if optional_deps:
                cmd = [sys.executable, "-m", "pip", "install"] + optional_deps
                result = subprocess.run(cmd, capture_output=True, text=True)
                
                if result.returncode != 0:
                    logging.warning(f"‚ö†Ô∏è  √âchec installation d√©pendances optionnelles: {result.stderr}")
                else:
                    logging.info(f"‚úÖ D√©pendances install√©es: {optional_deps}")
            
            return True
            
        except Exception as e:
            logging.error(f"‚ùå Erreur installation d√©pendances: {e}")
            return False
    
    async def _setup_system_services(self) -> bool:
        """Configure les services syst√®me."""
        logging.info("üîß Configuration des services syst√®me...")
        
        try:
            # Service systemd (Linux uniquement)
            if platform.system() == "Linux" and self.deployment_config.auto_setup_systemd:
                await self._create_systemd_service()
            
            # Configuration Nginx (optionnel)
            if self.deployment_config.auto_setup_nginx:
                await self._create_nginx_config()
            
            logging.info("‚úÖ Services syst√®me configur√©s")
            return True
            
        except Exception as e:
            logging.error(f"‚ùå Erreur configuration services: {e}")
            return False
    
    async def _create_systemd_service(self):
        """Cr√©e le service systemd."""
        service_content = f"""[Unit]
Description=Enterprise Metrics System
After=network.target

[Service]
Type=exec
User=metrics
Group=metrics
WorkingDirectory={self.deployment_config.base_dir}
ExecStart={sys.executable} -m metrics_collector --config {self.deployment_config.config_dir}/main.json --daemon
ExecReload=/bin/kill -HUP $MAINPID
Restart=always
RestartSec=5
StandardOutput=journal
StandardError=journal
SyslogIdentifier=metrics-system
PIDFile={self.deployment_config.pid_dir}/metrics-system.pid

[Install]
WantedBy=multi-user.target
"""
        
        service_path = "/etc/systemd/system/metrics-system.service"
        
        try:
            with open(service_path, 'w') as f:
                f.write(service_content)
            
            # Rechargement systemd
            subprocess.run(["systemctl", "daemon-reload"], check=True)
            subprocess.run(["systemctl", "enable", "metrics-system"], check=True)
            
            logging.info("‚úÖ Service systemd cr√©√© et activ√©")
            
        except (PermissionError, subprocess.CalledProcessError) as e:
            logging.warning(f"‚ö†Ô∏è  Impossible de cr√©er le service systemd: {e}")
    
    async def _create_nginx_config(self):
        """Cr√©e la configuration Nginx."""
        nginx_config = f"""
upstream metrics_backend {{
    server 127.0.0.1:{self.deployment_config.bind_port};
}}

server {{
    listen 80;
    server_name metrics.example.com;
    
    location / {{
        proxy_pass http://metrics_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }}
    
    location /health {{
        proxy_pass http://127.0.0.1:{self.deployment_config.health_port}/health;
    }}
    
    location /metrics {{
        proxy_pass http://127.0.0.1:{self.deployment_config.metrics_port}/metrics;
    }}
}}
"""
        
        nginx_path = f"{self.deployment_config.config_dir}/nginx.conf"
        
        try:
            with open(nginx_path, 'w') as f:
                f.write(nginx_config)
            
            logging.info(f"‚úÖ Configuration Nginx cr√©√©e: {nginx_path}")
            
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è  Erreur cr√©ation config Nginx: {e}")
    
    async def _deploy_components(self) -> bool:
        """D√©ploie les composants du syst√®me."""
        logging.info("üö¢ D√©ploiement des composants...")
        
        try:
            if self.deployment_config.infrastructure == InfrastructureType.DOCKER:
                return await self._deploy_docker()
            elif self.deployment_config.infrastructure == InfrastructureType.KUBERNETES:
                return await self._deploy_kubernetes()
            else:
                return await self._deploy_standalone()
            
        except Exception as e:
            logging.error(f"‚ùå Erreur d√©ploiement composants: {e}")
            return False
    
    async def _deploy_standalone(self) -> bool:
        """D√©ploie en mode standalone."""
        logging.info("üñ•Ô∏è  D√©ploiement standalone...")
        
        # Copie des scripts et modules
        current_dir = os.path.dirname(os.path.abspath(__file__))
        
        # Copie des fichiers Python
        src_files = [
            "__init__.py",
            "collector.py",
            "config.py"
        ]
        
        for src_file in src_files:
            src_path = os.path.join(current_dir, src_file)
            dst_path = os.path.join(self.deployment_config.base_dir, src_file)
            
            if os.path.exists(src_path):
                shutil.copy2(src_path, dst_path)
                logging.debug(f"Fichier copi√©: {src_file}")
        
        return True
    
    async def _deploy_docker(self) -> bool:
        """D√©ploie avec Docker."""
        logging.info("üê≥ D√©ploiement Docker...")
        
        if not HAS_DOCKER:
            logging.error("‚ùå Module docker non disponible")
            return False
        
        try:
            # V√©rification Docker
            client = docker.from_env()
            client.ping()
            
            # D√©marrage des conteneurs
            compose_path = os.path.join(self.deployment_config.config_dir, "docker-compose.yml")
            
            if os.path.exists(compose_path):
                cmd = ["docker-compose", "-f", compose_path, "up", "-d"]
                result = subprocess.run(cmd, capture_output=True, text=True)
                
                if result.returncode == 0:
                    logging.info("‚úÖ Conteneurs Docker d√©marr√©s")
                    return True
                else:
                    logging.error(f"‚ùå Erreur Docker Compose: {result.stderr}")
                    return False
            
            return True
            
        except Exception as e:
            logging.error(f"‚ùå Erreur d√©ploiement Docker: {e}")
            return False
    
    async def _deploy_kubernetes(self) -> bool:
        """D√©ploie sur Kubernetes."""
        logging.info("‚ò∏Ô∏è  D√©ploiement Kubernetes...")
        
        try:
            # Application des configurations K8s
            config_dir = self.deployment_config.config_dir
            k8s_files = [f for f in os.listdir(config_dir) if f.startswith("k8s-") and f.endswith(".yaml")]
            
            for k8s_file in k8s_files:
                file_path = os.path.join(config_dir, k8s_file)
                cmd = ["kubectl", "apply", "-f", file_path]
                
                result = subprocess.run(cmd, capture_output=True, text=True)
                
                if result.returncode == 0:
                    logging.info(f"‚úÖ Configuration K8s appliqu√©e: {k8s_file}")
                else:
                    logging.error(f"‚ùå Erreur K8s {k8s_file}: {result.stderr}")
                    return False
            
            return True
            
        except Exception as e:
            logging.error(f"‚ùå Erreur d√©ploiement Kubernetes: {e}")
            return False
    
    async def _run_health_checks(self) -> bool:
        """Ex√©cute les tests de sant√©."""
        logging.info("üè• Tests de sant√© du syst√®me...")
        
        try:
            # Attente du d√©marrage
            await asyncio.sleep(10)
            
            # Test de connectivit√©
            health_url = f"http://127.0.0.1:{self.deployment_config.health_port}/health"
            
            if HAS_REQUESTS:
                try:
                    response = requests.get(health_url, timeout=5)
                    if response.status_code == 200:
                        logging.info("‚úÖ Test de sant√© r√©ussi")
                        return True
                    else:
                        logging.error(f"‚ùå Test de sant√© √©chou√©: {response.status_code}")
                        return False
                except requests.RequestException as e:
                    logging.error(f"‚ùå Erreur test de sant√©: {e}")
                    return False
            else:
                # Test basique de connectivit√© socket
                try:
                    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                        sock.settimeout(5)
                        result = sock.connect_ex(('127.0.0.1', self.deployment_config.health_port))
                        if result == 0:
                            logging.info("‚úÖ Test de connectivit√© r√©ussi")
                            return True
                        else:
                            logging.error(f"‚ùå Test de connectivit√© √©chou√©")
                            return False
                except Exception as e:
                    logging.error(f"‚ùå Erreur test connectivit√©: {e}")
                    return False
            
        except Exception as e:
            logging.error(f"‚ùå Erreur tests de sant√©: {e}")
            return False
    
    async def _finalize_deployment(self):
        """Finalise le d√©ploiement."""
        logging.info("üéØ Finalisation du d√©ploiement...")
        
        # G√©n√©ration du rapport de d√©ploiement
        deployment_report = {
            "deployment_id": self.deployment_config.deployment_id,
            "deployment_name": self.deployment_config.deployment_name,
            "version": self.deployment_config.version,
            "timestamp": datetime.now().isoformat(),
            "mode": self.deployment_config.mode.value,
            "infrastructure": self.deployment_config.infrastructure.value,
            "endpoints": {
                "api": f"http://127.0.0.1:{self.deployment_config.bind_port}",
                "metrics": f"http://127.0.0.1:{self.deployment_config.metrics_port}",
                "health": f"http://127.0.0.1:{self.deployment_config.health_port}"
            },
            "configuration": {
                "config_dir": self.deployment_config.config_dir,
                "data_dir": self.deployment_config.data_dir,
                "log_dir": self.deployment_config.log_dir
            },
            "resources": SystemRequirements.check_system_resources()
        }
        
        # Sauvegarde du rapport
        report_path = os.path.join(self.deployment_config.config_dir, "deployment-report.json")
        with open(report_path, 'w') as f:
            json.dump(deployment_report, f, indent=2, default=str)
        
        logging.info(f"üìä Rapport de d√©ploiement: {report_path}")
        
        # Affichage des informations de d√©ploiement
        print("\n" + "="*60)
        print("üéâ D√âPLOIEMENT TERMIN√â AVEC SUCC√àS")
        print("="*60)
        print(f"üìù ID de d√©ploiement: {self.deployment_config.deployment_id}")
        print(f"üè∑Ô∏è  Nom: {self.deployment_config.deployment_name}")
        print(f"üìä Version: {self.deployment_config.version}")
        print(f"üåç Mode: {self.deployment_config.mode.value}")
        print(f"üèóÔ∏è  Infrastructure: {self.deployment_config.infrastructure.value}")
        print("\nüìç Points d'acc√®s:")
        print(f"   ‚Ä¢ API: http://127.0.0.1:{self.deployment_config.bind_port}")
        print(f"   ‚Ä¢ M√©triques: http://127.0.0.1:{self.deployment_config.metrics_port}")
        print(f"   ‚Ä¢ Sant√©: http://127.0.0.1:{self.deployment_config.health_port}")
        print("\nüìÅ R√©pertoires:")
        print(f"   ‚Ä¢ Configuration: {self.deployment_config.config_dir}")
        print(f"   ‚Ä¢ Donn√©es: {self.deployment_config.data_dir}")
        print(f"   ‚Ä¢ Logs: {self.deployment_config.log_dir}")
        print("="*60)
    
    async def _rollback_deployment(self):
        """Effectue un rollback en cas d'√©chec."""
        logging.warning("üîÑ Rollback du d√©ploiement...")
        
        try:
            # Arr√™t des services
            if self.deployment_config.infrastructure == InfrastructureType.DOCKER:
                compose_path = os.path.join(self.deployment_config.config_dir, "docker-compose.yml")
                if os.path.exists(compose_path):
                    subprocess.run(["docker-compose", "-f", compose_path, "down"], 
                                 capture_output=True)
            
            # Nettoyage des fichiers cr√©√©s
            # Attention: ne pas supprimer les r√©pertoires de donn√©es
            logging.info("üßπ Nettoyage effectu√©")
            
        except Exception as e:
            logging.error(f"‚ùå Erreur lors du rollback: {e}")


async def main():
    """Fonction principale de configuration et d√©ploiement."""
    parser = argparse.ArgumentParser(description="Configuration et d√©ploiement automatis√© du syst√®me de m√©triques")
    
    # Configuration de base
    parser.add_argument("--mode", choices=["development", "staging", "production", "testing", "demo"],
                       default="development", help="Mode de d√©ploiement")
    parser.add_argument("--infrastructure", choices=["standalone", "docker", "kubernetes"],
                       default="standalone", help="Type d'infrastructure")
    parser.add_argument("--name", default="enterprise-metrics-system", help="Nom du d√©ploiement")
    
    # R√©pertoires
    parser.add_argument("--base-dir", default="/opt/metrics-system", help="R√©pertoire de base")
    parser.add_argument("--config-dir", default="/etc/metrics-system", help="R√©pertoire de configuration")
    parser.add_argument("--data-dir", default="/var/lib/metrics-system", help="R√©pertoire de donn√©es")
    parser.add_argument("--log-dir", default="/var/log/metrics-system", help="R√©pertoire de logs")
    
    # R√©seau
    parser.add_argument("--bind-host", default="0.0.0.0", help="Adresse d'√©coute")
    parser.add_argument("--bind-port", type=int, default=8080, help="Port d'√©coute API")
    parser.add_argument("--metrics-port", type=int, default=9090, help="Port m√©triques")
    parser.add_argument("--health-port", type=int, default=8081, help="Port sant√©")
    
    # Base de donn√©es
    parser.add_argument("--database", choices=["sqlite", "redis", "postgresql"],
                       default="sqlite", help="Type de base de donn√©es")
    parser.add_argument("--database-url", help="URL de la base de donn√©es")
    
    # Options
    parser.add_argument("--auto-tune", action="store_true", help="Auto-tuning des performances")
    parser.add_argument("--setup-systemd", action="store_true", help="Configuration service systemd")
    parser.add_argument("--setup-nginx", action="store_true", help="Configuration Nginx")
    parser.add_argument("--enable-monitoring", action="store_true", default=True, help="Activation monitoring")
    parser.add_argument("--enable-prometheus", action="store_true", help="Activation Prometheus")
    parser.add_argument("--enable-grafana", action="store_true", help="Activation Grafana")
    
    # Logs
    parser.add_argument("--log-level", default="INFO", help="Niveau de log")
    parser.add_argument("--log-file", help="Fichier de log")
    
    # Actions
    parser.add_argument("--dry-run", action="store_true", help="Simulation sans d√©ploiement")
    parser.add_argument("--generate-config-only", action="store_true", help="G√©n√©ration config uniquement")
    
    args = parser.parse_args()
    
    # Configuration du logging
    log_level = getattr(logging, args.log_level.upper())
    log_format = '%(asctime)s - %(levelname)s - %(message)s'
    
    if args.log_file:
        logging.basicConfig(level=log_level, format=log_format, filename=args.log_file)
    else:
        logging.basicConfig(level=log_level, format=log_format)
    
    # Configuration du d√©ploiement
    deployment_config = DeploymentConfig(
        deployment_name=args.name,
        mode=DeploymentMode(args.mode),
        infrastructure=InfrastructureType(args.infrastructure),
        base_dir=args.base_dir,
        config_dir=args.config_dir,
        data_dir=args.data_dir,
        log_dir=args.log_dir,
        bind_host=args.bind_host,
        bind_port=args.bind_port,
        metrics_port=args.metrics_port,
        health_port=args.health_port,
        database_type=args.database,
        database_url=args.database_url or "",
        auto_tune_performance=args.auto_tune,
        auto_setup_systemd=args.setup_systemd,
        auto_setup_nginx=args.setup_nginx,
        enable_monitoring=args.enable_monitoring,
        enable_prometheus=args.enable_prometheus,
        enable_grafana=args.enable_grafana
    )
    
    try:
        if args.generate_config_only:
            # G√©n√©ration de configuration uniquement
            logging.info("üìù G√©n√©ration de configuration uniquement")
            config_generator = ConfigurationGenerator(deployment_config)
            config = config_generator.generate_base_config()
            
            print(json.dumps(config, indent=2, default=str))
            
        elif args.dry_run:
            # Simulation
            logging.info("üß™ Mode simulation (dry-run)")
            orchestrator = DeploymentOrchestrator(deployment_config)
            
            # V√©rifications seulement
            await orchestrator._check_prerequisites()
            
        else:
            # D√©ploiement complet
            orchestrator = DeploymentOrchestrator(deployment_config)
            success = await orchestrator.deploy()
            
            if success:
                logging.info("üéâ D√©ploiement termin√© avec succ√®s")
                sys.exit(0)
            else:
                logging.error("üí• √âchec du d√©ploiement")
                sys.exit(1)
                
    except KeyboardInterrupt:
        logging.info("‚èπÔ∏è  D√©ploiement interrompu par l'utilisateur")
        sys.exit(1)
    except Exception as e:
        logging.error(f"üí• Erreur fatale: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
