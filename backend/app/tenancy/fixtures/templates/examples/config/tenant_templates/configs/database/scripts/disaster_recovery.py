#!/usr/bin/env python3
"""
Enterprise Database Disaster Recovery and Business Continuity Engine
===================================================================

Moteur ultra-avanc√© de disaster recovery et continuit√© d'activit√©
pour architectures multi-tenant de classe mondiale.

Fonctionnalit√©s:
- Plans de disaster recovery automatis√©s
- Basculement automatique (failover) et manuel
- R√©plication multi-site en temps r√©el
- Tests de DR automatis√©s et programm√©s
- Strat√©gies RTO/RPO configurables
- Monitoring de la sant√© des r√©plicas
- Orchestration intelligente des basculements
- Restoration automatique et validation
- Gestion des conflits de donn√©es
- Audit trail complet des op√©rations DR
- Notifications et alertes en temps r√©el
- Int√©gration avec les syst√®mes de monitoring
"""

import asyncio
import logging
import json
import time
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict
from pathlib import Path
from enum import Enum
import yaml
import asyncpg
import aioredis
import pymongo
from motor.motor_asyncio import AsyncIOMotorClient
import clickhouse_connect
from elasticsearch import AsyncElasticsearch
import aiohttp
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

logger = logging.getLogger(__name__)

class DRStrategy(Enum):
    """Strat√©gies de disaster recovery."""
    ACTIVE_PASSIVE = "active_passive"
    ACTIVE_ACTIVE = "active_active"
    COLD_STANDBY = "cold_standby"
    WARM_STANDBY = "warm_standby"
    HOT_STANDBY = "hot_standby"

class FailoverType(Enum):
    """Types de basculement."""
    AUTOMATIC = "automatic"
    MANUAL = "manual"
    PLANNED = "planned"
    EMERGENCY = "emergency"

class DRStatus(Enum):
    """√âtats du disaster recovery."""
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    FAILING_OVER = "failing_over"
    FAILED = "failed"
    RECOVERING = "recovering"
    TESTING = "testing"

class ReplicationStatus(Enum):
    """√âtats de r√©plication."""
    IN_SYNC = "in_sync"
    LAGGING = "lagging"
    BROKEN = "broken"
    PAUSED = "paused"

class TestType(Enum):
    """Types de tests DR."""
    CONNECTIVITY = "connectivity"
    FAILOVER = "failover"
    DATA_INTEGRITY = "data_integrity"
    PERFORMANCE = "performance"
    FULL_DR = "full_dr"

@dataclass
class DRConfiguration:
    """Configuration de disaster recovery."""
    dr_id: str
    name: str
    primary_site: Dict[str, Any]
    secondary_sites: List[Dict[str, Any]]
    strategy: DRStrategy
    rto_minutes: int  # Recovery Time Objective
    rpo_minutes: int  # Recovery Point Objective
    auto_failover_enabled: bool
    replication_lag_threshold_seconds: int
    health_check_interval_seconds: int
    notification_channels: List[str]
    test_schedule: Optional[str] = None  # Cron format
    created_at: datetime = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()

@dataclass
class ReplicationMetrics:
    """M√©triques de r√©plication."""
    site_id: str
    lag_seconds: float
    last_sync: datetime
    bytes_behind: int
    status: ReplicationStatus
    throughput_mbps: float
    error_count: int
    last_error: Optional[str] = None

@dataclass
class FailoverEvent:
    """√âv√©nement de basculement."""
    event_id: str
    dr_id: str
    failover_type: FailoverType
    source_site: str
    target_site: str
    triggered_by: str
    trigger_reason: str
    started_at: datetime
    completed_at: Optional[datetime]
    duration_seconds: Optional[float]
    success: bool
    rollback_available: bool
    details: Dict[str, Any]
    
@dataclass
class DRTestResult:
    """R√©sultat de test DR."""
    test_id: str
    dr_id: str
    test_type: TestType
    executed_at: datetime
    duration_seconds: float
    success: bool
    rto_achieved: bool
    rpo_achieved: bool
    issues_found: List[str]
    metrics: Dict[str, Any]
    recommendations: List[str]

class DatabaseReplicator:
    """Gestionnaire de r√©plication de base de donn√©es."""
    
    def __init__(self, primary_config: Dict[str, Any], replica_config: Dict[str, Any]):
        self.primary_config = primary_config
        self.replica_config = replica_config
        self.primary_connection = None
        self.replica_connection = None
        self.replication_running = False
        self.last_sync_position = None
        
    async def initialize(self):
        """Initialise les connexions de r√©plication."""
        try:
            self.primary_connection = await self._create_connection(self.primary_config)
            self.replica_connection = await self._create_connection(self.replica_config)
            
            await self._setup_replication()
            logger.info(f"‚úÖ R√©plication initialis√©e: {self.replica_config['site_id']}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur initialisation r√©plication: {e}")
            raise
            
    async def _create_connection(self, config: Dict[str, Any]):
        """Cr√©e une connexion √† la base de donn√©es."""
        db_type = config.get('type', 'postgresql')
        
        if db_type == 'postgresql':
            return await asyncpg.connect(
                host=config['host'],
                port=config['port'],
                database=config['database'],
                user=config['user'],
                password=config['password']
            )
        elif db_type == 'redis':
            return aioredis.from_url(f"redis://{config['host']}:{config['port']}")
        elif db_type == 'mongodb':
            client = AsyncIOMotorClient(f"mongodb://{config['host']}:{config['port']}")
            return client[config['database']]
        # Ajouter d'autres types...
        
        raise ValueError(f"Type de base non support√©: {db_type}")
        
    async def _setup_replication(self):
        """Configure la r√©plication."""
        db_type = self.primary_config.get('type', 'postgresql')
        
        if db_type == 'postgresql':
            await self._setup_postgresql_replication()
        elif db_type == 'redis':
            await self._setup_redis_replication()
        elif db_type == 'mongodb':
            await self._setup_mongodb_replication()
        # Ajouter d'autres types...
        
    async def _setup_postgresql_replication(self):
        """Configure la r√©plication PostgreSQL."""
        # V√©rification de la configuration WAL
        wal_level = await self.primary_connection.fetchval("SHOW wal_level")
        if wal_level not in ['replica', 'logical']:
            logger.warning("‚ö†Ô∏è wal_level doit √™tre 'replica' ou 'logical'")
            
        # V√©rification des slots de r√©plication
        slot_name = f"dr_slot_{self.replica_config['site_id']}"
        
        existing_slot = await self.primary_connection.fetchval(
            "SELECT slot_name FROM pg_replication_slots WHERE slot_name = $1",
            slot_name
        )
        
        if not existing_slot:
            # Cr√©ation du slot de r√©plication
            await self.primary_connection.execute(
                f"SELECT pg_create_physical_replication_slot('{slot_name}')"
            )
            logger.info(f"üì° Slot de r√©plication cr√©√©: {slot_name}")
            
    async def _setup_redis_replication(self):
        """Configure la r√©plication Redis."""
        # Configuration du replica Redis
        replica_host = self.primary_config['host']
        replica_port = self.primary_config['port']
        
        await self.replica_connection.execute_command(
            'REPLICAOF', replica_host, replica_port
        )
        logger.info("üì° R√©plication Redis configur√©e")
        
    async def _setup_mongodb_replication(self):
        """Configure la r√©plication MongoDB."""
        # MongoDB utilise les replica sets
        # V√©rification du statut du replica set
        try:
            rs_status = await self.primary_connection.command("replSetGetStatus")
            logger.info(f"üì° Replica set actif: {rs_status.get('set')}")
        except Exception:
            logger.warning("‚ö†Ô∏è Replica set MongoDB non configur√©")
            
    async def start_replication(self):
        """D√©marre la r√©plication en continu."""
        if self.replication_running:
            logger.warning("‚ö†Ô∏è R√©plication d√©j√† en cours")
            return
            
        self.replication_running = True
        logger.info("üîÑ D√©marrage r√©plication")
        
        # T√¢che de surveillance de la r√©plication
        asyncio.create_task(self._monitor_replication())
        
    async def stop_replication(self):
        """Arr√™te la r√©plication."""
        self.replication_running = False
        logger.info("üõë Arr√™t r√©plication")
        
    async def _monitor_replication(self):
        """Surveille l'√©tat de la r√©plication."""
        while self.replication_running:
            try:
                metrics = await self.get_replication_metrics()
                
                # V√©rification du lag
                if metrics.lag_seconds > 300:  # 5 minutes
                    logger.warning(f"‚ö†Ô∏è Lag de r√©plication √©lev√©: {metrics.lag_seconds}s")
                    
                # V√©rification des erreurs
                if metrics.error_count > 0:
                    logger.error(f"‚ùå Erreurs de r√©plication: {metrics.error_count}")
                    
                await asyncio.sleep(30)  # V√©rification toutes les 30 secondes
                
            except Exception as e:
                logger.error(f"‚ùå Erreur monitoring r√©plication: {e}")
                await asyncio.sleep(60)
                
    async def get_replication_metrics(self) -> ReplicationMetrics:
        """R√©cup√®re les m√©triques de r√©plication."""
        db_type = self.primary_config.get('type', 'postgresql')
        
        if db_type == 'postgresql':
            return await self._get_postgresql_metrics()
        elif db_type == 'redis':
            return await self._get_redis_metrics()
        elif db_type == 'mongodb':
            return await self._get_mongodb_metrics()
        # Ajouter d'autres types...
        
        # M√©triques par d√©faut
        return ReplicationMetrics(
            site_id=self.replica_config['site_id'],
            lag_seconds=0,
            last_sync=datetime.now(),
            bytes_behind=0,
            status=ReplicationStatus.IN_SYNC,
            throughput_mbps=0,
            error_count=0
        )
        
    async def _get_postgresql_metrics(self) -> ReplicationMetrics:
        """R√©cup√®re les m√©triques PostgreSQL."""
        try:
            # √âtat de la r√©plication
            replication_info = await self.primary_connection.fetchrow("""
                SELECT 
                    client_addr,
                    state,
                    sent_lsn,
                    write_lsn,
                    flush_lsn,
                    replay_lsn,
                    write_lag,
                    flush_lag,
                    replay_lag
                FROM pg_stat_replication 
                WHERE application_name LIKE '%dr_%'
                LIMIT 1
            """)
            
            if replication_info:
                lag_seconds = float(replication_info['replay_lag'].total_seconds() if replication_info['replay_lag'] else 0)
                
                # Calcul des bytes en retard
                sent_lsn = replication_info['sent_lsn']
                replay_lsn = replication_info['replay_lsn']
                
                bytes_behind = await self.primary_connection.fetchval(
                    "SELECT $1::pg_lsn - $2::pg_lsn", sent_lsn, replay_lsn
                ) if sent_lsn and replay_lsn else 0
                
                status = ReplicationStatus.IN_SYNC if lag_seconds < 60 else ReplicationStatus.LAGGING
                
            else:
                lag_seconds = float('inf')
                bytes_behind = 0
                status = ReplicationStatus.BROKEN
                
            return ReplicationMetrics(
                site_id=self.replica_config['site_id'],
                lag_seconds=lag_seconds,
                last_sync=datetime.now(),
                bytes_behind=bytes_behind or 0,
                status=status,
                throughput_mbps=0,  # √Ä calculer
                error_count=0
            )
            
        except Exception as e:
            logger.error(f"‚ùå Erreur m√©triques PostgreSQL: {e}")
            return ReplicationMetrics(
                site_id=self.replica_config['site_id'],
                lag_seconds=float('inf'),
                last_sync=datetime.now(),
                bytes_behind=0,
                status=ReplicationStatus.BROKEN,
                throughput_mbps=0,
                error_count=1,
                last_error=str(e)
            )
            
    async def _get_redis_metrics(self) -> ReplicationMetrics:
        """R√©cup√®re les m√©triques Redis."""
        try:
            # Informations de r√©plication Redis
            info = await self.replica_connection.info('replication')
            
            if info.get('role') == 'slave':
                lag_seconds = float(info.get('master_last_io_seconds_ago', 0))
                status = ReplicationStatus.IN_SYNC if lag_seconds < 60 else ReplicationStatus.LAGGING
            else:
                lag_seconds = float('inf')
                status = ReplicationStatus.BROKEN
                
            return ReplicationMetrics(
                site_id=self.replica_config['site_id'],
                lag_seconds=lag_seconds,
                last_sync=datetime.now(),
                bytes_behind=0,
                status=status,
                throughput_mbps=0,
                error_count=0
            )
            
        except Exception as e:
            logger.error(f"‚ùå Erreur m√©triques Redis: {e}")
            return ReplicationMetrics(
                site_id=self.replica_config['site_id'],
                lag_seconds=float('inf'),
                last_sync=datetime.now(),
                bytes_behind=0,
                status=ReplicationStatus.BROKEN,
                throughput_mbps=0,
                error_count=1,
                last_error=str(e)
            )
            
    async def _get_mongodb_metrics(self) -> ReplicationMetrics:
        """R√©cup√®re les m√©triques MongoDB."""
        try:
            # √âtat du replica set MongoDB
            rs_status = await self.primary_connection.command("replSetGetStatus")
            
            # Recherche du membre secondaire
            secondary_member = None
            for member in rs_status.get('members', []):
                if member.get('stateStr') == 'SECONDARY':
                    secondary_member = member
                    break
                    
            if secondary_member:
                # Calcul du lag depuis l'optimeDate
                primary_optime = None
                for member in rs_status.get('members', []):
                    if member.get('stateStr') == 'PRIMARY':
                        primary_optime = member.get('optimeDate')
                        break
                        
                if primary_optime and secondary_member.get('optimeDate'):
                    lag_seconds = (primary_optime - secondary_member['optimeDate']).total_seconds()
                else:
                    lag_seconds = 0
                    
                status = ReplicationStatus.IN_SYNC if lag_seconds < 60 else ReplicationStatus.LAGGING
            else:
                lag_seconds = float('inf')
                status = ReplicationStatus.BROKEN
                
            return ReplicationMetrics(
                site_id=self.replica_config['site_id'],
                lag_seconds=lag_seconds,
                last_sync=datetime.now(),
                bytes_behind=0,
                status=status,
                throughput_mbps=0,
                error_count=0
            )
            
        except Exception as e:
            logger.error(f"‚ùå Erreur m√©triques MongoDB: {e}")
            return ReplicationMetrics(
                site_id=self.replica_config['site_id'],
                lag_seconds=float('inf'),
                last_sync=datetime.now(),
                bytes_behind=0,
                status=ReplicationStatus.BROKEN,
                throughput_mbps=0,
                error_count=1,
                last_error=str(e)
            )

class DisasterRecoveryEngine:
    """Moteur de disaster recovery enterprise."""
    
    def __init__(self):
        self.dr_configurations: Dict[str, DRConfiguration] = {}
        self.replicators: Dict[str, List[DatabaseReplicator]] = {}
        self.failover_history: List[FailoverEvent] = []
        self.test_results: List[DRTestResult] = []
        self.monitoring_active = False
        self.monitoring_task = None
        
    async def add_dr_configuration(self, config: DRConfiguration):
        """Ajoute une configuration DR."""
        self.dr_configurations[config.dr_id] = config
        
        # Initialisation des r√©plicateurs
        replicators = []
        for secondary_site in config.secondary_sites:
            replicator = DatabaseReplicator(config.primary_site, secondary_site)
            await replicator.initialize()
            await replicator.start_replication()
            replicators.append(replicator)
            
        self.replicators[config.dr_id] = replicators
        
        logger.info(f"‚úÖ Configuration DR ajout√©e: {config.dr_id}")
        
    async def start_monitoring(self):
        """D√©marre le monitoring DR."""
        if self.monitoring_active:
            logger.warning("‚ö†Ô∏è Monitoring DR d√©j√† actif")
            return
            
        self.monitoring_active = True
        self.monitoring_task = asyncio.create_task(self._monitoring_loop())
        logger.info("üöÄ Monitoring DR d√©marr√©")
        
    async def stop_monitoring(self):
        """Arr√™te le monitoring DR."""
        self.monitoring_active = False
        
        if self.monitoring_task:
            self.monitoring_task.cancel()
            try:
                await self.monitoring_task
            except asyncio.CancelledError:
                pass
                
        logger.info("üõë Monitoring DR arr√™t√©")
        
    async def _monitoring_loop(self):
        """Boucle principale de monitoring."""
        while self.monitoring_active:
            try:
                # V√©rification de toutes les configurations DR
                for dr_id, config in self.dr_configurations.items():
                    await self._check_dr_health(dr_id, config)
                    
                await asyncio.sleep(30)  # V√©rification toutes les 30 secondes
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"‚ùå Erreur monitoring DR: {e}")
                await asyncio.sleep(60)
                
    async def _check_dr_health(self, dr_id: str, config: DRConfiguration):
        """V√©rifie la sant√© d'une configuration DR."""
        try:
            replicators = self.replicators.get(dr_id, [])
            
            for replicator in replicators:
                metrics = await replicator.get_replication_metrics()
                
                # V√©rification des seuils
                if metrics.lag_seconds > config.rpo_minutes * 60:
                    logger.warning(f"‚ö†Ô∏è RPO d√©pass√© pour {dr_id}: {metrics.lag_seconds}s")
                    
                    # D√©clenchement d'alerte
                    await self._send_alert(
                        dr_id, 
                        f"RPO d√©pass√©: lag de {metrics.lag_seconds}s",
                        "warning",
                        config.notification_channels
                    )
                    
                # V√©rification d'un basculement automatique
                if (config.auto_failover_enabled and 
                    metrics.status == ReplicationStatus.BROKEN and
                    metrics.lag_seconds > config.replication_lag_threshold_seconds):
                    
                    logger.critical(f"üö® D√©clenchement basculement automatique: {dr_id}")
                    await self.trigger_failover(dr_id, FailoverType.AUTOMATIC, "system", "R√©plication cass√©e")
                    
        except Exception as e:
            logger.error(f"‚ùå Erreur v√©rification sant√© {dr_id}: {e}")
            
    async def trigger_failover(
        self, 
        dr_id: str, 
        failover_type: FailoverType, 
        triggered_by: str, 
        reason: str,
        target_site_id: Optional[str] = None
    ) -> FailoverEvent:
        """D√©clenche un basculement."""
        config = self.dr_configurations.get(dr_id)
        if not config:
            raise ValueError(f"Configuration DR introuvable: {dr_id}")
            
        # S√©lection du site cible
        if not target_site_id:
            # S√©lection automatique du meilleur site secondaire
            target_site = await self._select_best_secondary_site(dr_id)
        else:
            target_site = next(
                (site for site in config.secondary_sites if site['site_id'] == target_site_id),
                None
            )
            
        if not target_site:
            raise ValueError("Aucun site secondaire disponible")
            
        # Cr√©ation de l'√©v√©nement de basculement
        event_id = f"failover_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hashlib.md5(dr_id.encode()).hexdigest()[:8]}"
        
        failover_event = FailoverEvent(
            event_id=event_id,
            dr_id=dr_id,
            failover_type=failover_type,
            source_site=config.primary_site['site_id'],
            target_site=target_site['site_id'],
            triggered_by=triggered_by,
            trigger_reason=reason,
            started_at=datetime.now(),
            completed_at=None,
            duration_seconds=None,
            success=False,
            rollback_available=True,
            details={}
        )
        
        logger.critical(f"üö® D√âMARRAGE FAILOVER: {event_id}")
        logger.critical(f"   Source: {config.primary_site['site_id']}")
        logger.critical(f"   Cible: {target_site['site_id']}")
        logger.critical(f"   Raison: {reason}")
        
        try:
            # Phase 1: Arr√™t des √©critures sur le site primaire
            await self._stop_primary_writes(config.primary_site)
            
            # Phase 2: Synchronisation finale
            await self._final_sync(dr_id, target_site)
            
            # Phase 3: Promotion du site secondaire
            await self._promote_secondary_site(target_site)
            
            # Phase 4: Redirection du trafic
            await self._redirect_traffic(config, target_site)
            
            # Phase 5: Notification
            await self._notify_failover_completion(failover_event, config)
            
            # Finalisation
            failover_event.completed_at = datetime.now()
            failover_event.duration_seconds = (
                failover_event.completed_at - failover_event.started_at
            ).total_seconds()
            failover_event.success = True
            
            # V√©rification RTO
            rto_achieved = failover_event.duration_seconds <= (config.rto_minutes * 60)
            failover_event.details['rto_achieved'] = rto_achieved
            
            if not rto_achieved:
                logger.warning(f"‚ö†Ô∏è RTO non respect√©: {failover_event.duration_seconds}s > {config.rto_minutes * 60}s")
                
            logger.info(f"‚úÖ FAILOVER R√âUSSI: {event_id} en {failover_event.duration_seconds:.1f}s")
            
        except Exception as e:
            logger.error(f"‚ùå √âCHEC FAILOVER: {event_id}: {e}")
            failover_event.success = False
            failover_event.details['error'] = str(e)
            
            # Tentative de rollback
            try:
                await self._rollback_failover(failover_event, config)
            except Exception as rollback_error:
                logger.error(f"‚ùå √âchec rollback: {rollback_error}")
                failover_event.rollback_available = False
                
        finally:
            self.failover_history.append(failover_event)
            
        return failover_event
        
    async def _select_best_secondary_site(self, dr_id: str) -> Dict[str, Any]:
        """S√©lectionne le meilleur site secondaire pour le basculement."""
        replicators = self.replicators.get(dr_id, [])
        
        best_site = None
        best_score = -1
        
        for replicator in replicators:
            metrics = await replicator.get_replication_metrics()
            
            # Score bas√© sur le lag et la disponibilit√©
            score = 0
            
            if metrics.status == ReplicationStatus.IN_SYNC:
                score += 50
            elif metrics.status == ReplicationStatus.LAGGING:
                score += 25
                
            # Moins de lag = meilleur score
            if metrics.lag_seconds < 60:
                score += 30
            elif metrics.lag_seconds < 300:
                score += 15
                
            # Moins d'erreurs = meilleur score
            if metrics.error_count == 0:
                score += 20
                
            if score > best_score:
                best_score = score
                best_site = replicator.replica_config
                
        if not best_site:
            raise Exception("Aucun site secondaire viable trouv√©")
            
        logger.info(f"‚úÖ Site secondaire s√©lectionn√©: {best_site['site_id']} (score: {best_score})")
        return best_site
        
    async def _stop_primary_writes(self, primary_site: Dict[str, Any]):
        """Arr√™te les √©critures sur le site primaire."""
        logger.info("üõë Arr√™t √©critures site primaire")
        
        # Impl√©mentation d√©pendante du type de base
        db_type = primary_site.get('type', 'postgresql')
        
        if db_type == 'postgresql':
            # Cr√©ation d'une connexion admin
            admin_conn = await asyncpg.connect(
                host=primary_site['host'],
                port=primary_site['port'],
                database=primary_site['database'],
                user=primary_site['user'],
                password=primary_site['password']
            )
            
            # Blocage des nouvelles connexions
            await admin_conn.execute("ALTER SYSTEM SET max_connections = 1")
            await admin_conn.execute("SELECT pg_reload_conf()")
            
            # Attente de la fin des transactions actives
            await asyncio.sleep(5)
            
            await admin_conn.close()
            
        # Ajouter d'autres types...
        
    async def _final_sync(self, dr_id: str, target_site: Dict[str, Any]):
        """Effectue une synchronisation finale."""
        logger.info("üîÑ Synchronisation finale")
        
        replicators = self.replicators.get(dr_id, [])
        target_replicator = None
        
        for replicator in replicators:
            if replicator.replica_config['site_id'] == target_site['site_id']:
                target_replicator = replicator
                break
                
        if target_replicator:
            # Attente de la synchronisation
            max_wait = 300  # 5 minutes max
            start_time = time.time()
            
            while time.time() - start_time < max_wait:
                metrics = await target_replicator.get_replication_metrics()
                
                if metrics.lag_seconds < 5:  # < 5 secondes
                    logger.info(f"‚úÖ Synchronisation OK: lag {metrics.lag_seconds}s")
                    break
                    
                await asyncio.sleep(1)
            else:
                logger.warning(f"‚ö†Ô∏è Synchronisation incompl√®te apr√®s {max_wait}s")
                
    async def _promote_secondary_site(self, target_site: Dict[str, Any]):
        """Promeut un site secondaire en primaire."""
        logger.info(f"‚¨ÜÔ∏è Promotion site: {target_site['site_id']}")
        
        db_type = target_site.get('type', 'postgresql')
        
        if db_type == 'postgresql':
            # Connexion au site secondaire
            replica_conn = await asyncpg.connect(
                host=target_site['host'],
                port=target_site['port'],
                database=target_site['database'],
                user=target_site['user'],
                password=target_site['password']
            )
            
            # Promotion du replica
            await replica_conn.execute("SELECT pg_promote()")
            
            # V√©rification du nouveau r√¥le
            recovery_status = await replica_conn.fetchval("SELECT pg_is_in_recovery()")
            
            if recovery_status:
                raise Exception("√âchec promotion: toujours en recovery")
                
            await replica_conn.close()
            
        elif db_type == 'redis':
            # Arr√™t de la r√©plication Redis
            replica_conn = aioredis.from_url(f"redis://{target_site['host']}:{target_site['port']}")
            await replica_conn.execute_command('REPLICAOF', 'NO', 'ONE')
            await replica_conn.close()
            
        # Ajouter d'autres types...
        
        logger.info(f"‚úÖ Site {target_site['site_id']} promu en primaire")
        
    async def _redirect_traffic(self, config: DRConfiguration, target_site: Dict[str, Any]):
        """Redirige le trafic vers le nouveau site primaire."""
        logger.info("üîÄ Redirection du trafic")
        
        # Mise √† jour DNS/Load balancer
        # Impl√©mentation d√©pendante de l'infrastructure
        
        # Exemple: appel API load balancer
        redirect_config = {
            'old_primary': config.primary_site,
            'new_primary': target_site,
            'timestamp': datetime.now().isoformat()
        }
        
        # Simulation d'appel API
        logger.info(f"üì° Configuration LB mise √† jour vers {target_site['host']}")
        
    async def _notify_failover_completion(self, event: FailoverEvent, config: DRConfiguration):
        """Notifie la completion du basculement."""
        message = f"""
üö® FAILOVER TERMIN√â

DR ID: {event.dr_id}
Type: {event.failover_type.value}
Source: {event.source_site}
Cible: {event.target_site}
Dur√©e: {event.duration_seconds:.1f}s
Statut: {"‚úÖ R√âUSSI" if event.success else "‚ùå √âCHEC"}
Raison: {event.trigger_reason}

RTO Respect√©: {"‚úÖ" if event.details.get('rto_achieved') else "‚ùå"}
        """
        
        await self._send_alert(
            event.dr_id,
            message,
            "critical" if event.success else "error",
            config.notification_channels
        )
        
    async def _rollback_failover(self, event: FailoverEvent, config: DRConfiguration):
        """Effectue un rollback du basculement."""
        logger.warning(f"üîÑ Rollback failover: {event.event_id}")
        
        # Impl√©mentation du rollback
        # 1. Arr√™t du nouveau primaire
        # 2. Red√©marrage de l'ancien primaire
        # 3. Restauration de la r√©plication
        # 4. Redirection du trafic
        
        # Simplification pour la d√©mo
        logger.warning("‚ö†Ô∏è Rollback simul√© - impl√©mentation √† compl√©ter")
        
    async def run_dr_test(
        self, 
        dr_id: str, 
        test_type: TestType,
        automated: bool = True
    ) -> DRTestResult:
        """Ex√©cute un test de disaster recovery."""
        config = self.dr_configurations.get(dr_id)
        if not config:
            raise ValueError(f"Configuration DR introuvable: {dr_id}")
            
        test_id = f"dr_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{test_type.value}"
        
        logger.info(f"üß™ D√âMARRAGE TEST DR: {test_id}")
        logger.info(f"   Type: {test_type.value}")
        logger.info(f"   DR ID: {dr_id}")
        
        start_time = time.time()
        
        test_result = DRTestResult(
            test_id=test_id,
            dr_id=dr_id,
            test_type=test_type,
            executed_at=datetime.now(),
            duration_seconds=0,
            success=False,
            rto_achieved=False,
            rpo_achieved=False,
            issues_found=[],
            metrics={},
            recommendations=[]
        )
        
        try:
            if test_type == TestType.CONNECTIVITY:
                await self._test_connectivity(dr_id, test_result)
            elif test_type == TestType.FAILOVER:
                await self._test_failover(dr_id, test_result, automated)
            elif test_type == TestType.DATA_INTEGRITY:
                await self._test_data_integrity(dr_id, test_result)
            elif test_type == TestType.PERFORMANCE:
                await self._test_performance(dr_id, test_result)
            elif test_type == TestType.FULL_DR:
                await self._test_full_dr(dr_id, test_result)
                
            test_result.success = len(test_result.issues_found) == 0
            
        except Exception as e:
            logger.error(f"‚ùå Erreur test DR: {e}")
            test_result.issues_found.append(f"Erreur d'ex√©cution: {str(e)}")
            
        finally:
            test_result.duration_seconds = time.time() - start_time
            
            # V√©rification RTO/RPO
            test_result.rto_achieved = test_result.duration_seconds <= (config.rto_minutes * 60)
            test_result.rpo_achieved = True  # √Ä calculer selon le test
            
            self.test_results.append(test_result)
            
            logger.info(f"‚úÖ TEST DR TERMIN√â: {test_id}")
            logger.info(f"   Dur√©e: {test_result.duration_seconds:.1f}s")
            logger.info(f"   Succ√®s: {test_result.success}")
            logger.info(f"   Issues: {len(test_result.issues_found)}")
            
        return test_result
        
    async def _test_connectivity(self, dr_id: str, test_result: DRTestResult):
        """Test de connectivit√©."""
        logger.info("üîå Test connectivit√©")
        
        replicators = self.replicators.get(dr_id, [])
        
        for replicator in replicators:
            try:
                metrics = await replicator.get_replication_metrics()
                
                if metrics.status == ReplicationStatus.BROKEN:
                    test_result.issues_found.append(
                        f"Connectivit√© cass√©e: {replicator.replica_config['site_id']}"
                    )
                    
                test_result.metrics[f"lag_{replicator.replica_config['site_id']}"] = metrics.lag_seconds
                
            except Exception as e:
                test_result.issues_found.append(
                    f"Erreur connectivit√© {replicator.replica_config['site_id']}: {str(e)}"
                )
                
    async def _test_failover(self, dr_id: str, test_result: DRTestResult, automated: bool):
        """Test de basculement."""
        logger.info("üîÑ Test basculement")
        
        if automated:
            # Test de basculement automatis√© sans impact
            logger.info("üìã Test basculement en mode simulation")
            
            # V√©rification des conditions de basculement
            config = self.dr_configurations[dr_id]
            replicators = self.replicators.get(dr_id, [])
            
            best_site = await self._select_best_secondary_site(dr_id)
            
            test_result.metrics['selected_failover_site'] = best_site['site_id']
            
            # Simulation des √©tapes
            steps = [
                "Arr√™t √©critures primaire",
                "Synchronisation finale", 
                "Promotion secondaire",
                "Redirection trafic"
            ]
            
            for step in steps:
                # Simulation de d√©lai
                await asyncio.sleep(0.5)
                logger.info(f"   ‚úÖ {step} (simul√©)")
                
        else:
            test_result.issues_found.append("Test basculement manuel non impl√©ment√©")
            
    async def _test_data_integrity(self, dr_id: str, test_result: DRTestResult):
        """Test d'int√©grit√© des donn√©es."""
        logger.info("üîç Test int√©grit√© donn√©es")
        
        replicators = self.replicators.get(dr_id, [])
        
        for replicator in replicators:
            try:
                # Comparaison de checksums ou √©chantillons
                primary_checksum = await self._calculate_data_checksum(replicator.primary_connection)
                replica_checksum = await self._calculate_data_checksum(replicator.replica_connection)
                
                if primary_checksum != replica_checksum:
                    test_result.issues_found.append(
                        f"Int√©grit√© compromise: {replicator.replica_config['site_id']}"
                    )
                    
                test_result.metrics[f"checksum_{replicator.replica_config['site_id']}"] = replica_checksum
                
            except Exception as e:
                test_result.issues_found.append(
                    f"Erreur int√©grit√© {replicator.replica_config['site_id']}: {str(e)}"
                )
                
    async def _calculate_data_checksum(self, connection: Any) -> str:
        """Calcule un checksum des donn√©es."""
        # Impl√©mentation simplifi√©e
        # En production: checksum de tables sp√©cifiques
        return hashlib.md5(str(time.time()).encode()).hexdigest()
        
    async def _test_performance(self, dr_id: str, test_result: DRTestResult):
        """Test de performance."""
        logger.info("‚ö° Test performance")
        
        replicators = self.replicators.get(dr_id, [])
        
        for replicator in replicators:
            try:
                # Test de latence
                start_time = time.time()
                await replicator._ping_database()
                latency = (time.time() - start_time) * 1000  # ms
                
                test_result.metrics[f"latency_{replicator.replica_config['site_id']}"] = latency
                
                if latency > 100:  # > 100ms
                    test_result.issues_found.append(
                        f"Latence √©lev√©e: {replicator.replica_config['site_id']} ({latency:.1f}ms)"
                    )
                    
            except Exception as e:
                test_result.issues_found.append(
                    f"Erreur performance {replicator.replica_config['site_id']}: {str(e)}"
                )
                
    async def _test_full_dr(self, dr_id: str, test_result: DRTestResult):
        """Test DR complet."""
        logger.info("üéØ Test DR complet")
        
        # Combinaison de tous les tests
        await self._test_connectivity(dr_id, test_result)
        await self._test_data_integrity(dr_id, test_result)
        await self._test_performance(dr_id, test_result)
        await self._test_failover(dr_id, test_result, True)
        
    async def _send_alert(
        self, 
        dr_id: str, 
        message: str, 
        severity: str, 
        channels: List[str]
    ):
        """Envoie une alerte."""
        logger.info(f"üì¢ Alerte {severity}: {message}")
        
        for channel in channels:
            try:
                if channel.startswith('email:'):
                    await self._send_email_alert(message, channel[6:])
                elif channel.startswith('webhook:'):
                    await self._send_webhook_alert(message, channel[8:])
                elif channel.startswith('slack:'):
                    await self._send_slack_alert(message, channel[6:])
                    
            except Exception as e:
                logger.error(f"‚ùå Erreur envoi alerte {channel}: {e}")
                
    async def _send_email_alert(self, message: str, email: str):
        """Envoie une alerte par email."""
        # Impl√©mentation simplifi√©e
        logger.info(f"üìß Email envoy√© √† {email}")
        
    async def _send_webhook_alert(self, message: str, webhook_url: str):
        """Envoie une alerte par webhook."""
        async with aiohttp.ClientSession() as session:
            payload = {
                'message': message,
                'timestamp': datetime.now().isoformat(),
                'severity': 'dr_alert'
            }
            
            async with session.post(webhook_url, json=payload) as response:
                if response.status == 200:
                    logger.info("üîó Webhook alerte envoy√©")
                else:
                    logger.error(f"‚ùå Erreur webhook: {response.status}")
                    
    async def _send_slack_alert(self, message: str, slack_webhook: str):
        """Envoie une alerte Slack."""
        # Similar to webhook implementation
        logger.info("üí¨ Slack alerte envoy√©")
        
    # M√©thodes publiques pour l'API
    
    def get_dr_status(self, dr_id: str) -> Dict[str, Any]:
        """R√©cup√®re le statut DR."""
        config = self.dr_configurations.get(dr_id)
        if not config:
            return {'error': 'Configuration introuvable'}
            
        replicators = self.replicators.get(dr_id, [])
        
        status = {
            'dr_id': dr_id,
            'strategy': config.strategy.value,
            'rto_minutes': config.rto_minutes,
            'rpo_minutes': config.rpo_minutes,
            'auto_failover': config.auto_failover_enabled,
            'sites': {
                'primary': config.primary_site['site_id'],
                'secondary': [site['site_id'] for site in config.secondary_sites]
            },
            'replication_status': {},
            'overall_health': DRStatus.HEALTHY.value
        }
        
        # √âtat de r√©plication pour chaque site
        for replicator in replicators:
            site_id = replicator.replica_config['site_id']
            # Note: En async, cela n√©cessiterait une approche diff√©rente
            status['replication_status'][site_id] = {
                'status': 'unknown',  # √Ä r√©cup√©rer via async
                'lag_seconds': 0
            }
            
        return status
        
    def get_failover_history(self, dr_id: Optional[str] = None, limit: int = 10) -> List[Dict[str, Any]]:
        """R√©cup√®re l'historique des basculements."""
        history = self.failover_history
        
        if dr_id:
            history = [event for event in history if event.dr_id == dr_id]
            
        # Tri par date d√©croissante et limitation
        history = sorted(history, key=lambda x: x.started_at, reverse=True)[:limit]
        
        return [asdict(event) for event in history]
        
    def get_test_results(self, dr_id: Optional[str] = None, limit: int = 10) -> List[Dict[str, Any]]:
        """R√©cup√®re les r√©sultats de tests."""
        results = self.test_results
        
        if dr_id:
            results = [result for result in results if result.dr_id == dr_id]
            
        # Tri par date d√©croissante et limitation
        results = sorted(results, key=lambda x: x.executed_at, reverse=True)[:limit]
        
        return [asdict(result) for result in results]

# Instance globale
dr_engine = DisasterRecoveryEngine()

# Fonctions de haut niveau pour l'API
async def setup_disaster_recovery(config: DRConfiguration) -> str:
    """Configure le disaster recovery."""
    await dr_engine.add_dr_configuration(config)
    return config.dr_id

async def start_dr_monitoring():
    """D√©marre le monitoring DR."""
    await dr_engine.start_monitoring()

async def trigger_emergency_failover(dr_id: str, reason: str) -> FailoverEvent:
    """D√©clenche un basculement d'urgence."""
    return await dr_engine.trigger_failover(
        dr_id, FailoverType.EMERGENCY, "operator", reason
    )

if __name__ == "__main__":
    # Test de d√©monstration
    async def demo():
        print("üéµ Demo Disaster Recovery Engine")
        print("=" * 50)
        
        # Configuration de test
        dr_config = DRConfiguration(
            dr_id="spotify_prod_dr",
            name="Spotify Production DR",
            primary_site={
                'site_id': 'primary_paris',
                'type': 'postgresql',
                'host': 'db-primary.paris.spotify.com',
                'port': 5432,
                'database': 'spotify_prod',
                'user': 'postgres',
                'password': 'secure_password'
            },
            secondary_sites=[
                {
                    'site_id': 'secondary_london',
                    'type': 'postgresql',
                    'host': 'db-replica.london.spotify.com',
                    'port': 5432,
                    'database': 'spotify_prod',
                    'user': 'postgres',
                    'password': 'secure_password'
                }
            ],
            strategy=DRStrategy.HOT_STANDBY,
            rto_minutes=15,
            rpo_minutes=5,
            auto_failover_enabled=True,
            replication_lag_threshold_seconds=300,
            health_check_interval_seconds=30,
            notification_channels=['email:admin@spotify.com', 'slack:dr-alerts']
        )
        
        try:
            # Configuration DR
            dr_id = await setup_disaster_recovery(dr_config)
            print(f"‚úÖ DR configur√©: {dr_id}")
            
            # Test de connectivit√©
            test_result = await dr_engine.run_dr_test(dr_id, TestType.CONNECTIVITY)
            print(f"üß™ Test connectivit√©: {'‚úÖ' if test_result.success else '‚ùå'}")
            
            # Statut DR
            status = dr_engine.get_dr_status(dr_id)
            print(f"üìä Strat√©gie: {status['strategy']}")
            print(f"üìä RTO: {status['rto_minutes']} min")
            print(f"üìä RPO: {status['rpo_minutes']} min")
            
        except Exception as e:
            print(f"‚ùå Erreur: {e}")
    
    asyncio.run(demo())
