#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
üèóÔ∏è DATA PIPELINE ULTRA-AVANC√â - ORCHESTRATION INDUSTRIELLE
Pipeline de donn√©es r√©volutionnaire pour processing enterprise de niveau industriel

Architecture Pipeline Enterprise :
‚îú‚îÄ‚îÄ üîÑ ETL/ELT Ultra-Optimis√© (Extract, Transform, Load)
‚îú‚îÄ‚îÄ üìä Data Quality Engine (Validation + Profiling + Cleaning)
‚îú‚îÄ‚îÄ üß¨ Schema Evolution (Automatic schema discovery)
‚îú‚îÄ‚îÄ üîÑ Stream Processing (Real-time + Batch + Micro-batch)
‚îú‚îÄ‚îÄ üõ°Ô∏è Data Lineage (Full traceability + audit trail)
‚îú‚îÄ‚îÄ üìà Performance Optimization (Query optimization + caching)
‚îú‚îÄ‚îÄ üö® Error Handling (Circuit breakers + retry policies)
‚îú‚îÄ‚îÄ üìã Data Catalog (Metadata management + discovery)
‚îú‚îÄ‚îÄ üîß Auto-scaling (Dynamic resource allocation)
‚îî‚îÄ‚îÄ ‚ö° Throughput: 10M+ records/hour

D√©velopp√© par l'√©quipe d'experts Achiri avec pipeline de niveau industriel
Version: 3.0.0 - Production Ready Enterprise
"""

__version__ = "3.0.0"
__author__ = "Achiri Expert Team - Data Pipeline Division"
__license__ = "Enterprise Commercial"

import asyncio
import logging
import sys
import time
import json
import uuid
import hashlib
from datetime import datetime, timedelta
from typing import (
    Dict, List, Optional, Any, Union, Tuple, Set, 
    AsyncGenerator, Callable, TypeVar, Generic, Protocol
)
from dataclasses import dataclass, field, asdict
from enum import Enum, auto
from abc import ABC, abstractmethod
from contextlib import asynccontextmanager
from concurrent.futures import ThreadPoolExecutor
import threading
from collections import defaultdict, deque
import inspect

# Traitement de donn√©es
try:
    import pandas as pd
    import numpy as np
    from scipy import stats
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False

try:
    import dask.dataframe as dd
    from dask.distributed import Client, as_completed
    from dask import delayed
    DASK_AVAILABLE = True
except ImportError:
    DASK_AVAILABLE = False

# Validation et qualit√© des donn√©es
try:
    import pydantic
    from pydantic import BaseModel, Field, validator
    PYDANTIC_AVAILABLE = True
except ImportError:
    PYDANTIC_AVAILABLE = False

try:
    import great_expectations as ge
    from great_expectations.core import ExpectationSuite
    GREAT_EXPECTATIONS_AVAILABLE = True
except ImportError:
    GREAT_EXPECTATIONS_AVAILABLE = False

# Processing parall√®le
try:
    import joblib
    from joblib import Parallel, delayed
    JOBLIB_AVAILABLE = True
except ImportError:
    JOBLIB_AVAILABLE = False

# Cache et optimisations
try:
    import redis
    import memcache
    CACHE_LIBS_AVAILABLE = True
except ImportError:
    CACHE_LIBS_AVAILABLE = False

# Workflow orchestration
try:
    import airflow
    from airflow import DAG
    from airflow.operators.python_operator import PythonOperator
    AIRFLOW_AVAILABLE = True
except ImportError:
    AIRFLOW_AVAILABLE = False

# Monitoring et observabilit√©
try:
    from prometheus_client import Counter, Histogram, Gauge
    MONITORING_AVAILABLE = True
except ImportError:
    MONITORING_AVAILABLE = False

logger = logging.getLogger(__name__)

# =============================================================================
# √âNUM√âRATIONS ET TYPES
# =============================================================================

class PipelineType(Enum):
    """Types de pipeline"""
    BATCH = auto()          # Traitement par lot
    STREAM = auto()         # Traitement continu
    MICRO_BATCH = auto()    # Micro-lots
    LAMBDA = auto()         # Architecture Lambda
    KAPPA = auto()          # Architecture Kappa

class TaskType(Enum):
    """Types de t√¢ches de pipeline"""
    EXTRACT = auto()        # Extraction
    TRANSFORM = auto()      # Transformation
    LOAD = auto()          # Chargement
    VALIDATE = auto()      # Validation
    CLEAN = auto()         # Nettoyage
    AGGREGATE = auto()     # Agr√©gation
    ENRICH = auto()        # Enrichissement

class DataQualityLevel(Enum):
    """Niveaux de qualit√© des donn√©es"""
    HIGH = auto()          # Qualit√© √©lev√©e (> 95%)
    MEDIUM = auto()        # Qualit√© moyenne (80-95%)
    LOW = auto()           # Qualit√© faible (60-80%)
    POOR = auto()          # Qualit√© insuffisante (< 60%)

class ExecutionStrategy(Enum):
    """Strat√©gies d'ex√©cution"""
    SEQUENTIAL = auto()    # S√©quentiel
    PARALLEL = auto()      # Parall√®le
    DISTRIBUTED = auto()   # Distribu√©
    ADAPTIVE = auto()      # Adaptatif

class PipelineStatus(Enum):
    """√âtats de pipeline"""
    CREATED = auto()
    RUNNING = auto()
    COMPLETED = auto()
    FAILED = auto()
    PAUSED = auto()
    CANCELLED = auto()

# =============================================================================
# MOD√àLES DE DONN√âES
# =============================================================================

@dataclass
class DataSchema:
    """Sch√©ma de donn√©es avec validation"""
    name: str
    version: str
    
    # D√©finition des champs
    fields: Dict[str, Dict[str, Any]]
    
    # Contraintes
    required_fields: List[str] = field(default_factory=list)
    unique_fields: List[str] = field(default_factory=list)
    
    # M√©tadonn√©es
    description: str = ""
    tags: List[str] = field(default_factory=list)
    created_at: datetime = field(default_factory=datetime.utcnow)
    
    def validate_record(self, record: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """Validation d'un enregistrement"""
        errors = []
        
        # V√©rification des champs requis
        for field_name in self.required_fields:
            if field_name not in record:
                errors.append(f"Champ requis manquant: {field_name}")
        
        # Validation des types
        for field_name, field_def in self.fields.items():
            if field_name in record:
                expected_type = field_def.get('type')
                value = record[field_name]
                
                if expected_type == 'int' and not isinstance(value, int):
                    errors.append(f"Type incorrect pour {field_name}: attendu int, re√ßu {type(value)}")
                elif expected_type == 'float' and not isinstance(value, (int, float)):
                    errors.append(f"Type incorrect pour {field_name}: attendu float, re√ßu {type(value)}")
                elif expected_type == 'str' and not isinstance(value, str):
                    errors.append(f"Type incorrect pour {field_name}: attendu str, re√ßu {type(value)}")
        
        return len(errors) == 0, errors

@dataclass
class PipelineTask:
    """T√¢che de pipeline"""
    task_id: str
    task_type: TaskType
    function: Callable
    
    # D√©pendances
    dependencies: List[str] = field(default_factory=list)
    
    # Configuration
    config: Dict[str, Any] = field(default_factory=dict)
    retry_count: int = 3
    timeout_seconds: int = 300
    
    # Ressources
    cpu_cores: int = 1
    memory_mb: int = 512
    
    # √âtat
    status: PipelineStatus = PipelineStatus.CREATED
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    error_message: Optional[str] = None
    
    # M√©triques
    execution_time_ms: float = 0.0
    records_processed: int = 0
    records_output: int = 0

@dataclass
class DataQualityReport:
    """Rapport de qualit√© des donn√©es"""
    dataset_name: str
    
    # M√©triques de base
    total_records: int
    valid_records: int
    invalid_records: int
    duplicate_records: int
    
    # Qualit√© par champ
    field_quality: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    
    # Erreurs d√©tect√©es
    validation_errors: List[Dict[str, Any]] = field(default_factory=list)
    data_drift: Dict[str, float] = field(default_factory=dict)
    
    # Score global
    quality_score: float = 0.0
    quality_level: DataQualityLevel = DataQualityLevel.POOR
    
    # Recommandations
    recommendations: List[str] = field(default_factory=list)
    
    # Timestamps
    created_at: datetime = field(default_factory=datetime.utcnow)
    
    @property
    def validity_rate(self) -> float:
        """Taux de validit√©"""
        return (self.valid_records / self.total_records * 100) if self.total_records > 0 else 0.0
    
    @property
    def duplicate_rate(self) -> float:
        """Taux de doublons"""
        return (self.duplicate_records / self.total_records * 100) if self.total_records > 0 else 0.0

@dataclass
class PipelineMetrics:
    """M√©triques de pipeline"""
    pipeline_id: str
    
    # M√©triques d'ex√©cution
    total_tasks: int = 0
    completed_tasks: int = 0
    failed_tasks: int = 0
    
    # M√©triques de performance
    total_execution_time_ms: float = 0.0
    avg_task_time_ms: float = 0.0
    throughput_records_per_second: float = 0.0
    
    # M√©triques de ressources
    cpu_utilization: float = 0.0
    memory_utilization: float = 0.0
    
    # M√©triques de qualit√©
    data_quality_score: float = 0.0
    error_rate: float = 0.0
    
    # Timestamps
    start_time: datetime = field(default_factory=datetime.utcnow)
    end_time: Optional[datetime] = None
    last_updated: datetime = field(default_factory=datetime.utcnow)

# =============================================================================
# MOTEURS DE QUALIT√â DES DONN√âES
# =============================================================================

class DataQualityEngine:
    """
    üõ°Ô∏è MOTEUR DE QUALIT√â DES DONN√âES ULTRA-AVANC√â
    
    Validation et profiling de donn√©es enterprise :
    - Validation automatique de sch√©mas
    - D√©tection de d√©rives de donn√©es
    - Profiling statistique complet
    - Nettoyage automatique
    - Recommandations d'am√©lioration
    """
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.DataQualityEngine")
        
        # Sch√©mas enregistr√©s
        self._schemas = {}
        
        # Profileurs de donn√©es
        self._profilers = {}
        
        # Historique de qualit√©
        self._quality_history = defaultdict(list)
        
        # Seuils de qualit√©
        self._quality_thresholds = {
            'validity_rate': 95.0,
            'completeness_rate': 90.0,
            'duplicate_rate': 5.0,
            'drift_threshold': 0.1
        }
    
    def register_schema(self, schema: DataSchema) -> bool:
        """Enregistrement d'un sch√©ma de donn√©es"""
        try:
            self._schemas[schema.name] = schema
            self.logger.info(f"‚úÖ Sch√©ma {schema.name} v{schema.version} enregistr√©")
            return True
        except Exception as e:
            self.logger.error(f"‚ùå Erreur enregistrement sch√©ma: {e}")
            return False
    
    async def validate_data(
        self,
        data: Union[pd.DataFrame, List[Dict[str, Any]]],
        schema_name: str
    ) -> DataQualityReport:
        """Validation compl√®te de donn√©es"""
        start_time = time.time()
        
        try:
            self.logger.info(f"üîç Validation donn√©es avec sch√©ma {schema_name}...")
            
            if schema_name not in self._schemas:
                raise ValueError(f"Sch√©ma {schema_name} non trouv√©")
            
            schema = self._schemas[schema_name]
            
            # Conversion en DataFrame si n√©cessaire
            if not isinstance(data, pd.DataFrame):
                if PANDAS_AVAILABLE:
                    df = pd.DataFrame(data)
                else:
                    raise ValueError("Pandas non disponible pour la conversion")
            else:
                df = data
            
            total_records = len(df)
            
            # Initialisation du rapport
            report = DataQualityReport(
                dataset_name=schema_name,
                total_records=total_records
            )
            
            # Validation par enregistrement
            valid_count = 0
            validation_errors = []
            
            for idx, row in df.iterrows():
                is_valid, errors = schema.validate_record(row.to_dict())
                if is_valid:
                    valid_count += 1
                else:
                    validation_errors.extend([
                        {'record_id': idx, 'error': error} for error in errors
                    ])
            
            report.valid_records = valid_count
            report.invalid_records = total_records - valid_count
            report.validation_errors = validation_errors[:100]  # Limiter les erreurs
            
            # D√©tection de doublons
            if PANDAS_AVAILABLE:
                duplicates = df.duplicated().sum()
                report.duplicate_records = duplicates
            
            # Profiling par champ
            await self._profile_fields(df, schema, report)
            
            # Calcul du score de qualit√©
            report.quality_score = self._calculate_quality_score(report)
            report.quality_level = self._determine_quality_level(report.quality_score)
            
            # G√©n√©ration de recommandations
            report.recommendations = self._generate_recommendations(report)
            
            # Enregistrement dans l'historique
            self._quality_history[schema_name].append(report)
            
            validation_time = (time.time() - start_time) * 1000
            self.logger.info(f"‚úÖ Validation termin√©e en {validation_time:.2f}ms - Score: {report.quality_score:.1f}%")
            
            return report
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur validation donn√©es: {e}")
            raise
    
    async def _profile_fields(
        self,
        df: pd.DataFrame,
        schema: DataSchema,
        report: DataQualityReport
    ):
        """Profiling d√©taill√© des champs"""
        if not PANDAS_AVAILABLE:
            return
        
        for field_name, field_def in schema.fields.items():
            if field_name not in df.columns:
                continue
            
            column = df[field_name]
            field_profile = {
                'total_count': len(column),
                'null_count': column.isnull().sum(),
                'unique_count': column.nunique(),
                'data_type': str(column.dtype)
            }
            
            # Calcul du taux de compl√©tude
            field_profile['completeness_rate'] = (
                (field_profile['total_count'] - field_profile['null_count']) / 
                field_profile['total_count'] * 100
            ) if field_profile['total_count'] > 0 else 0.0
            
            # Statistiques pour les champs num√©riques
            if column.dtype in ['int64', 'float64']:
                field_profile.update({
                    'min_value': column.min(),
                    'max_value': column.max(),
                    'mean_value': column.mean(),
                    'std_value': column.std(),
                    'median_value': column.median()
                })
            
            # Statistiques pour les champs texte
            elif column.dtype == 'object':
                field_profile.update({
                    'avg_length': column.astype(str).str.len().mean(),
                    'min_length': column.astype(str).str.len().min(),
                    'max_length': column.astype(str).str.len().max(),
                    'most_common': column.value_counts().head(5).to_dict()
                })
            
            report.field_quality[field_name] = field_profile
    
    def _calculate_quality_score(self, report: DataQualityReport) -> float:
        """Calcul du score de qualit√© global"""
        weights = {
            'validity': 0.4,
            'completeness': 0.3,
            'uniqueness': 0.2,
            'consistency': 0.1
        }
        
        # Score de validit√©
        validity_score = report.validity_rate
        
        # Score de compl√©tude (moyenne des champs)
        completeness_scores = [
            field['completeness_rate'] 
            for field in report.field_quality.values()
        ]
        completeness_score = np.mean(completeness_scores) if completeness_scores else 0.0
        
        # Score d'unicit√© (inverse du taux de doublons)
        uniqueness_score = max(0, 100 - report.duplicate_rate)
        
        # Score de consistance (bas√© sur les erreurs de validation)
        consistency_score = max(0, 100 - len(report.validation_errors))
        
        # Score pond√©r√©
        total_score = (
            validity_score * weights['validity'] +
            completeness_score * weights['completeness'] +
            uniqueness_score * weights['uniqueness'] +
            consistency_score * weights['consistency']
        )
        
        return min(100.0, max(0.0, total_score))
    
    def _determine_quality_level(self, score: float) -> DataQualityLevel:
        """D√©termination du niveau de qualit√©"""
        if score >= 95:
            return DataQualityLevel.HIGH
        elif score >= 80:
            return DataQualityLevel.MEDIUM
        elif score >= 60:
            return DataQualityLevel.LOW
        else:
            return DataQualityLevel.POOR
    
    def _generate_recommendations(self, report: DataQualityReport) -> List[str]:
        """G√©n√©ration de recommandations d'am√©lioration"""
        recommendations = []
        
        # Recommandations bas√©es sur la validit√©
        if report.validity_rate < self._quality_thresholds['validity_rate']:
            recommendations.append(
                f"Am√©liorer la validit√© des donn√©es ({report.validity_rate:.1f}% < {self._quality_thresholds['validity_rate']}%)"
            )
        
        # Recommandations bas√©es sur les doublons
        if report.duplicate_rate > self._quality_thresholds['duplicate_rate']:
            recommendations.append(
                f"R√©duire les doublons ({report.duplicate_rate:.1f}% > {self._quality_thresholds['duplicate_rate']}%)"
            )
        
        # Recommandations par champ
        for field_name, field_info in report.field_quality.items():
            if field_info['completeness_rate'] < self._quality_thresholds['completeness_rate']:
                recommendations.append(
                    f"Am√©liorer la compl√©tude du champ '{field_name}' ({field_info['completeness_rate']:.1f}%)"
                )
        
        return recommendations

# =============================================================================
# GESTIONNAIRE DE PIPELINE PRINCIPAL
# =============================================================================

class DataPipeline:
    """
    üèóÔ∏è PIPELINE DE DONN√âES ULTRA-AVANC√â
    
    Orchestrateur de pipeline enterprise :
    - Ex√©cution s√©quentielle et parall√®le
    - Gestion d'erreurs et recovery
    - Monitoring en temps r√©el
    - Optimisation automatique
    - Tra√ßabilit√© compl√®te
    """
    
    def __init__(self, pipeline_id: str, pipeline_type: PipelineType = PipelineType.BATCH):
        self.pipeline_id = pipeline_id
        self.pipeline_type = pipeline_type
        self.logger = logging.getLogger(f"{__name__}.DataPipeline")
        
        # T√¢ches du pipeline
        self._tasks = {}
        self._task_order = []
        self._dependency_graph = defaultdict(list)
        
        # √âtat d'ex√©cution
        self._status = PipelineStatus.CREATED
        self._current_tasks = set()
        
        # Moteur de qualit√©
        self._quality_engine = DataQualityEngine()
        
        # M√©triques
        self._metrics = PipelineMetrics(pipeline_id=pipeline_id)
        
        # Configuration
        self._execution_strategy = ExecutionStrategy.ADAPTIVE
        self._max_parallel_tasks = 4
        
        # Monitoring
        if MONITORING_AVAILABLE:
            self._task_counter = Counter('pipeline_tasks_total', 'Total pipeline tasks', ['pipeline_id', 'status'])
            self._execution_timer = Histogram('pipeline_execution_seconds', 'Pipeline execution time', ['pipeline_id'])
    
    def add_task(self, task: PipelineTask) -> bool:
        """Ajout d'une t√¢che au pipeline"""
        try:
            # Validation de la t√¢che
            if not callable(task.function):
                raise ValueError(f"Fonction de t√¢che {task.task_id} non callable")
            
            # Enregistrement
            self._tasks[task.task_id] = task
            self._task_order.append(task.task_id)
            
            # Gestion des d√©pendances
            for dep in task.dependencies:
                if dep not in self._tasks:
                    self.logger.warning(f"D√©pendance {dep} non trouv√©e pour t√¢che {task.task_id}")
                self._dependency_graph[dep].append(task.task_id)
            
            self._metrics.total_tasks += 1
            
            self.logger.info(f"‚úÖ T√¢che {task.task_id} ajout√©e au pipeline {self.pipeline_id}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur ajout t√¢che {task.task_id}: {e}")
            return False
    
    async def execute(self, input_data: Any = None) -> Dict[str, Any]:
        """Ex√©cution du pipeline"""
        start_time = time.time()
        self._status = PipelineStatus.RUNNING
        self._metrics.start_time = datetime.utcnow()
        
        try:
            self.logger.info(f"üöÄ D√©marrage pipeline {self.pipeline_id} ({self.pipeline_type.name})")
            
            # Validation du graphe de d√©pendances
            if not self._validate_dependencies():
                raise ValueError("Graphe de d√©pendances invalide")
            
            # Ordre d'ex√©cution topologique
            execution_order = self._topological_sort()
            
            # Ex√©cution selon la strat√©gie
            if self._execution_strategy == ExecutionStrategy.SEQUENTIAL:
                results = await self._execute_sequential(execution_order, input_data)
            elif self._execution_strategy == ExecutionStrategy.PARALLEL:
                results = await self._execute_parallel(execution_order, input_data)
            else:  # ADAPTIVE
                results = await self._execute_adaptive(execution_order, input_data)
            
            # Finalisation
            self._status = PipelineStatus.COMPLETED
            self._metrics.end_time = datetime.utcnow()
            
            execution_time = (time.time() - start_time) * 1000
            self._metrics.total_execution_time_ms = execution_time
            
            if MONITORING_AVAILABLE:
                self._execution_timer.observe(execution_time / 1000)
            
            self.logger.info(f"‚úÖ Pipeline {self.pipeline_id} termin√© en {execution_time:.2f}ms")
            
            return {
                'status': 'completed',
                'execution_time_ms': execution_time,
                'results': results,
                'metrics': asdict(self._metrics)
            }
            
        except Exception as e:
            self._status = PipelineStatus.FAILED
            self.logger.error(f"‚ùå √âchec pipeline {self.pipeline_id}: {e}")
            
            return {
                'status': 'failed',
                'error': str(e),
                'metrics': asdict(self._metrics)
            }
    
    def _validate_dependencies(self) -> bool:
        """Validation du graphe de d√©pendances"""
        # V√©rification de cycles
        visited = set()
        rec_stack = set()
        
        def has_cycle(task_id: str) -> bool:
            visited.add(task_id)
            rec_stack.add(task_id)
            
            for dependent in self._dependency_graph.get(task_id, []):
                if dependent not in visited:
                    if has_cycle(dependent):
                        return True
                elif dependent in rec_stack:
                    return True
            
            rec_stack.remove(task_id)
            return False
        
        for task_id in self._tasks:
            if task_id not in visited:
                if has_cycle(task_id):
                    self.logger.error(f"‚ùå Cycle d√©tect√© dans les d√©pendances")
                    return False
        
        return True
    
    def _topological_sort(self) -> List[str]:
        """Tri topologique pour ordre d'ex√©cution"""
        in_degree = defaultdict(int)
        
        # Calcul des degr√©s entrants
        for task_id in self._tasks:
            for dependent in self._dependency_graph.get(task_id, []):
                in_degree[dependent] += 1
        
        # Queue des t√¢ches sans d√©pendances
        queue = deque([task_id for task_id in self._tasks if in_degree[task_id] == 0])
        result = []
        
        while queue:
            task_id = queue.popleft()
            result.append(task_id)
            
            # Mise √† jour des d√©pendances
            for dependent in self._dependency_graph.get(task_id, []):
                in_degree[dependent] -= 1
                if in_degree[dependent] == 0:
                    queue.append(dependent)
        
        return result
    
    async def _execute_sequential(
        self,
        execution_order: List[str],
        input_data: Any
    ) -> Dict[str, Any]:
        """Ex√©cution s√©quentielle des t√¢ches"""
        results = {}
        current_data = input_data
        
        for task_id in execution_order:
            result = await self._execute_single_task(task_id, current_data)
            results[task_id] = result
            current_data = result.get('output_data', current_data)
        
        return results
    
    async def _execute_parallel(
        self,
        execution_order: List[str],
        input_data: Any
    ) -> Dict[str, Any]:
        """Ex√©cution parall√®le des t√¢ches"""
        results = {}
        completed_tasks = set()
        task_data = {None: input_data}  # Donn√©es par t√¢che
        
        while len(completed_tasks) < len(execution_order):
            # T√¢ches pr√™tes √† ex√©cuter
            ready_tasks = [
                task_id for task_id in execution_order
                if (task_id not in completed_tasks and 
                    task_id not in self._current_tasks and
                    all(dep in completed_tasks for dep in self._tasks[task_id].dependencies))
            ]
            
            # Limitation du parall√©lisme
            ready_tasks = ready_tasks[:self._max_parallel_tasks - len(self._current_tasks)]
            
            if not ready_tasks:
                await asyncio.sleep(0.1)
                continue
            
            # Lancement des t√¢ches parall√®les
            task_futures = []
            for task_id in ready_tasks:
                self._current_tasks.add(task_id)
                
                # Donn√©es d'entr√©e de la t√¢che
                task_input = self._prepare_task_input(task_id, task_data)
                
                future = asyncio.create_task(
                    self._execute_single_task(task_id, task_input)
                )
                task_futures.append((task_id, future))
            
            # Attente de compl√©tion
            for task_id, future in task_futures:
                try:
                    result = await future
                    results[task_id] = result
                    task_data[task_id] = result.get('output_data')
                    completed_tasks.add(task_id)
                    self._current_tasks.remove(task_id)
                except Exception as e:
                    self.logger.error(f"‚ùå Erreur t√¢che {task_id}: {e}")
                    self._current_tasks.remove(task_id)
                    raise
        
        return results
    
    async def _execute_adaptive(
        self,
        execution_order: List[str],
        input_data: Any
    ) -> Dict[str, Any]:
        """Ex√©cution adaptative (hybride s√©quentiel/parall√®le)"""
        # Analyse de la complexit√© des t√¢ches
        complex_tasks = [
            task_id for task_id in execution_order
            if self._tasks[task_id].cpu_cores > 1 or self._tasks[task_id].memory_mb > 1024
        ]
        
        # Strat√©gie adaptative
        if len(complex_tasks) > len(execution_order) * 0.5:
            # Beaucoup de t√¢ches complexes -> s√©quentiel
            return await self._execute_sequential(execution_order, input_data)
        else:
            # T√¢ches l√©g√®res -> parall√®le
            return await self._execute_parallel(execution_order, input_data)
    
    def _prepare_task_input(self, task_id: str, task_data: Dict[str, Any]) -> Any:
        """Pr√©paration des donn√©es d'entr√©e pour une t√¢che"""
        task = self._tasks[task_id]
        
        if not task.dependencies:
            return task_data.get(None)  # Donn√©es initiales
        elif len(task.dependencies) == 1:
            return task_data.get(task.dependencies[0])
        else:
            # Fusion des donn√©es de multiple d√©pendances
            merged_data = {}
            for dep in task.dependencies:
                if dep in task_data:
                    if isinstance(task_data[dep], dict):
                        merged_data.update(task_data[dep])
            return merged_data
    
    async def _execute_single_task(self, task_id: str, input_data: Any) -> Dict[str, Any]:
        """Ex√©cution d'une t√¢che unique"""
        task = self._tasks[task_id]
        start_time = time.time()
        
        try:
            self.logger.info(f"üîß Ex√©cution t√¢che {task_id} ({task.task_type.name})")
            
            task.status = PipelineStatus.RUNNING
            task.start_time = datetime.utcnow()
            
            # Pr√©paration des arguments
            function_args = [input_data] if input_data is not None else []
            function_kwargs = task.config.copy()
            
            # Ex√©cution avec timeout
            try:
                if asyncio.iscoroutinefunction(task.function):
                    result = await asyncio.wait_for(
                        task.function(*function_args, **function_kwargs),
                        timeout=task.timeout_seconds
                    )
                else:
                    # Ex√©cution synchrone dans thread pool
                    loop = asyncio.get_event_loop()
                    result = await loop.run_in_executor(
                        None,
                        lambda: task.function(*function_args, **function_kwargs)
                    )
            except asyncio.TimeoutError:
                raise TimeoutError(f"T√¢che {task_id} timeout apr√®s {task.timeout_seconds}s")
            
            # Finalisation
            task.status = PipelineStatus.COMPLETED
            task.end_time = datetime.utcnow()
            task.execution_time_ms = (time.time() - start_time) * 1000
            
            self._metrics.completed_tasks += 1
            
            if MONITORING_AVAILABLE:
                self._task_counter.labels(pipeline_id=self.pipeline_id, status='completed').inc()
            
            self.logger.info(f"‚úÖ T√¢che {task_id} termin√©e en {task.execution_time_ms:.2f}ms")
            
            return {
                'status': 'completed',
                'output_data': result,
                'execution_time_ms': task.execution_time_ms,
                'records_processed': getattr(result, 'records_processed', 0) if hasattr(result, 'records_processed') else 0
            }
            
        except Exception as e:
            task.status = PipelineStatus.FAILED
            task.end_time = datetime.utcnow()
            task.error_message = str(e)
            
            self._metrics.failed_tasks += 1
            
            if MONITORING_AVAILABLE:
                self._task_counter.labels(pipeline_id=self.pipeline_id, status='failed').inc()
            
            self.logger.error(f"‚ùå √âchec t√¢che {task_id}: {e}")
            
            # Gestion des retry
            if task.retry_count > 0:
                task.retry_count -= 1
                self.logger.info(f"üîÑ Retry t√¢che {task_id} ({task.retry_count} restants)")
                await asyncio.sleep(1)  # Backoff simple
                return await self._execute_single_task(task_id, input_data)
            
            raise
    
    def get_status(self) -> Dict[str, Any]:
        """√âtat actuel du pipeline"""
        return {
            'pipeline_id': self.pipeline_id,
            'status': self._status.name,
            'total_tasks': len(self._tasks),
            'completed_tasks': self._metrics.completed_tasks,
            'failed_tasks': self._metrics.failed_tasks,
            'current_tasks': list(self._current_tasks),
            'execution_strategy': self._execution_strategy.name,
            'metrics': asdict(self._metrics)
        }
    
    async def pause(self):
        """Pause du pipeline"""
        self._status = PipelineStatus.PAUSED
        self.logger.info(f"‚è∏Ô∏è Pipeline {self.pipeline_id} mis en pause")
    
    async def resume(self):
        """Reprise du pipeline"""
        if self._status == PipelineStatus.PAUSED:
            self._status = PipelineStatus.RUNNING
            self.logger.info(f"‚ñ∂Ô∏è Pipeline {self.pipeline_id} repris")
    
    async def cancel(self):
        """Annulation du pipeline"""
        self._status = PipelineStatus.CANCELLED
        self.logger.info(f"‚ùå Pipeline {self.pipeline_id} annul√©")

# =============================================================================
# GESTIONNAIRE DE PIPELINES
# =============================================================================

class PipelineManager:
    """
    üèóÔ∏è GESTIONNAIRE DE PIPELINES ULTRA-AVANC√â
    
    Orchestrateur de multiples pipelines :
    - Gestion de cycle de vie
    - Ordonnancement intelligent
    - Monitoring centralis√©
    - Optimisation de ressources
    """
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.PipelineManager")
        
        # Registre des pipelines
        self._pipelines = {}
        self._pipeline_history = defaultdict(list)
        
        # Ordonnanceur
        self._scheduler_task = None
        self._is_running = False
        
        # M√©triques globales
        self._global_metrics = {
            "total_pipelines": 0,
            "active_pipelines": 0,
            "completed_pipelines": 0,
            "failed_pipelines": 0
        }
    
    async def initialize(self) -> bool:
        """Initialisation du gestionnaire"""
        try:
            self.logger.info("üèóÔ∏è Initialisation Pipeline Manager...")
            
            self._is_running = True
            
            # D√©marrage de l'ordonnanceur
            self._scheduler_task = asyncio.create_task(self._scheduler_loop())
            
            self.logger.info("‚úÖ Pipeline Manager initialis√©")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation Pipeline Manager: {e}")
            return False
    
    def register_pipeline(self, pipeline: DataPipeline) -> bool:
        """Enregistrement d'un pipeline"""
        try:
            self._pipelines[pipeline.pipeline_id] = pipeline
            self._global_metrics["total_pipelines"] += 1
            
            self.logger.info(f"‚úÖ Pipeline {pipeline.pipeline_id} enregistr√©")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur enregistrement pipeline: {e}")
            return False
    
    async def execute_pipeline(self, pipeline_id: str, input_data: Any = None) -> Dict[str, Any]:
        """Ex√©cution d'un pipeline"""
        if pipeline_id not in self._pipelines:
            raise ValueError(f"Pipeline {pipeline_id} non trouv√©")
        
        pipeline = self._pipelines[pipeline_id]
        
        # Mise √† jour m√©triques
        self._global_metrics["active_pipelines"] += 1
        
        try:
            result = await pipeline.execute(input_data)
            
            # Historique
            self._pipeline_history[pipeline_id].append({
                'timestamp': datetime.utcnow(),
                'result': result
            })
            
            # M√©triques
            if result['status'] == 'completed':
                self._global_metrics["completed_pipelines"] += 1
            else:
                self._global_metrics["failed_pipelines"] += 1
            
            return result
            
        finally:
            self._global_metrics["active_pipelines"] -= 1
    
    async def _scheduler_loop(self):
        """Boucle d'ordonnancement"""
        while self._is_running:
            try:
                # Monitoring des pipelines actifs
                await self._monitor_pipelines()
                
                # Optimisation des ressources
                await self._optimize_resources()
                
                await asyncio.sleep(5)  # Check toutes les 5 secondes
                
            except Exception as e:
                self.logger.error(f"‚ùå Erreur ordonnanceur: {e}")
                await asyncio.sleep(10)
    
    async def _monitor_pipelines(self):
        """Monitoring des pipelines"""
        for pipeline_id, pipeline in self._pipelines.items():
            try:
                status = pipeline.get_status()
                if status['status'] in ['RUNNING', 'PAUSED']:
                    self.logger.debug(f"üìä Pipeline {pipeline_id}: {status['completed_tasks']}/{status['total_tasks']} t√¢ches")
            except Exception as e:
                self.logger.error(f"‚ùå Erreur monitoring pipeline {pipeline_id}: {e}")
    
    async def _optimize_resources(self):
        """Optimisation des ressources"""
        # Exemple d'optimisation simple
        active_pipelines = [
            p for p in self._pipelines.values() 
            if p._status == PipelineStatus.RUNNING
        ]
        
        if len(active_pipelines) > 3:  # Limitation arbitraire
            self.logger.warning("‚ö†Ô∏è Beaucoup de pipelines actifs, consid√©rer la limitation")
    
    def get_system_status(self) -> Dict[str, Any]:
        """√âtat global du syst√®me"""
        pipeline_statuses = {}
        for pipeline_id, pipeline in self._pipelines.items():
            pipeline_statuses[pipeline_id] = pipeline.get_status()
        
        return {
            "global_metrics": self._global_metrics,
            "pipelines": pipeline_statuses,
            "is_running": self._is_running,
            "timestamp": datetime.utcnow().isoformat()
        }
    
    async def shutdown(self):
        """Arr√™t propre du gestionnaire"""
        self.logger.info("üîÑ Arr√™t Pipeline Manager...")
        
        self._is_running = False
        
        if self._scheduler_task:
            self._scheduler_task.cancel()
        
        # Arr√™t des pipelines actifs
        for pipeline_id, pipeline in self._pipelines.items():
            if pipeline._status == PipelineStatus.RUNNING:
                await pipeline.cancel()
        
        self.logger.info("‚úÖ Pipeline Manager arr√™t√©")

# =============================================================================
# UTILITAIRES ET EXPORTS
# =============================================================================

async def create_pipeline_manager() -> PipelineManager:
    """Cr√©ation et initialisation du gestionnaire de pipelines"""
    manager = PipelineManager()
    await manager.initialize()
    return manager

# T√¢ches pr√©d√©finies utiles
def extract_from_database(connection_string: str, query: str) -> pd.DataFrame:
    """T√¢che d'extraction depuis base de donn√©es"""
    # Impl√©mentation d'exemple
    return pd.DataFrame()

async def transform_data(data: pd.DataFrame, transformations: List[str]) -> pd.DataFrame:
    """T√¢che de transformation de donn√©es"""
    # Impl√©mentation d'exemple
    return data

def load_to_storage(data: pd.DataFrame, destination: str) -> Dict[str, Any]:
    """T√¢che de chargement vers stockage"""
    # Impl√©mentation d'exemple
    return {"records_loaded": len(data)}

__all__ = [
    # Classes principales
    "DataPipeline",
    "PipelineManager",
    "DataQualityEngine",
    
    # Mod√®les
    "DataSchema",
    "PipelineTask",
    "DataQualityReport",
    "PipelineMetrics",
    
    # Enums
    "PipelineType",
    "TaskType",
    "DataQualityLevel",
    "ExecutionStrategy",
    "PipelineStatus",
    
    # Utilitaires
    "create_pipeline_manager",
    "extract_from_database",
    "transform_data",
    "load_to_storage"
]
