#!/usr/bin/env python3
"""
Analytics Performance Monitor - Moniteur de Performances Analytics
================================================================

Script de monitoring et d'optimisation des performances du syst√®me analytics.
Surveille les m√©triques syst√®me, identifie les goulots d'√©tranglement,
et fournit des recommandations d'optimisation.

Fonctionnalit√©s:
- Monitoring CPU, m√©moire, disque, r√©seau
- Surveillance des performances des bases de donn√©es
- Monitoring des mod√®les ML
- Alertes de performance
- Recommandations d'optimisation automatiques
- G√©n√©ration de rapports de performance

Usage:
    python performance_monitor.py [options]

Auteur: Fahed Mlaiel - Expert DevOps & Performance
√âquipe: ML Engineers, DBA & Infrastructure Specialists
"""

import asyncio
import psutil
import time
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import aioredis
import aiofiles
from collections import deque, defaultdict

# Analytics modules
from config import AnalyticsConfig, get_config
from utils import Logger, Timer, Formatter
from storage import StorageManager


@dataclass
class SystemMetrics:
    """M√©triques syst√®me."""
    timestamp: datetime
    cpu_percent: float
    memory_percent: float
    memory_available: int
    memory_used: int
    disk_usage_percent: float
    disk_free: int
    network_bytes_sent: int
    network_bytes_recv: int
    load_average: Tuple[float, float, float]
    process_count: int
    thread_count: int


@dataclass
class DatabaseMetrics:
    """M√©triques base de donn√©es."""
    timestamp: datetime
    database_name: str
    connection_count: int
    active_queries: int
    cache_hit_ratio: float
    slow_queries: int
    response_time_avg: float
    throughput_ops_per_sec: float
    storage_size_mb: float
    index_efficiency: float


@dataclass
class MLModelMetrics:
    """M√©triques mod√®le ML."""
    timestamp: datetime
    model_name: str
    prediction_latency_ms: float
    memory_usage_mb: float
    cpu_usage_percent: float
    predictions_per_second: float
    accuracy_score: float
    feature_processing_time_ms: float
    model_size_mb: float
    cache_hit_ratio: float


@dataclass
class PerformanceAlert:
    """Alerte de performance."""
    timestamp: datetime
    alert_type: str  # 'critical', 'warning', 'info'
    component: str  # 'system', 'database', 'ml_model', 'network'
    message: str
    metric_value: float
    threshold: float
    recommendation: str


class PerformanceProfiler:
    """Profileur de performances d√©taill√©."""
    
    def __init__(self):
        self.logger = Logger("PerformanceProfiler")
        self.profiling_data = defaultdict(list)
        self.start_time = None
        self.end_time = None
    
    def start_profiling(self, session_name: str):
        """D√©marre une session de profiling."""
        self.start_time = time.time()
        self.logger.info(f"D√©marrage profiling session: {session_name}")
    
    def stop_profiling(self, session_name: str):
        """Arr√™te une session de profiling."""
        self.end_time = time.time()
        duration = self.end_time - self.start_time
        self.logger.info(f"Fin profiling session: {session_name} ({duration:.2f}s)")
        return duration
    
    def profile_function(self, func_name: str):
        """D√©corateur de profiling de fonction."""
        def decorator(func):
            async def wrapper(*args, **kwargs):
                start_time = time.time()
                try:
                    result = await func(*args, **kwargs)
                    success = True
                    error = None
                except Exception as e:
                    result = None
                    success = False
                    error = str(e)
                finally:
                    end_time = time.time()
                    duration = end_time - start_time
                    
                    self.profiling_data[func_name].append({
                        'timestamp': datetime.utcnow(),
                        'duration': duration,
                        'success': success,
                        'error': error,
                        'args_count': len(args),
                        'kwargs_count': len(kwargs)
                    })
                
                return result
            return wrapper
        return decorator
    
    def get_function_stats(self, func_name: str) -> Dict[str, Any]:
        """Obtient les statistiques d'une fonction."""
        data = self.profiling_data.get(func_name, [])
        if not data:
            return {}
        
        durations = [d['duration'] for d in data]
        successes = [d['success'] for d in data]
        
        return {
            'call_count': len(data),
            'success_rate': sum(successes) / len(successes),
            'avg_duration': sum(durations) / len(durations),
            'min_duration': min(durations),
            'max_duration': max(durations),
            'total_duration': sum(durations),
            'errors': [d['error'] for d in data if d['error']],
            'last_call': data[-1]['timestamp'].isoformat()
        }


class PerformanceThresholds:
    """Seuils de performance configurables."""
    
    # Seuils syst√®me
    CPU_WARNING = 70.0
    CPU_CRITICAL = 90.0
    MEMORY_WARNING = 80.0
    MEMORY_CRITICAL = 95.0
    DISK_WARNING = 85.0
    DISK_CRITICAL = 95.0
    
    # Seuils base de donn√©es
    DB_RESPONSE_TIME_WARNING = 100.0  # ms
    DB_RESPONSE_TIME_CRITICAL = 500.0  # ms
    DB_CACHE_HIT_RATIO_WARNING = 0.8
    DB_SLOW_QUERIES_WARNING = 10
    
    # Seuils ML
    ML_PREDICTION_LATENCY_WARNING = 50.0  # ms
    ML_PREDICTION_LATENCY_CRITICAL = 200.0  # ms
    ML_MEMORY_WARNING = 1024.0  # MB
    ML_ACCURACY_WARNING = 0.85


class PerformanceMonitor:
    """Moniteur de performances principal."""
    
    def __init__(self, config: AnalyticsConfig):
        self.config = config
        self.logger = Logger("PerformanceMonitor")
        self.profiler = PerformanceProfiler()
        self.thresholds = PerformanceThresholds()
        
        # Historique des m√©triques
        self.system_metrics_history = deque(maxlen=1000)
        self.database_metrics_history = deque(maxlen=1000)
        self.ml_metrics_history = deque(maxlen=1000)
        self.alerts_history = deque(maxlen=500)
        
        # √âtat du monitoring
        self.is_monitoring = False
        self.storage_manager: Optional[StorageManager] = None
        self.redis_client: Optional[aioredis.Redis] = None
        
        # Cache des m√©triques
        self.metrics_cache = {}
        self.cache_ttl = 30  # secondes
    
    async def start_monitoring(self):
        """D√©marre le monitoring."""
        if self.is_monitoring:
            return
        
        try:
            self.logger.info("D√©marrage du monitoring de performances...")
            
            # Connexions
            self.storage_manager = StorageManager(self.config)
            await self.storage_manager.connect_all()
            
            self.redis_client = await aioredis.from_url(
                self.config.cache.redis_url,
                decode_responses=True
            )
            
            self.is_monitoring = True
            self.logger.info("Monitoring d√©marr√© avec succ√®s")
            
        except Exception as e:
            self.logger.error(f"Erreur d√©marrage monitoring: {e}")
            raise
    
    async def stop_monitoring(self):
        """Arr√™te le monitoring."""
        if not self.is_monitoring:
            return
        
        try:
            self.logger.info("Arr√™t du monitoring...")
            
            if self.storage_manager:
                await self.storage_manager.disconnect_all()
            
            if self.redis_client:
                await self.redis_client.close()
            
            self.is_monitoring = False
            self.logger.info("Monitoring arr√™t√©")
            
        except Exception as e:
            self.logger.error(f"Erreur arr√™t monitoring: {e}")
    
    def collect_system_metrics(self) -> SystemMetrics:
        """Collecte les m√©triques syst√®me."""
        try:
            # CPU
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # M√©moire
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            memory_available = memory.available
            memory_used = memory.used
            
            # Disque
            disk = psutil.disk_usage('/')
            disk_usage_percent = (disk.used / disk.total) * 100
            disk_free = disk.free
            
            # R√©seau
            network = psutil.net_io_counters()
            network_bytes_sent = network.bytes_sent
            network_bytes_recv = network.bytes_recv
            
            # Charge syst√®me
            load_average = psutil.getloadavg()
            
            # Processus
            process_count = len(psutil.pids())
            thread_count = sum(p.num_threads() for p in psutil.process_iter(['num_threads']) if p.info['num_threads'])
            
            metrics = SystemMetrics(
                timestamp=datetime.utcnow(),
                cpu_percent=cpu_percent,
                memory_percent=memory_percent,
                memory_available=memory_available,
                memory_used=memory_used,
                disk_usage_percent=disk_usage_percent,
                disk_free=disk_free,
                network_bytes_sent=network_bytes_sent,
                network_bytes_recv=network_bytes_recv,
                load_average=load_average,
                process_count=process_count,
                thread_count=thread_count
            )
            
            self.system_metrics_history.append(metrics)
            return metrics
            
        except Exception as e:
            self.logger.error(f"Erreur collecte m√©triques syst√®me: {e}")
            raise
    
    async def collect_database_metrics(self) -> List[DatabaseMetrics]:
        """Collecte les m√©triques des bases de donn√©es."""
        if not self.storage_manager:
            return []
        
        metrics_list = []
        
        try:
            # M√©triques Redis
            if self.redis_client:
                redis_info = await self.redis_client.info()
                redis_metrics = DatabaseMetrics(
                    timestamp=datetime.utcnow(),
                    database_name="redis",
                    connection_count=redis_info.get('connected_clients', 0),
                    active_queries=redis_info.get('instantaneous_ops_per_sec', 0),
                    cache_hit_ratio=self._calculate_redis_hit_ratio(redis_info),
                    slow_queries=redis_info.get('slowlog_len', 0),
                    response_time_avg=0.0,  # Redis ne fournit pas cette m√©trique directement
                    throughput_ops_per_sec=redis_info.get('instantaneous_ops_per_sec', 0),
                    storage_size_mb=redis_info.get('used_memory', 0) / (1024 * 1024),
                    index_efficiency=1.0  # Redis utilise des structures optimis√©es
                )
                metrics_list.append(redis_metrics)
            
            # M√©triques PostgreSQL (si configur√©)
            if hasattr(self.config.database, 'postgresql_url'):
                # Simulation des m√©triques PostgreSQL
                pg_metrics = DatabaseMetrics(
                    timestamp=datetime.utcnow(),
                    database_name="postgresql",
                    connection_count=15,
                    active_queries=3,
                    cache_hit_ratio=0.95,
                    slow_queries=2,
                    response_time_avg=25.5,
                    throughput_ops_per_sec=150.0,
                    storage_size_mb=2048.0,
                    index_efficiency=0.92
                )
                metrics_list.append(pg_metrics)
            
            self.database_metrics_history.extend(metrics_list)
            return metrics_list
            
        except Exception as e:
            self.logger.error(f"Erreur collecte m√©triques DB: {e}")
            return []
    
    def _calculate_redis_hit_ratio(self, redis_info: Dict) -> float:
        """Calcule le ratio de cache hit pour Redis."""
        hits = redis_info.get('keyspace_hits', 0)
        misses = redis_info.get('keyspace_misses', 0)
        total = hits + misses
        return hits / total if total > 0 else 0.0
    
    async def collect_ml_metrics(self) -> List[MLModelMetrics]:
        """Collecte les m√©triques des mod√®les ML."""
        # Simulation des m√©triques ML
        models = ['anomaly_detector', 'predictive_analytics', 'behavior_analyzer']
        metrics_list = []
        
        for model_name in models:
            metrics = MLModelMetrics(
                timestamp=datetime.utcnow(),
                model_name=model_name,
                prediction_latency_ms=float(hash(model_name + str(time.time())) % 100 + 10),
                memory_usage_mb=float(hash(model_name) % 500 + 200),
                cpu_usage_percent=float(hash(model_name + "cpu") % 30 + 5),
                predictions_per_second=float(hash(model_name + "pps") % 100 + 50),
                accuracy_score=0.85 + (hash(model_name + "acc") % 15) / 100,
                feature_processing_time_ms=float(hash(model_name + "feat") % 20 + 5),
                model_size_mb=float(hash(model_name + "size") % 100 + 50),
                cache_hit_ratio=0.7 + (hash(model_name + "cache") % 30) / 100
            )
            metrics_list.append(metrics)
        
        self.ml_metrics_history.extend(metrics_list)
        return metrics_list
    
    def analyze_performance(self) -> List[PerformanceAlert]:
        """Analyse les performances et g√©n√®re des alertes."""
        alerts = []
        
        # Analyse syst√®me
        if self.system_metrics_history:
            latest_system = self.system_metrics_history[-1]
            alerts.extend(self._analyze_system_metrics(latest_system))
        
        # Analyse base de donn√©es
        if self.database_metrics_history:
            for db_metrics in list(self.database_metrics_history)[-10:]:  # Derni√®res 10
                alerts.extend(self._analyze_database_metrics(db_metrics))
        
        # Analyse ML
        if self.ml_metrics_history:
            for ml_metrics in list(self.ml_metrics_history)[-15:]:  # Derni√®res 15
                alerts.extend(self._analyze_ml_metrics(ml_metrics))
        
        # Ajouter √† l'historique
        self.alerts_history.extend(alerts)
        
        return alerts
    
    def _analyze_system_metrics(self, metrics: SystemMetrics) -> List[PerformanceAlert]:
        """Analyse les m√©triques syst√®me."""
        alerts = []
        
        # CPU
        if metrics.cpu_percent >= self.thresholds.CPU_CRITICAL:
            alerts.append(PerformanceAlert(
                timestamp=metrics.timestamp,
                alert_type='critical',
                component='system',
                message=f"CPU critique: {metrics.cpu_percent:.1f}%",
                metric_value=metrics.cpu_percent,
                threshold=self.thresholds.CPU_CRITICAL,
                recommendation="Optimiser les processus, augmenter les ressources CPU"
            ))
        elif metrics.cpu_percent >= self.thresholds.CPU_WARNING:
            alerts.append(PerformanceAlert(
                timestamp=metrics.timestamp,
                alert_type='warning',
                component='system',
                message=f"CPU √©lev√©: {metrics.cpu_percent:.1f}%",
                metric_value=metrics.cpu_percent,
                threshold=self.thresholds.CPU_WARNING,
                recommendation="Surveiller les processus consommateurs"
            ))
        
        # M√©moire
        if metrics.memory_percent >= self.thresholds.MEMORY_CRITICAL:
            alerts.append(PerformanceAlert(
                timestamp=metrics.timestamp,
                alert_type='critical',
                component='system',
                message=f"M√©moire critique: {metrics.memory_percent:.1f}%",
                metric_value=metrics.memory_percent,
                threshold=self.thresholds.MEMORY_CRITICAL,
                recommendation="Lib√©rer de la m√©moire, optimiser le cache"
            ))
        
        # Disque
        if metrics.disk_usage_percent >= self.thresholds.DISK_CRITICAL:
            alerts.append(PerformanceAlert(
                timestamp=metrics.timestamp,
                alert_type='critical',
                component='system',
                message=f"Disque critique: {metrics.disk_usage_percent:.1f}%",
                metric_value=metrics.disk_usage_percent,
                threshold=self.thresholds.DISK_CRITICAL,
                recommendation="Nettoyer les fichiers, archiver les donn√©es"
            ))
        
        return alerts
    
    def _analyze_database_metrics(self, metrics: DatabaseMetrics) -> List[PerformanceAlert]:
        """Analyse les m√©triques de base de donn√©es."""
        alerts = []
        
        # Temps de r√©ponse
        if metrics.response_time_avg >= self.thresholds.DB_RESPONSE_TIME_CRITICAL:
            alerts.append(PerformanceAlert(
                timestamp=metrics.timestamp,
                alert_type='critical',
                component='database',
                message=f"DB {metrics.database_name}: temps r√©ponse critique {metrics.response_time_avg:.1f}ms",
                metric_value=metrics.response_time_avg,
                threshold=self.thresholds.DB_RESPONSE_TIME_CRITICAL,
                recommendation="Optimiser les requ√™tes, v√©rifier les index"
            ))
        
        # Cache hit ratio
        if metrics.cache_hit_ratio < self.thresholds.DB_CACHE_HIT_RATIO_WARNING:
            alerts.append(PerformanceAlert(
                timestamp=metrics.timestamp,
                alert_type='warning',
                component='database',
                message=f"DB {metrics.database_name}: ratio cache faible {metrics.cache_hit_ratio:.2f}",
                metric_value=metrics.cache_hit_ratio,
                threshold=self.thresholds.DB_CACHE_HIT_RATIO_WARNING,
                recommendation="Augmenter la taille du cache, optimiser les requ√™tes"
            ))
        
        return alerts
    
    def _analyze_ml_metrics(self, metrics: MLModelMetrics) -> List[PerformanceAlert]:
        """Analyse les m√©triques ML."""
        alerts = []
        
        # Latence de pr√©diction
        if metrics.prediction_latency_ms >= self.thresholds.ML_PREDICTION_LATENCY_CRITICAL:
            alerts.append(PerformanceAlert(
                timestamp=metrics.timestamp,
                alert_type='critical',
                component='ml_model',
                message=f"Mod√®le {metrics.model_name}: latence critique {metrics.prediction_latency_ms:.1f}ms",
                metric_value=metrics.prediction_latency_ms,
                threshold=self.thresholds.ML_PREDICTION_LATENCY_CRITICAL,
                recommendation="Optimiser le mod√®le, utiliser un cache de pr√©dictions"
            ))
        
        # Pr√©cision
        if metrics.accuracy_score < self.thresholds.ML_ACCURACY_WARNING:
            alerts.append(PerformanceAlert(
                timestamp=metrics.timestamp,
                alert_type='warning',
                component='ml_model',
                message=f"Mod√®le {metrics.model_name}: pr√©cision faible {metrics.accuracy_score:.3f}",
                metric_value=metrics.accuracy_score,
                threshold=self.thresholds.ML_ACCURACY_WARNING,
                recommendation="R√©entra√Æner le mod√®le, v√©rifier les donn√©es"
            ))
        
        return alerts
    
    def generate_performance_report(self) -> Dict[str, Any]:
        """G√©n√®re un rapport de performances complet."""
        report = {
            'timestamp': datetime.utcnow().isoformat(),
            'monitoring_duration': len(self.system_metrics_history),
            'summary': self._generate_summary(),
            'system_analysis': self._analyze_system_trends(),
            'database_analysis': self._analyze_database_trends(),
            'ml_analysis': self._analyze_ml_trends(),
            'alerts_summary': self._generate_alerts_summary(),
            'recommendations': self._generate_recommendations(),
            'profiling_data': self._get_profiling_summary()
        }
        
        return report
    
    def _generate_summary(self) -> Dict[str, Any]:
        """G√©n√®re un r√©sum√© des performances."""
        if not self.system_metrics_history:
            return {}
        
        latest_system = self.system_metrics_history[-1]
        
        return {
            'current_cpu': latest_system.cpu_percent,
            'current_memory': latest_system.memory_percent,
            'current_disk': latest_system.disk_usage_percent,
            'active_alerts': len([a for a in self.alerts_history if a.alert_type in ['warning', 'critical']]),
            'monitoring_status': 'healthy' if latest_system.cpu_percent < 70 and latest_system.memory_percent < 80 else 'degraded'
        }
    
    def _analyze_system_trends(self) -> Dict[str, Any]:
        """Analyse les tendances syst√®me."""
        if len(self.system_metrics_history) < 2:
            return {}
        
        recent_metrics = list(self.system_metrics_history)[-10:]
        cpu_values = [m.cpu_percent for m in recent_metrics]
        memory_values = [m.memory_percent for m in recent_metrics]
        
        return {
            'cpu_trend': 'increasing' if cpu_values[-1] > cpu_values[0] else 'decreasing',
            'cpu_avg': sum(cpu_values) / len(cpu_values),
            'memory_trend': 'increasing' if memory_values[-1] > memory_values[0] else 'decreasing',
            'memory_avg': sum(memory_values) / len(memory_values),
            'peak_cpu': max(cpu_values),
            'peak_memory': max(memory_values)
        }
    
    def _analyze_database_trends(self) -> Dict[str, Any]:
        """Analyse les tendances des bases de donn√©es."""
        if not self.database_metrics_history:
            return {}
        
        db_groups = defaultdict(list)
        for metrics in self.database_metrics_history:
            db_groups[metrics.database_name].append(metrics)
        
        analysis = {}
        for db_name, metrics_list in db_groups.items():
            if len(metrics_list) >= 2:
                response_times = [m.response_time_avg for m in metrics_list[-10:]]
                throughputs = [m.throughput_ops_per_sec for m in metrics_list[-10:]]
                
                analysis[db_name] = {
                    'avg_response_time': sum(response_times) / len(response_times),
                    'avg_throughput': sum(throughputs) / len(throughputs),
                    'response_time_trend': 'increasing' if response_times[-1] > response_times[0] else 'decreasing'
                }
        
        return analysis
    
    def _analyze_ml_trends(self) -> Dict[str, Any]:
        """Analyse les tendances ML."""
        if not self.ml_metrics_history:
            return {}
        
        model_groups = defaultdict(list)
        for metrics in self.ml_metrics_history:
            model_groups[metrics.model_name].append(metrics)
        
        analysis = {}
        for model_name, metrics_list in model_groups.items():
            if len(metrics_list) >= 2:
                latencies = [m.prediction_latency_ms for m in metrics_list[-10:]]
                accuracies = [m.accuracy_score for m in metrics_list[-10:]]
                
                analysis[model_name] = {
                    'avg_latency': sum(latencies) / len(latencies),
                    'avg_accuracy': sum(accuracies) / len(accuracies),
                    'latency_trend': 'increasing' if latencies[-1] > latencies[0] else 'decreasing',
                    'performance_rating': self._calculate_ml_performance_rating(
                        sum(latencies) / len(latencies),
                        sum(accuracies) / len(accuracies)
                    )
                }
        
        return analysis
    
    def _calculate_ml_performance_rating(self, avg_latency: float, avg_accuracy: float) -> str:
        """Calcule une note de performance pour un mod√®le ML."""
        if avg_latency < 50 and avg_accuracy > 0.9:
            return 'excellent'
        elif avg_latency < 100 and avg_accuracy > 0.85:
            return 'good'
        elif avg_latency < 200 and avg_accuracy > 0.8:
            return 'average'
        else:
            return 'poor'
    
    def _generate_alerts_summary(self) -> Dict[str, Any]:
        """G√©n√®re un r√©sum√© des alertes."""
        if not self.alerts_history:
            return {'total': 0, 'by_type': {}, 'by_component': {}}
        
        recent_alerts = [a for a in self.alerts_history 
                        if (datetime.utcnow() - a.timestamp).total_seconds() < 3600]  # Derni√®re heure
        
        by_type = defaultdict(int)
        by_component = defaultdict(int)
        
        for alert in recent_alerts:
            by_type[alert.alert_type] += 1
            by_component[alert.component] += 1
        
        return {
            'total': len(recent_alerts),
            'by_type': dict(by_type),
            'by_component': dict(by_component),
            'latest_alert': recent_alerts[-1].message if recent_alerts else None
        }
    
    def _generate_recommendations(self) -> List[str]:
        """G√©n√®re des recommandations d'optimisation."""
        recommendations = []
        
        # Recommandations bas√©es sur les alertes r√©centes
        recent_alerts = [a for a in self.alerts_history 
                        if (datetime.utcnow() - a.timestamp).total_seconds() < 1800]  # 30 minutes
        
        critical_alerts = [a for a in recent_alerts if a.alert_type == 'critical']
        warning_alerts = [a for a in recent_alerts if a.alert_type == 'warning']
        
        if critical_alerts:
            recommendations.append("üö® Alertes critiques d√©tect√©es - intervention imm√©diate requise")
            for alert in critical_alerts[-3:]:  # Derni√®res 3 alertes critiques
                recommendations.append(f"   ‚Ä¢ {alert.recommendation}")
        
        if warning_alerts:
            recommendations.append("‚ö†Ô∏è Optimisations recommand√©es:")
            for alert in warning_alerts[-5:]:  # Derni√®res 5 alertes warning
                recommendations.append(f"   ‚Ä¢ {alert.recommendation}")
        
        # Recommandations g√©n√©rales
        if self.system_metrics_history:
            latest = self.system_metrics_history[-1]
            
            if latest.cpu_percent > 60:
                recommendations.append("üìä Optimiser l'utilisation CPU avec mise en cache et parall√©lisation")
            
            if latest.memory_percent > 70:
                recommendations.append("üíæ Optimiser la gestion m√©moire et nettoyer les caches")
            
            if latest.thread_count > 500:
                recommendations.append("üîß R√©duire le nombre de threads actifs")
        
        if not recommendations:
            recommendations.append("‚úÖ Syst√®me performant - continuer la surveillance")
        
        return recommendations
    
    def _get_profiling_summary(self) -> Dict[str, Any]:
        """R√©sum√© des donn√©es de profiling."""
        summary = {}
        for func_name, data in self.profiler.profiling_data.items():
            if data:
                summary[func_name] = self.profiler.get_function_stats(func_name)
        return summary
    
    async def save_report(self, report: Dict[str, Any], filepath: str = None):
        """Sauvegarde le rapport de performances."""
        if not filepath:
            timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
            filepath = f"performance_report_{timestamp}.json"
        
        try:
            async with aiofiles.open(filepath, 'w') as f:
                await f.write(json.dumps(report, indent=2, default=str))
            
            self.logger.info(f"Rapport sauvegard√©: {filepath}")
            
        except Exception as e:
            self.logger.error(f"Erreur sauvegarde rapport: {e}")


async def main():
    """Fonction principale de monitoring."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Analytics Performance Monitor")
    parser.add_argument('--duration', type=int, default=300, help='Dur√©e monitoring (secondes)')
    parser.add_argument('--interval', type=int, default=10, help='Intervalle collecte (secondes)')
    parser.add_argument('--output', help='Fichier de sortie du rapport')
    parser.add_argument('--config', help='Fichier de configuration')
    
    args = parser.parse_args()
    
    # Configuration
    config = get_config() if not args.config else AnalyticsConfig()
    if args.config:
        config.load_from_file(args.config)
    
    # Monitoring
    monitor = PerformanceMonitor(config)
    
    try:
        await monitor.start_monitoring()
        
        print("üöÄ Monitoring d√©marr√©...")
        print(f"üìä Dur√©e: {args.duration}s, Intervalle: {args.interval}s")
        
        start_time = time.time()
        cycle_count = 0
        
        while time.time() - start_time < args.duration:
            cycle_start = time.time()
            
            # Collecte des m√©triques
            system_metrics = monitor.collect_system_metrics()
            db_metrics = await monitor.collect_database_metrics()
            ml_metrics = await monitor.collect_ml_metrics()
            
            # Analyse des performances
            alerts = monitor.analyze_performance()
            
            # Affichage du statut
            cycle_count += 1
            if cycle_count % 5 == 0:  # Affichage toutes les 5 cycles
                print(f"üîÑ Cycle {cycle_count}")
                print(f"   CPU: {system_metrics.cpu_percent:.1f}% | "
                      f"M√©moire: {system_metrics.memory_percent:.1f}% | "
                      f"Alertes: {len([a for a in alerts if a.alert_type in ['warning', 'critical']])}")
            
            # Alertes en temps r√©el
            for alert in alerts:
                if alert.alert_type == 'critical':
                    print(f"üö® CRITIQUE: {alert.message}")
                elif alert.alert_type == 'warning':
                    print(f"‚ö†Ô∏è  WARNING: {alert.message}")
            
            # Attendre le prochain cycle
            cycle_duration = time.time() - cycle_start
            sleep_time = max(0, args.interval - cycle_duration)
            await asyncio.sleep(sleep_time)
        
        # G√©n√©ration du rapport final
        print("\nüìã G√©n√©ration du rapport final...")
        report = monitor.generate_performance_report()
        
        # Sauvegarde
        if args.output:
            await monitor.save_report(report, args.output)
        else:
            timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
            await monitor.save_report(report, f"performance_report_{timestamp}.json")
        
        # Affichage du r√©sum√©
        summary = report['summary']
        print(f"\nüìä R√âSUM√â DE PERFORMANCE:")
        print(f"   Statut: {summary.get('monitoring_status', 'unknown')}")
        print(f"   CPU actuel: {summary.get('current_cpu', 0):.1f}%")
        print(f"   M√©moire actuelle: {summary.get('current_memory', 0):.1f}%")
        print(f"   Alertes actives: {summary.get('active_alerts', 0)}")
        
        print(f"\nüéØ RECOMMANDATIONS:")
        for rec in report['recommendations']:
            print(f"   {rec}")
        
    except KeyboardInterrupt:
        print("\nüõë Arr√™t du monitoring...")
    
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
    
    finally:
        await monitor.stop_monitoring()
        print("‚úÖ Monitoring termin√©")


if __name__ == "__main__":
    # Banni√®re
    print("""
    PerformanceMonitor Analytics v2.0
    ==================================
    üîç Monitoring avanc√© des performances
    üìä Analyse en temps r√©el
    üö® Alertes intelligentes
    üéØ Recommandations automatiques
    
    By Fahed Mlaiel & Performance Team
    """)
    
    asyncio.run(main())
