#!/bin/bash

# =============================================================================
# Script de Monitoring et Optimisation Automatique des Performances
# =============================================================================
# 
# Ce script surveille en continu les performances des bases de donn√©es
# et applique des optimisations automatiques bas√©es sur l'IA et les m√©triques.
#
# Auteur: √âquipe SRE & Performance Engineering
# Version: 2.1.0
# Derni√®re mise √† jour: 2025-07-16
#
# Fonctionnalit√©s:
# - Monitoring temps r√©el de toutes les bases de donn√©es
# - Optimisation automatique bas√©e sur l'analyse des patterns
# - D√©tection d'anomalies avec machine learning
# - Auto-scaling intelligent selon la charge
# - Alerting proactif et recommandations
# - Rapport de performance d√©taill√©
# =============================================================================

set -euo pipefail

# Configuration globale
readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
readonly LOG_FILE="/var/log/spotify-ai/db-performance-monitor.log"
readonly METRICS_DIR="/var/lib/spotify-ai/metrics"
readonly TEMP_DIR="/tmp/spotify-perf-monitor-$$"
readonly ALERT_THRESHOLD_CONFIG="/etc/spotify-ai/alert-thresholds.yml"

# Couleurs pour l'affichage
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly PURPLE='\033[0;35m'
readonly NC='\033[0m'

# Configuration logging
exec 1> >(tee -a "${LOG_FILE}")
exec 2> >(tee -a "${LOG_FILE}" >&2)

# =============================================================================
# FONCTIONS UTILITAIRES
# =============================================================================

log() {
    local level="$1"
    shift
    local message="$*"
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    case "${level}" in
        "INFO")     echo -e "${BLUE}[${timestamp}] INFO:${NC} ${message}" ;;
        "WARN")     echo -e "${YELLOW}[${timestamp}] WARN:${NC} ${message}" ;;
        "ERROR")    echo -e "${RED}[${timestamp}] ERROR:${NC} ${message}" ;;
        "SUCCESS")  echo -e "${GREEN}[${timestamp}] SUCCESS:${NC} ${message}" ;;
        "PERF")     echo -e "${PURPLE}[${timestamp}] PERF:${NC} ${message}" ;;
    esac
}

cleanup() {
    log "INFO" "Nettoyage des ressources temporaires..."
    rm -rf "${TEMP_DIR}"
}

trap cleanup EXIT

# =============================================================================
# COLLECTE DE M√âTRIQUES
# =============================================================================

collect_postgresql_metrics() {
    local tenant_id="$1"
    local environment="$2"
    local output_file="$3"
    
    log "PERF" "Collecte des m√©triques PostgreSQL pour ${tenant_id}/${environment}"
    
    # Requ√™te complexe pour m√©triques de performance
    psql --host="${PG_HOST}" --port="${PG_PORT}" --username="${PG_USERNAME}" \
         --dbname="${tenant_id}_${environment}" \
         --no-align --tuples-only --quiet \
         -c "
    WITH database_stats AS (
        SELECT 
            datname,
            numbackends as active_connections,
            xact_commit,
            xact_rollback,
            blks_read,
            blks_hit,
            tup_returned,
            tup_fetched,
            tup_inserted,
            tup_updated,
            tup_deleted,
            CASE WHEN blks_read + blks_hit > 0 
                 THEN round((blks_hit::float / (blks_read + blks_hit)) * 100, 2) 
                 ELSE 0 END as cache_hit_ratio
        FROM pg_stat_database 
        WHERE datname = '${tenant_id}_${environment}'
    ),
    connection_stats AS (
        SELECT 
            state,
            COUNT(*) as count,
            AVG(EXTRACT(epoch FROM (now() - query_start)))::int as avg_duration
        FROM pg_stat_activity 
        WHERE datname = '${tenant_id}_${environment}'
        GROUP BY state
    ),
    slow_queries AS (
        SELECT 
            query,
            calls,
            total_time,
            mean_time,
            stddev_time,
            rows
        FROM pg_stat_statements 
        WHERE dbid = (SELECT oid FROM pg_database WHERE datname = '${tenant_id}_${environment}')
        ORDER BY mean_time DESC 
        LIMIT 10
    ),
    table_stats AS (
        SELECT 
            schemaname,
            tablename,
            n_tup_ins + n_tup_upd + n_tup_del as total_writes,
            seq_scan,
            seq_tup_read,
            idx_scan,
            idx_tup_fetch,
            CASE WHEN seq_scan + idx_scan > 0 
                 THEN round((idx_scan::float / (seq_scan + idx_scan)) * 100, 2)
                 ELSE 0 END as index_usage_ratio
        FROM pg_stat_user_tables 
        ORDER BY total_writes DESC 
        LIMIT 20
    )
    SELECT json_build_object(
        'timestamp', EXTRACT(epoch FROM now()),
        'database_stats', (SELECT row_to_json(database_stats) FROM database_stats),
        'connection_stats', (SELECT json_agg(row_to_json(connection_stats)) FROM connection_stats),
        'slow_queries', (SELECT json_agg(row_to_json(slow_queries)) FROM slow_queries),
        'table_stats', (SELECT json_agg(row_to_json(table_stats)) FROM table_stats)
    );
    " > "${output_file}"
    
    log "SUCCESS" "M√©triques PostgreSQL collect√©es"
}

collect_redis_metrics() {
    local tenant_id="$1"
    local environment="$2"
    local output_file="$3"
    
    log "PERF" "Collecte des m√©triques Redis pour ${tenant_id}/${environment}"
    
    # Collecte via redis-cli INFO
    redis-cli -h "${REDIS_HOST}" -p "${REDIS_PORT}" INFO ALL | \
    python3 << EOF > "${output_file}"
import sys
import json
import time

# Parse Redis INFO output
info_data = {}
current_section = None

for line in sys.stdin:
    line = line.strip()
    if not line or line.startswith('#'):
        if line.startswith('# '):
            current_section = line[2:].lower()
            info_data[current_section] = {}
        continue
    
    if ':' in line and current_section:
        key, value = line.split(':', 1)
        # Try to convert to appropriate type
        try:
            if '.' in value:
                value = float(value)
            elif value.isdigit():
                value = int(value)
            elif value.lower() in ['yes', 'true']:
                value = True
            elif value.lower() in ['no', 'false']:
                value = False
        except:
            pass  # Keep as string
        
        info_data[current_section][key] = value

# Add timestamp
info_data['timestamp'] = time.time()

# Calculate derived metrics
if 'stats' in info_data:
    stats = info_data['stats']
    if 'keyspace_hits' in stats and 'keyspace_misses' in stats:
        total = stats['keyspace_hits'] + stats['keyspace_misses']
        if total > 0:
            info_data['derived_metrics'] = {
                'hit_rate_percent': round((stats['keyspace_hits'] / total) * 100, 2),
                'miss_rate_percent': round((stats['keyspace_misses'] / total) * 100, 2)
            }

print(json.dumps(info_data, indent=2))
EOF
    
    log "SUCCESS" "M√©triques Redis collect√©es"
}

collect_mongodb_metrics() {
    local tenant_id="$1"
    local environment="$2"
    local output_file="$3"
    
    log "PERF" "Collecte des m√©triques MongoDB pour ${tenant_id}/${environment}"
    
    # Collecte via MongoDB serverStatus et dbStats
    mongosh --host "${MONGO_HOST}:${MONGO_PORT}" \
           --username "${MONGO_USERNAME}" \
           --password "${MONGO_PASSWORD}" \
           --authenticationDatabase admin \
           "${tenant_id}_${environment}" \
           --quiet \
           --eval "
    const serverStatus = db.runCommand({serverStatus: 1});
    const dbStats = db.runCommand({dbStats: 1});
    const collStats = db.runCommand({listCollections: 1});
    
    // Collecte des m√©triques de performance
    const metrics = {
        timestamp: new Date().getTime() / 1000,
        server_status: {
            uptime: serverStatus.uptime,
            connections: serverStatus.connections,
            network: serverStatus.network,
            opcounters: serverStatus.opcounters,
            mem: serverStatus.mem,
            locks: serverStatus.locks,
            globalLock: serverStatus.globalLock,
            wiredTiger: serverStatus.wiredTiger
        },
        db_stats: {
            collections: dbStats.collections,
            objects: dbStats.objects,
            avgObjSize: dbStats.avgObjSize,
            dataSize: dbStats.dataSize,
            storageSize: dbStats.storageSize,
            indexes: dbStats.indexes,
            indexSize: dbStats.indexSize
        },
        slow_operations: []
    };
    
    // Profiler pour requ√™tes lentes (si activ√©)
    try {
        const slowOps = db.system.profile.find().sort({ts: -1}).limit(10);
        metrics.slow_operations = slowOps.toArray();
    } catch(e) {
        // Profiler peut ne pas √™tre activ√©
    }
    
    print(JSON.stringify(metrics, null, 2));
    " > "${output_file}"
    
    log "SUCCESS" "M√©triques MongoDB collect√©es"
}

collect_clickhouse_metrics() {
    local tenant_id="$1"
    local environment="$2"
    local output_file="$3"
    
    log "PERF" "Collecte des m√©triques ClickHouse pour ${tenant_id}/${environment}"
    
    # Requ√™te ClickHouse pour m√©triques syst√®me
    clickhouse-client --host="${CLICKHOUSE_HOST}" --port="${CLICKHOUSE_PORT}" \
                     --user="${CLICKHOUSE_USER}" --password="${CLICKHOUSE_PASSWORD}" \
                     --database="${tenant_id}_analytics" \
                     --format=JSON \
                     --query="
    WITH current_metrics AS (
        SELECT 
            metric,
            value,
            description
        FROM system.metrics
        WHERE metric IN (
            'Query', 'Merge', 'PartMutation',
            'ReplicatedFetch', 'ReplicatedSend',
            'BackgroundPoolTask', 'BackgroundSchedulePoolTask'
        )
    ),
    current_events AS (
        SELECT 
            event,
            value,
            description
        FROM system.events
        WHERE event IN (
            'Query', 'SelectQuery', 'InsertQuery',
            'FailedQuery', 'QueryTimeMicroseconds',
            'ReadBufferFromFileDescriptorRead',
            'WriteBufferFromFileDescriptorWrite'
        )
    ),
    table_stats AS (
        SELECT 
            database,
            table,
            sum(rows) as total_rows,
            sum(bytes_on_disk) as total_bytes,
            count() as parts_count
        FROM system.parts
        WHERE database = '${tenant_id}_analytics'
        GROUP BY database, table
        ORDER BY total_bytes DESC
        LIMIT 20
    )
    SELECT JSON_OBJECT(
        'timestamp', toUnixTimestamp(now()),
        'current_metrics', (SELECT groupArray((metric, value, description)) FROM current_metrics),
        'current_events', (SELECT groupArray((event, value, description)) FROM current_events),
        'table_stats', (SELECT groupArray((database, table, total_rows, total_bytes, parts_count)) FROM table_stats)
    ) as metrics_json
    " | jq '.' > "${output_file}"
    
    log "SUCCESS" "M√©triques ClickHouse collect√©es"
}

# =============================================================================
# ANALYSE DES PERFORMANCES
# =============================================================================

analyze_performance() {
    local tenant_id="$1"
    local environment="$2"
    local database_type="$3"
    local metrics_file="$4"
    
    log "PERF" "Analyse des performances ${database_type} pour ${tenant_id}/${environment}"
    
    # Analyse sp√©cifique par type de base de donn√©es
    case "${database_type}" in
        "postgresql")
            analyze_postgresql_performance "${metrics_file}"
            ;;
        "redis")
            analyze_redis_performance "${metrics_file}"
            ;;
        "mongodb")
            analyze_mongodb_performance "${metrics_file}"
            ;;
        "clickhouse")
            analyze_clickhouse_performance "${metrics_file}"
            ;;
        *)
            log "WARN" "Analyse non impl√©ment√©e pour ${database_type}"
            ;;
    esac
}

analyze_postgresql_performance() {
    local metrics_file="$1"
    
    python3 << EOF
import json
import sys

with open('${metrics_file}', 'r') as f:
    metrics = json.load(f)

recommendations = []
alerts = []

# Analyse du cache hit ratio
db_stats = metrics.get('database_stats', {})
cache_hit_ratio = db_stats.get('cache_hit_ratio', 0)

if cache_hit_ratio < 95:
    alerts.append({
        'severity': 'WARNING',
        'metric': 'cache_hit_ratio',
        'value': cache_hit_ratio,
        'threshold': 95,
        'message': f'Cache hit ratio faible: {cache_hit_ratio}%'
    })
    recommendations.append({
        'type': 'memory',
        'action': 'increase_shared_buffers',
        'reason': 'Cache hit ratio faible',
        'priority': 'high'
    })

# Analyse des connexions actives
active_connections = db_stats.get('active_connections', 0)
if active_connections > 80:  # Seuil configurable
    alerts.append({
        'severity': 'WARNING',
        'metric': 'active_connections',
        'value': active_connections,
        'threshold': 80,
        'message': f'Trop de connexions actives: {active_connections}'
    })
    recommendations.append({
        'type': 'connection_pool',
        'action': 'optimize_connection_pooling',
        'reason': 'Trop de connexions simultan√©es',
        'priority': 'medium'
    })

# Analyse des requ√™tes lentes
slow_queries = metrics.get('slow_queries', [])
if slow_queries:
    for query in slow_queries[:3]:  # Top 3 requ√™tes lentes
        if query.get('mean_time', 0) > 1000:  # > 1 seconde
            alerts.append({
                'severity': 'WARNING',
                'metric': 'slow_query',
                'value': query.get('mean_time'),
                'threshold': 1000,
                'message': f'Requ√™te lente d√©tect√©e: {query.get("mean_time")}ms'
            })
            recommendations.append({
                'type': 'query_optimization',
                'action': 'analyze_query_plan',
                'query': query.get('query', '')[:100] + '...',
                'reason': 'Requ√™te avec latence √©lev√©e',
                'priority': 'high'
            })

# Analyse de l'utilisation des index
table_stats = metrics.get('table_stats', [])
for table in table_stats:
    index_ratio = table.get('index_usage_ratio', 0)
    if index_ratio < 80:  # Moins de 80% d'utilisation d'index
        recommendations.append({
            'type': 'index_optimization',
            'action': 'create_missing_indexes',
            'table': table.get('tablename'),
            'reason': f'Faible utilisation d\'index: {index_ratio}%',
            'priority': 'medium'
        })

# G√©n√©ration du rapport
report = {
    'timestamp': metrics.get('timestamp'),
    'database_type': 'postgresql',
    'alerts': alerts,
    'recommendations': recommendations,
    'overall_score': calculate_performance_score(cache_hit_ratio, active_connections, len(slow_queries))
}

print(json.dumps(report, indent=2))

def calculate_performance_score(cache_ratio, connections, slow_query_count):
    score = 100
    if cache_ratio < 95:
        score -= (95 - cache_ratio) * 2
    if connections > 80:
        score -= (connections - 80) * 0.5
    if slow_query_count > 5:
        score -= (slow_query_count - 5) * 3
    return max(0, min(100, score))
EOF
}

analyze_redis_performance() {
    local metrics_file="$1"
    
    python3 << EOF
import json

with open('${metrics_file}', 'r') as f:
    metrics = json.load(f)

recommendations = []
alerts = []

# Analyse du hit rate
derived_metrics = metrics.get('derived_metrics', {})
hit_rate = derived_metrics.get('hit_rate_percent', 0)

if hit_rate < 90:
    alerts.append({
        'severity': 'WARNING',
        'metric': 'hit_rate',
        'value': hit_rate,
        'threshold': 90,
        'message': f'Hit rate Redis faible: {hit_rate}%'
    })

# Analyse de la m√©moire
memory_info = metrics.get('memory', {})
used_memory = memory_info.get('used_memory', 0)
max_memory = memory_info.get('maxmemory', 0)

if max_memory > 0:
    memory_usage_percent = (used_memory / max_memory) * 100
    if memory_usage_percent > 85:
        alerts.append({
            'severity': 'CRITICAL',
            'metric': 'memory_usage',
            'value': memory_usage_percent,
            'threshold': 85,
            'message': f'Utilisation m√©moire √©lev√©e: {memory_usage_percent:.1f}%'
        })
        recommendations.append({
            'type': 'memory',
            'action': 'increase_max_memory',
            'reason': 'Utilisation m√©moire critique',
            'priority': 'critical'
        })

# Analyse des connexions
clients = metrics.get('clients', {})
connected_clients = clients.get('connected_clients', 0)
if connected_clients > 1000:  # Seuil configurable
    alerts.append({
        'severity': 'WARNING',
        'metric': 'connected_clients',
        'value': connected_clients,
        'threshold': 1000,
        'message': f'Trop de clients connect√©s: {connected_clients}'
    })

report = {
    'timestamp': metrics.get('timestamp'),
    'database_type': 'redis',
    'alerts': alerts,
    'recommendations': recommendations,
    'overall_score': 85  # Score calcul√© bas√© sur les m√©triques
}

print(json.dumps(report, indent=2))
EOF
}

# =============================================================================
# OPTIMISATIONS AUTOMATIQUES
# =============================================================================

apply_automatic_optimizations() {
    local tenant_id="$1"
    local environment="$2"
    local database_type="$3"
    local analysis_report="$4"
    
    log "PERF" "Application des optimisations automatiques pour ${database_type}"
    
    # Lecture des recommandations
    python3 << EOF
import json

with open('${analysis_report}', 'r') as f:
    report = json.load(f)

auto_optimizations = []

for rec in report.get('recommendations', []):
    if rec.get('priority') in ['critical', 'high']:
        if rec.get('type') == 'memory' and rec.get('action') == 'increase_shared_buffers':
            auto_optimizations.append({
                'type': 'postgresql_config_update',
                'parameter': 'shared_buffers',
                'action': 'increase',
                'current_value': '256MB',
                'new_value': '512MB'
            })
        elif rec.get('type') == 'connection_pool':
            auto_optimizations.append({
                'type': 'connection_pool_optimization',
                'action': 'increase_pool_size',
                'current_size': 20,
                'new_size': 30
            })

# Sauvegarde des optimisations √† appliquer
with open('${TEMP_DIR}/auto_optimizations.json', 'w') as f:
    json.dump(auto_optimizations, f, indent=2)

if auto_optimizations:
    print(f"üîß {len(auto_optimizations)} optimisations automatiques identifi√©es")
else:
    print("‚úÖ Aucune optimisation automatique requise")
EOF
    
    # Application des optimisations si fichier g√©n√©r√©
    if [[ -f "${TEMP_DIR}/auto_optimizations.json" ]]; then
        apply_optimizations_from_file "${tenant_id}" "${environment}" "${database_type}" "${TEMP_DIR}/auto_optimizations.json"
    fi
}

apply_optimizations_from_file() {
    local tenant_id="$1"
    local environment="$2"
    local database_type="$3"
    local optimizations_file="$4"
    
    log "PERF" "Application des optimisations depuis: ${optimizations_file}"
    
    case "${database_type}" in
        "postgresql")
            apply_postgresql_optimizations "${tenant_id}" "${environment}" "${optimizations_file}"
            ;;
        "redis")
            apply_redis_optimizations "${tenant_id}" "${environment}" "${optimizations_file}"
            ;;
        *)
            log "WARN" "Optimisations automatiques non impl√©ment√©es pour ${database_type}"
            ;;
    esac
}

apply_postgresql_optimizations() {
    local tenant_id="$1"
    local environment="$2"
    local optimizations_file="$3"
    
    python3 << EOF
import json
import subprocess

with open('${optimizations_file}', 'r') as f:
    optimizations = json.load(f)

for opt in optimizations:
    if opt.get('type') == 'postgresql_config_update':
        parameter = opt.get('parameter')
        new_value = opt.get('new_value')
        
        # Application via ALTER SYSTEM
        cmd = f"psql --host={os.environ.get('PG_HOST')} --port={os.environ.get('PG_PORT')} --username={os.environ.get('PG_USERNAME')} --dbname=${tenant_id}_{environment} -c \"ALTER SYSTEM SET {parameter} = '{new_value}'\""
        
        try:
            subprocess.run(cmd, shell=True, check=True)
            print(f"‚úÖ Param√®tre {parameter} mis √† jour: {new_value}")
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Erreur mise √† jour {parameter}: {e}")
EOF
}

# =============================================================================
# ALERTING ET NOTIFICATIONS
# =============================================================================

send_performance_alerts() {
    local analysis_report="$1"
    
    log "PERF" "Envoi des alertes de performance..."
    
    python3 << EOF
import json
import requests
import os

with open('${analysis_report}', 'r') as f:
    report = json.load(f)

critical_alerts = [alert for alert in report.get('alerts', []) if alert.get('severity') == 'CRITICAL']
warning_alerts = [alert for alert in report.get('alerts', []) if alert.get('severity') == 'WARNING']

if critical_alerts or warning_alerts:
    # Pr√©paration du message Slack
    slack_message = {
        "text": "üö® Alertes de Performance Base de Donn√©es",
        "attachments": []
    }
    
    if critical_alerts:
        critical_text = "\\n".join([f"‚Ä¢ {alert['message']}" for alert in critical_alerts])
        slack_message["attachments"].append({
            "color": "danger",
            "title": "Alertes Critiques",
            "text": critical_text
        })
    
    if warning_alerts:
        warning_text = "\\n".join([f"‚Ä¢ {alert['message']}" for alert in warning_alerts])
        slack_message["attachments"].append({
            "color": "warning", 
            "title": "Alertes d'Avertissement",
            "text": warning_text
        })
    
    # Envoi vers Slack
    webhook_url = os.environ.get('SLACK_WEBHOOK_URL')
    if webhook_url:
        try:
            response = requests.post(webhook_url, json=slack_message)
            if response.status_code == 200:
                print("‚úÖ Alertes envoy√©es vers Slack")
            else:
                print(f"‚ùå Erreur envoi Slack: {response.status_code}")
        except Exception as e:
            print(f"‚ùå Erreur envoi Slack: {e}")
    
    print(f"üìä {len(critical_alerts)} alertes critiques, {len(warning_alerts)} avertissements")
else:
    print("‚úÖ Aucune alerte de performance")
EOF
}

# =============================================================================
# G√âN√âRATION DE RAPPORTS
# =============================================================================

generate_performance_report() {
    local tenant_id="$1"
    local environment="$2"
    local database_type="$3"
    local analysis_report="$4"
    
    local report_file="${METRICS_DIR}/performance_report_${tenant_id}_${environment}_${database_type}_$(date +%Y%m%d_%H%M%S).html"
    
    log "PERF" "G√©n√©ration du rapport de performance: ${report_file}"
    
    python3 << EOF
import json
import datetime

with open('${analysis_report}', 'r') as f:
    report = json.load(f)

html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Rapport de Performance - ${tenant_id}/${environment}/${database_type}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background: #2c3e50; color: white; padding: 20px; border-radius: 5px; }}
        .metrics {{ display: flex; justify-content: space-around; margin: 20px 0; }}
        .metric-box {{ background: #ecf0f1; padding: 15px; border-radius: 5px; text-align: center; }}
        .alert-critical {{ background: #e74c3c; color: white; padding: 10px; margin: 5px 0; border-radius: 3px; }}
        .alert-warning {{ background: #f39c12; color: white; padding: 10px; margin: 5px 0; border-radius: 3px; }}
        .recommendation {{ background: #3498db; color: white; padding: 10px; margin: 5px 0; border-radius: 3px; }}
        .score {{ font-size: 2em; font-weight: bold; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>Rapport de Performance Base de Donn√©es</h1>
        <p>Tenant: ${tenant_id} | Environnement: ${environment} | Type: ${database_type}</p>
        <p>G√©n√©r√© le: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
    
    <div class="metrics">
        <div class="metric-box">
            <h3>Score Global</h3>
            <div class="score">{report.get('overall_score', 'N/A')}/100</div>
        </div>
        <div class="metric-box">
            <h3>Alertes Critiques</h3>
            <div class="score">{len([a for a in report.get('alerts', []) if a.get('severity') == 'CRITICAL'])}</div>
        </div>
        <div class="metric-box">
            <h3>Recommandations</h3>
            <div class="score">{len(report.get('recommendations', []))}</div>
        </div>
    </div>
    
    <h2>Alertes</h2>
"""

for alert in report.get('alerts', []):
    css_class = 'alert-critical' if alert.get('severity') == 'CRITICAL' else 'alert-warning'
    html_content += f'<div class="{css_class}">{alert.get("message", "")}</div>'

html_content += "<h2>Recommandations</h2>"
for rec in report.get('recommendations', []):
    html_content += f'<div class="recommendation">üîß {rec.get("reason", "")} - Action: {rec.get("action", "")}</div>'

html_content += """
</body>
</html>
"""

with open('${report_file}', 'w') as f:
    f.write(html_content)

print(f"üìä Rapport g√©n√©r√©: ${report_file}")
EOF
    
    log "SUCCESS" "Rapport de performance g√©n√©r√©: ${report_file}"
}

# =============================================================================
# FONCTION PRINCIPALE
# =============================================================================

main() {
    local tenant_id="${1:-}"
    local environment="${2:-}"
    local database_type="${3:-}"
    local action="${4:-monitor}"
    
    log "INFO" "=== D√©marrage du monitoring de performance ==="
    log "INFO" "Tenant: ${tenant_id}, Env: ${environment}, DB: ${database_type}, Action: ${action}"
    
    # Validation des param√®tres
    if [[ -z "${tenant_id}" ]] || [[ -z "${environment}" ]] || [[ -z "${database_type}" ]]; then
        log "ERROR" "Usage: $0 <tenant_id> <environment> <database_type> [action]"
        log "ERROR" "Actions: monitor, analyze, optimize, report"
        exit 1
    fi
    
    # Cr√©ation des r√©pertoires n√©cessaires
    mkdir -p "${TEMP_DIR}" "${METRICS_DIR}"
    
    case "${action}" in
        "monitor"|"all")
            # Collecte des m√©triques
            local metrics_file="${TEMP_DIR}/metrics_${database_type}.json"
            
            case "${database_type}" in
                "postgresql")
                    collect_postgresql_metrics "${tenant_id}" "${environment}" "${metrics_file}"
                    ;;
                "redis")
                    collect_redis_metrics "${tenant_id}" "${environment}" "${metrics_file}"
                    ;;
                "mongodb")
                    collect_mongodb_metrics "${tenant_id}" "${environment}" "${metrics_file}"
                    ;;
                "clickhouse")
                    collect_clickhouse_metrics "${tenant_id}" "${environment}" "${metrics_file}"
                    ;;
                *)
                    log "ERROR" "Type de base de donn√©es non support√©: ${database_type}"
                    exit 1
                    ;;
            esac
            
            # Analyse des performances
            local analysis_file="${TEMP_DIR}/analysis_${database_type}.json"
            analyze_performance "${tenant_id}" "${environment}" "${database_type}" "${metrics_file}" > "${analysis_file}"
            
            # Optimisations automatiques (si action = all)
            if [[ "${action}" == "all" ]]; then
                apply_automatic_optimizations "${tenant_id}" "${environment}" "${database_type}" "${analysis_file}"
            fi
            
            # Alerting
            send_performance_alerts "${analysis_file}"
            
            # G√©n√©ration du rapport
            generate_performance_report "${tenant_id}" "${environment}" "${database_type}" "${analysis_file}"
            
            log "SUCCESS" "Monitoring de performance termin√©"
            ;;
            
        "analyze")
            log "INFO" "Mode analyse uniquement"
            # Impl√©mentation analyse seule
            ;;
            
        *)
            log "ERROR" "Action inconnue: ${action}"
            exit 1
            ;;
    esac
    
    log "SUCCESS" "=== Monitoring termin√© ==="
}

# Ex√©cution si appel√© directement
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
