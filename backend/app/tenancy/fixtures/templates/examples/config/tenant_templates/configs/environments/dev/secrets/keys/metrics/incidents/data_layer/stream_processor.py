#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
üîÑ STREAM PROCESSOR ULTRA-AVANC√â - TRAITEMENT TEMPS R√âEL ENTERPRISE
Processeur de flux r√©volutionnaire pour streaming haute performance

Architecture Streaming Enterprise :
‚îú‚îÄ‚îÄ üåä Apache Kafka Integration (Producer/Consumer optimis√©)
‚îú‚îÄ‚îÄ üöÄ Redis Streams (Lightweight streaming)  
‚îú‚îÄ‚îÄ üì° WebSocket Real-time (Bi-directional)
‚îú‚îÄ‚îÄ ‚ö° Event-Driven Architecture (CQRS + Event Sourcing)
‚îú‚îÄ‚îÄ üî• Stream Analytics (Real-time aggregations)
‚îú‚îÄ‚îÄ üß† ML Stream Processing (Online learning)
‚îú‚îÄ‚îÄ üìä Windowing Operations (Tumbling/Sliding/Session)
‚îú‚îÄ‚îÄ üîÑ Backpressure Management (Flow control)
‚îú‚îÄ‚îÄ üõ°Ô∏è Fault Tolerance (Exactly-once semantics)
‚îî‚îÄ‚îÄ üìà Throughput: 1M+ events/sec

D√©velopp√© par l'√©quipe d'experts Achiri avec streaming de niveau industriel
Version: 3.0.0 - Production Ready Enterprise
"""

__version__ = "3.0.0"
__author__ = "Achiri Expert Team - Stream Processing Division"
__license__ = "Enterprise Commercial"

import asyncio
import logging
import sys
import time
import json
import uuid
import hashlib
from datetime import datetime, timedelta
from typing import (
    Dict, List, Optional, Any, Union, Tuple, Set, 
    AsyncGenerator, Callable, TypeVar, Generic, Protocol
)
from dataclasses import dataclass, field, asdict
from enum import Enum, auto
from abc import ABC, abstractmethod
from contextlib import asynccontextmanager
from concurrent.futures import ThreadPoolExecutor
import threading
from collections import defaultdict, deque
import weakref
import struct

# Imports pour streaming
try:
    from kafka import KafkaProducer, KafkaConsumer
    from kafka.errors import KafkaError
    import aiokafka
    from aiokafka import AIOKafkaProducer, AIOKafkaConsumer
    KAFKA_AVAILABLE = True
except ImportError:
    KAFKA_AVAILABLE = False

try:
    import aioredis
    from aioredis import Redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False

try:
    import websockets
    from websockets.server import serve as websocket_serve
    from websockets.client import connect as websocket_connect
    WEBSOCKETS_AVAILABLE = True
except ImportError:
    WEBSOCKETS_AVAILABLE = False

# Traitement de donn√©es
try:
    import numpy as np
    import pandas as pd
    from scipy import stats
    SCIENTIFIC_LIBS_AVAILABLE = True
except ImportError:
    SCIENTIFIC_LIBS_AVAILABLE = False

# S√©rialisation optimis√©e
try:
    import orjson
    import msgpack
    import avro.schema
    import avro.io
    from avro.datafile import DataFileReader, DataFileWriter
    SERIALIZATION_LIBS_AVAILABLE = True
except ImportError:
    SERIALIZATION_LIBS_AVAILABLE = False

# Compression ultra-rapide
try:
    import lz4.frame
    import zstandard as zstd
    import snappy
    COMPRESSION_LIBS_AVAILABLE = True
except ImportError:
    COMPRESSION_LIBS_AVAILABLE = False

# Monitoring et m√©triques
try:
    from prometheus_client import Counter, Histogram, Gauge
    MONITORING_AVAILABLE = True
except ImportError:
    MONITORING_AVAILABLE = False

logger = logging.getLogger(__name__)

# =============================================================================
# √âNUM√âRATIONS ET TYPES
# =============================================================================

class StreamType(Enum):
    """Types de flux support√©s"""
    KAFKA = auto()          # Apache Kafka
    REDIS_STREAM = auto()   # Redis Streams
    WEBSOCKET = auto()      # WebSocket
    HTTP_SSE = auto()       # Server-Sent Events
    GRPC_STREAM = auto()    # gRPC Streaming

class EventType(Enum):
    """Types d'√©v√©nements"""
    METRIC = auto()         # M√©trique de donn√©es
    ALERT = auto()          # Alerte syst√®me
    LOG = auto()            # Log d'application
    TRACE = auto()          # Trace de performance
    BUSINESS = auto()       # √âv√©nement m√©tier
    ML_PREDICTION = auto()  # Pr√©diction ML
    ANOMALY = auto()        # D√©tection d'anomalie

class WindowType(Enum):
    """Types de fen√™tres pour agr√©gations"""
    TUMBLING = auto()       # Fen√™tre fixe
    SLIDING = auto()        # Fen√™tre glissante
    SESSION = auto()        # Fen√™tre de session
    COUNT = auto()          # Fen√™tre par nombre

class CompressionType(Enum):
    """Types de compression"""
    NONE = auto()
    LZ4 = auto()           # Compression rapide
    ZSTD = auto()          # Compression optimale
    SNAPPY = auto()        # Compression Google
    GZIP = auto()          # Standard compression

class SerializationType(Enum):
    """Types de s√©rialisation"""
    JSON = auto()          # JSON standard
    ORJSON = auto()        # JSON optimis√©
    MSGPACK = auto()       # MessagePack binaire
    AVRO = auto()          # Apache Avro
    PROTOBUF = auto()      # Protocol Buffers

# =============================================================================
# MOD√àLES DE DONN√âES
# =============================================================================

@dataclass
class StreamEvent:
    """√âv√©nement de flux unifi√©"""
    event_id: str
    event_type: EventType
    
    # Donn√©es de l'√©v√©nement
    data: Dict[str, Any]
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    # Timing et tra√ßabilit√©
    timestamp: datetime = field(default_factory=datetime.utcnow)
    source: str = "unknown"
    trace_id: Optional[str] = None
    
    # Attributs streaming
    partition_key: Optional[str] = None
    correlation_id: Optional[str] = None
    
    # Qualit√© de donn√©es
    quality_score: float = 1.0
    schema_version: str = "1.0.0"
    
    def to_dict(self) -> Dict[str, Any]:
        """Conversion en dictionnaire"""
        return {
            "event_id": self.event_id,
            "event_type": self.event_type.name,
            "data": self.data,
            "metadata": self.metadata,
            "timestamp": self.timestamp.isoformat(),
            "source": self.source,
            "trace_id": self.trace_id,
            "partition_key": self.partition_key,
            "correlation_id": self.correlation_id,
            "quality_score": self.quality_score,
            "schema_version": self.schema_version
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'StreamEvent':
        """Cr√©ation depuis dictionnaire"""
        return cls(
            event_id=data["event_id"],
            event_type=EventType[data["event_type"]],
            data=data["data"],
            metadata=data.get("metadata", {}),
            timestamp=datetime.fromisoformat(data["timestamp"]),
            source=data.get("source", "unknown"),
            trace_id=data.get("trace_id"),
            partition_key=data.get("partition_key"),
            correlation_id=data.get("correlation_id"),
            quality_score=data.get("quality_score", 1.0),
            schema_version=data.get("schema_version", "1.0.0")
        )

@dataclass
class StreamConfig:
    """Configuration de flux"""
    stream_type: StreamType
    topic_name: str
    
    # Configuration connexion
    connection_params: Dict[str, Any] = field(default_factory=dict)
    
    # S√©rialisation et compression
    serialization: SerializationType = SerializationType.ORJSON
    compression: CompressionType = CompressionType.LZ4
    
    # Param√®tres de performance
    batch_size: int = 1000
    max_batch_time_ms: int = 100
    buffer_size: int = 10000
    
    # Tol√©rance aux pannes
    retry_attempts: int = 3
    retry_backoff_ms: int = 1000
    enable_idempotence: bool = True
    
    # Monitoring
    enable_metrics: bool = True
    health_check_interval_s: int = 30

@dataclass
class WindowConfig:
    """Configuration de fen√™tre d'agr√©gation"""
    window_type: WindowType
    
    # Param√®tres temporels
    size_ms: int = 60000  # 1 minute par d√©faut
    slide_ms: Optional[int] = None  # Pour sliding window
    
    # Param√®tres de session
    session_timeout_ms: int = 300000  # 5 minutes
    
    # Param√®tres de comptage
    count_threshold: int = 1000
    
    # Fonctions d'agr√©gation
    aggregation_functions: List[str] = field(default_factory=lambda: ["sum", "avg", "count", "min", "max"])

@dataclass
class StreamMetrics:
    """M√©triques de streaming"""
    stream_name: str
    stream_type: StreamType
    
    # M√©triques de d√©bit
    events_per_second: float = 0.0
    bytes_per_second: float = 0.0
    
    # M√©triques de latence
    avg_latency_ms: float = 0.0
    p95_latency_ms: float = 0.0
    p99_latency_ms: float = 0.0
    
    # M√©triques d'erreur
    error_rate: float = 0.0
    retry_count: int = 0
    
    # M√©triques de ressources
    buffer_utilization: float = 0.0
    memory_usage_mb: float = 0.0
    
    # √âtat de sant√©
    is_healthy: bool = True
    last_error: Optional[str] = None
    
    # Timestamps
    last_updated: datetime = field(default_factory=datetime.utcnow)

# =============================================================================
# PROCESSEURS DE FLUX SP√âCIALIS√âS
# =============================================================================

class KafkaStreamProcessor:
    """
    üåä PROCESSEUR KAFKA ULTRA-PERFORMANT
    
    Moteur Kafka enterprise avec optimisations avanc√©es :
    - Producteur/Consommateur asynchrone optimis√©
    - Partitioning intelligent
    - Exactly-once semantics
    - Compression adaptative
    - Monitoring en temps r√©el
    """
    
    def __init__(self, config: StreamConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.KafkaStreamProcessor")
        
        # Connexions Kafka
        self._producer = None
        self._consumer = None
        self._admin_client = None
        
        # M√©triques
        self._metrics = StreamMetrics(
            stream_name=config.topic_name,
            stream_type=StreamType.KAFKA
        )
        
        # Buffers et cache
        self._send_buffer = deque()
        self._batch_buffer = []
        self._last_batch_time = time.time()
        
        # √âtat interne
        self._is_running = False
        self._producer_task = None
        self._consumer_task = None
        
        # Monitoring Prometheus
        if MONITORING_AVAILABLE:
            self._events_counter = Counter(
                'kafka_events_total',
                'Total Kafka events',
                ['topic', 'event_type']
            )
            self._latency_histogram = Histogram(
                'kafka_latency_seconds',
                'Kafka processing latency',
                ['topic', 'operation']
            )
    
    async def initialize(self) -> bool:
        """Initialisation du processeur Kafka"""
        try:
            self.logger.info(f"üåä Initialisation Kafka Stream Processor pour {self.config.topic_name}...")
            
            if not KAFKA_AVAILABLE:
                self.logger.error("‚ùå Kafka libs non disponibles")
                return False
            
            # Configuration Kafka optimis√©e
            kafka_config = {
                'bootstrap_servers': self.config.connection_params.get('bootstrap_servers', 'localhost:9092'),
                'security_protocol': self.config.connection_params.get('security_protocol', 'PLAINTEXT'),
                'client_id': f"data_layer_{uuid.uuid4().hex[:8]}",
                **self.config.connection_params
            }
            
            # Initialisation du producteur
            producer_config = {
                **kafka_config,
                'enable_idempotence': self.config.enable_idempotence,
                'acks': 'all',  # Garantie de durabilit√©
                'retries': self.config.retry_attempts,
                'batch_size': self.config.batch_size,
                'linger_ms': self.config.max_batch_time_ms,
                'compression_type': self._get_kafka_compression(),
                'max_in_flight_requests_per_connection': 5,
                'buffer_memory': 32 * 1024 * 1024,  # 32MB buffer
            }
            
            self._producer = AIOKafkaProducer(**producer_config)
            await self._producer.start()
            
            # Initialisation du consommateur
            consumer_config = {
                **kafka_config,
                'group_id': f"data_layer_group_{uuid.uuid4().hex[:8]}",
                'auto_offset_reset': 'latest',
                'enable_auto_commit': False,  # Commit manuel pour exactly-once
                'max_poll_records': self.config.batch_size,
                'fetch_min_bytes': 1024,
                'fetch_max_wait_ms': self.config.max_batch_time_ms,
            }
            
            self._consumer = AIOKafkaConsumer(
                self.config.topic_name,
                **consumer_config
            )
            await self._consumer.start()
            
            # V√©rification de la connectivit√©
            metadata = await self._producer.client.fetch_metadata()
            self.logger.info(f"‚úÖ Kafka connect√©: {len(metadata.brokers)} brokers, {len(metadata.topics)} topics")
            
            self._is_running = True
            
            # D√©marrage des t√¢ches de traitement
            self._producer_task = asyncio.create_task(self._producer_loop())
            self._consumer_task = asyncio.create_task(self._consumer_loop())
            
            self.logger.info(f"‚úÖ Kafka Stream Processor initialis√© pour {self.config.topic_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation Kafka: {e}")
            return False
    
    def _get_kafka_compression(self) -> str:
        """Mapping compression vers Kafka"""
        compression_map = {
            CompressionType.NONE: 'none',
            CompressionType.LZ4: 'lz4',
            CompressionType.ZSTD: 'zstd',
            CompressionType.SNAPPY: 'snappy',
            CompressionType.GZIP: 'gzip'
        }
        return compression_map.get(self.config.compression, 'lz4')
    
    async def send_event(self, event: StreamEvent) -> bool:
        """Envoi d'√©v√©nement asynchrone avec optimisations"""
        if not self._is_running:
            return False
        
        try:
            # S√©rialisation optimis√©e
            serialized_data = await self._serialize_event(event)
            
            # Ajout au buffer
            self._send_buffer.append((event, serialized_data))
            
            # V√©rification des conditions de batch
            if (len(self._send_buffer) >= self.config.batch_size or
                (time.time() - self._last_batch_time) * 1000 >= self.config.max_batch_time_ms):
                await self._flush_buffer()
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur envoi √©v√©nement: {e}")
            self._metrics.error_rate += 1
            return False
    
    async def _serialize_event(self, event: StreamEvent) -> bytes:
        """S√©rialisation optimis√©e d'√©v√©nement"""
        event_dict = event.to_dict()
        
        if self.config.serialization == SerializationType.ORJSON and SERIALIZATION_LIBS_AVAILABLE:
            data = orjson.dumps(event_dict)
        elif self.config.serialization == SerializationType.MSGPACK and SERIALIZATION_LIBS_AVAILABLE:
            data = msgpack.packb(event_dict)
        else:
            data = json.dumps(event_dict).encode('utf-8')
        
        # Compression si activ√©e
        if self.config.compression == CompressionType.LZ4 and COMPRESSION_LIBS_AVAILABLE:
            data = lz4.frame.compress(data)
        elif self.config.compression == CompressionType.ZSTD and COMPRESSION_LIBS_AVAILABLE:
            compressor = zstd.ZstdCompressor()
            data = compressor.compress(data)
        elif self.config.compression == CompressionType.SNAPPY and COMPRESSION_LIBS_AVAILABLE:
            data = snappy.compress(data)
        
        return data
    
    async def _producer_loop(self):
        """Boucle producteur pour traitement par batch"""
        while self._is_running:
            try:
                if self._send_buffer:
                    await self._flush_buffer()
                
                await asyncio.sleep(0.001)  # 1ms yield
                
            except Exception as e:
                self.logger.error(f"‚ùå Erreur producer loop: {e}")
                await asyncio.sleep(1)
    
    async def _flush_buffer(self):
        """Envoi par batch du buffer"""
        if not self._send_buffer:
            return
        
        start_time = time.time()
        
        try:
            # Pr√©paration du batch
            batch = []
            while self._send_buffer and len(batch) < self.config.batch_size:
                event, serialized_data = self._send_buffer.popleft()
                
                # Cl√© de partition pour distribution
                partition_key = event.partition_key or event.event_id
                
                batch.append(
                    self._producer.send(
                        self.config.topic_name,
                        value=serialized_data,
                        key=partition_key.encode('utf-8') if partition_key else None,
                        headers={
                            'event_type': event.event_type.name.encode(),
                            'source': event.source.encode(),
                            'schema_version': event.schema_version.encode()
                        }
                    )
                )
            
            # Envoi du batch
            await asyncio.gather(*batch, return_exceptions=True)
            
            # M√©triques
            processing_time = (time.time() - start_time) * 1000
            self._metrics.events_per_second = len(batch) / (processing_time / 1000) if processing_time > 0 else 0
            self._metrics.avg_latency_ms = processing_time / len(batch) if batch else 0
            
            if MONITORING_AVAILABLE:
                self._events_counter.inc(len(batch))
                self._latency_histogram.observe(processing_time / 1000)
            
            self._last_batch_time = time.time()
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur flush buffer: {e}")
            self._metrics.error_rate += 1
    
    async def consume_events(self) -> AsyncGenerator[StreamEvent, None]:
        """Consommation d'√©v√©nements en streaming"""
        async for message in self._consumer:
            try:
                # D√©s√©rialisation
                event = await self._deserialize_message(message)
                
                # Commit manuel pour exactly-once
                await self._consumer.commit({
                    message.topic_partition: message.offset + 1
                })
                
                # M√©triques
                self._metrics.events_per_second += 1
                
                yield event
                
            except Exception as e:
                self.logger.error(f"‚ùå Erreur consommation message: {e}")
                self._metrics.error_rate += 1
    
    async def _consumer_loop(self):
        """Boucle consommateur pour traitement continu"""
        async for event in self.consume_events():
            try:
                # Traitement des √©v√©nements re√ßus
                await self._process_received_event(event)
                
            except Exception as e:
                self.logger.error(f"‚ùå Erreur traitement √©v√©nement: {e}")
    
    async def _deserialize_message(self, message) -> StreamEvent:
        """D√©s√©rialisation optimis√©e de message"""
        data = message.value
        
        # D√©compression si n√©cessaire
        if self.config.compression == CompressionType.LZ4 and COMPRESSION_LIBS_AVAILABLE:
            data = lz4.frame.decompress(data)
        elif self.config.compression == CompressionType.ZSTD and COMPRESSION_LIBS_AVAILABLE:
            decompressor = zstd.ZstdDecompressor()
            data = decompressor.decompress(data)
        elif self.config.compression == CompressionType.SNAPPY and COMPRESSION_LIBS_AVAILABLE:
            data = snappy.decompress(data)
        
        # D√©s√©rialisation
        if self.config.serialization == SerializationType.ORJSON and SERIALIZATION_LIBS_AVAILABLE:
            event_dict = orjson.loads(data)
        elif self.config.serialization == SerializationType.MSGPACK and SERIALIZATION_LIBS_AVAILABLE:
            event_dict = msgpack.unpackb(data)
        else:
            event_dict = json.loads(data.decode('utf-8'))
        
        return StreamEvent.from_dict(event_dict)
    
    async def _process_received_event(self, event: StreamEvent):
        """Traitement d'√©v√©nement re√ßu"""
        # Override dans les classes d√©riv√©es pour traitement sp√©cifique
        self.logger.debug(f"√âv√©nement re√ßu: {event.event_type.name} - {event.event_id}")
    
    async def get_metrics(self) -> StreamMetrics:
        """R√©cup√©ration des m√©triques actuelles"""
        # Mise √† jour des m√©triques de buffer
        self._metrics.buffer_utilization = len(self._send_buffer) / self.config.buffer_size * 100
        self._metrics.last_updated = datetime.utcnow()
        self._metrics.is_healthy = self._is_running and self._metrics.error_rate < 10
        
        return self._metrics
    
    async def shutdown(self):
        """Arr√™t propre du processeur"""
        self.logger.info("üîÑ Arr√™t Kafka Stream Processor...")
        
        self._is_running = False
        
        # Flush final du buffer
        if self._send_buffer:
            await self._flush_buffer()
        
        # Arr√™t des t√¢ches
        if self._producer_task:
            self._producer_task.cancel()
        if self._consumer_task:
            self._consumer_task.cancel()
        
        # Fermeture des connexions
        if self._producer:
            await self._producer.stop()
        if self._consumer:
            await self._consumer.stop()
        
        self.logger.info("‚úÖ Kafka Stream Processor arr√™t√©")

class RedisStreamProcessor:
    """
    üöÄ PROCESSEUR REDIS STREAMS ULTRA-RAPIDE
    
    Moteur Redis Streams avec optimisations :
    - Redis Streams natif
    - Consumer Groups
    - Acknowledgment automatique
    - Persistence et replay
    - Ultra-faible latence
    """
    
    def __init__(self, config: StreamConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.RedisStreamProcessor")
        
        # Connexion Redis
        self._redis = None
        self._consumer_group = f"group_{uuid.uuid4().hex[:8]}"
        self._consumer_name = f"consumer_{uuid.uuid4().hex[:8]}"
        
        # M√©triques
        self._metrics = StreamMetrics(
            stream_name=config.topic_name,
            stream_type=StreamType.REDIS_STREAM
        )
        
        # √âtat interne
        self._is_running = False
        self._consumer_task = None
    
    async def initialize(self) -> bool:
        """Initialisation du processeur Redis Streams"""
        try:
            self.logger.info(f"üöÄ Initialisation Redis Stream Processor pour {self.config.topic_name}...")
            
            if not REDIS_AVAILABLE:
                self.logger.error("‚ùå Redis libs non disponibles")
                return False
            
            # Connexion Redis
            redis_url = self.config.connection_params.get('url', 'redis://localhost:6379')
            self._redis = aioredis.from_url(redis_url, decode_responses=False)
            
            # Test de connexion
            pong = await self._redis.ping()
            if not pong:
                raise Exception("Redis ping failed")
            
            # Cr√©ation du consumer group
            try:
                await self._redis.xgroup_create(
                    self.config.topic_name,
                    self._consumer_group,
                    id='0',
                    mkstream=True
                )
            except Exception as e:
                if "BUSYGROUP" not in str(e):
                    raise
            
            self._is_running = True
            
            # D√©marrage de la t√¢che de consommation
            self._consumer_task = asyncio.create_task(self._consumer_loop())
            
            self.logger.info(f"‚úÖ Redis Stream Processor initialis√© pour {self.config.topic_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation Redis Streams: {e}")
            return False
    
    async def send_event(self, event: StreamEvent) -> bool:
        """Envoi d'√©v√©nement vers Redis Stream"""
        if not self._is_running:
            return False
        
        try:
            start_time = time.time()
            
            # S√©rialisation
            serialized_data = await self._serialize_event(event)
            
            # Envoi vers Redis Stream
            stream_id = await self._redis.xadd(
                self.config.topic_name,
                {
                    'event_id': event.event_id,
                    'event_type': event.event_type.name,
                    'data': serialized_data,
                    'timestamp': event.timestamp.isoformat(),
                    'source': event.source
                },
                maxlen=100000  # Limite de taille du stream
            )
            
            # M√©triques
            processing_time = (time.time() - start_time) * 1000
            self._metrics.avg_latency_ms = processing_time
            self._metrics.events_per_second += 1
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur envoi Redis Stream: {e}")
            self._metrics.error_rate += 1
            return False
    
    async def _serialize_event(self, event: StreamEvent) -> bytes:
        """S√©rialisation optimis√©e pour Redis"""
        event_dict = event.to_dict()
        
        if SERIALIZATION_LIBS_AVAILABLE:
            return msgpack.packb(event_dict)
        else:
            return json.dumps(event_dict).encode('utf-8')
    
    async def consume_events(self) -> AsyncGenerator[StreamEvent, None]:
        """Consommation d'√©v√©nements Redis Stream"""
        while self._is_running:
            try:
                # Lecture depuis le consumer group
                messages = await self._redis.xreadgroup(
                    self._consumer_group,
                    self._consumer_name,
                    {self.config.topic_name: '>'},
                    count=self.config.batch_size,
                    block=100  # 100ms timeout
                )
                
                for stream_name, stream_messages in messages:
                    for message_id, fields in stream_messages:
                        try:
                            # D√©s√©rialisation
                            event = await self._deserialize_redis_message(fields)
                            
                            # Acknowledgment
                            await self._redis.xack(
                                self.config.topic_name,
                                self._consumer_group,
                                message_id
                            )
                            
                            yield event
                            
                        except Exception as e:
                            self.logger.error(f"‚ùå Erreur traitement message Redis: {e}")
                            
            except Exception as e:
                if self._is_running:
                    self.logger.error(f"‚ùå Erreur consommation Redis Stream: {e}")
                    await asyncio.sleep(1)
    
    async def _deserialize_redis_message(self, fields: Dict[bytes, bytes]) -> StreamEvent:
        """D√©s√©rialisation message Redis"""
        # Extraction des champs
        event_id = fields[b'event_id'].decode('utf-8')
        event_type = EventType[fields[b'event_type'].decode('utf-8')]
        data = fields[b'data']
        timestamp = datetime.fromisoformat(fields[b'timestamp'].decode('utf-8'))
        source = fields[b'source'].decode('utf-8')
        
        # D√©s√©rialisation des donn√©es
        if SERIALIZATION_LIBS_AVAILABLE:
            event_data = msgpack.unpackb(data)
        else:
            event_data = json.loads(data.decode('utf-8'))
        
        return StreamEvent(
            event_id=event_id,
            event_type=event_type,
            data=event_data['data'],
            metadata=event_data.get('metadata', {}),
            timestamp=timestamp,
            source=source
        )
    
    async def _consumer_loop(self):
        """Boucle consommateur Redis"""
        async for event in self.consume_events():
            try:
                await self._process_received_event(event)
            except Exception as e:
                self.logger.error(f"‚ùå Erreur traitement √©v√©nement Redis: {e}")
    
    async def _process_received_event(self, event: StreamEvent):
        """Traitement d'√©v√©nement re√ßu"""
        self.logger.debug(f"√âv√©nement Redis re√ßu: {event.event_type.name} - {event.event_id}")
    
    async def get_metrics(self) -> StreamMetrics:
        """M√©triques Redis Stream"""
        try:
            # Information sur le stream
            info = await self._redis.xinfo_stream(self.config.topic_name)
            self._metrics.buffer_utilization = info.get('length', 0) / 100000 * 100  # Bas√© sur maxlen
        except Exception:
            pass
        
        self._metrics.last_updated = datetime.utcnow()
        self._metrics.is_healthy = self._is_running
        
        return self._metrics
    
    async def shutdown(self):
        """Arr√™t Redis Stream Processor"""
        self.logger.info("üîÑ Arr√™t Redis Stream Processor...")
        
        self._is_running = False
        
        if self._consumer_task:
            self._consumer_task.cancel()
        
        if self._redis:
            await self._redis.close()
        
        self.logger.info("‚úÖ Redis Stream Processor arr√™t√©")

# =============================================================================
# GESTIONNAIRE DE FLUX PRINCIPAL
# =============================================================================

class StreamProcessor:
    """
    üîÑ GESTIONNAIRE DE FLUX PRINCIPAL ULTRA-AVANC√â
    
    Orchestrateur de streaming enterprise :
    - Multi-protocol support (Kafka, Redis, WebSocket)
    - Event routing intelligent
    - Window-based aggregations
    - Real-time analytics
    - Fault tolerance et recovery
    """
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.StreamProcessor")
        
        # Processeurs de flux
        self._processors = {}
        self._active_streams = {}
        
        # Windows et agr√©gations
        self._windows = {}
        self._aggregators = {}
        
        # Event handlers
        self._event_handlers = defaultdict(list)
        
        # M√©triques globales
        self._global_metrics = {
            "total_streams": 0,
            "total_events": 0,
            "total_throughput_eps": 0.0,
            "avg_latency_ms": 0.0
        }
    
    async def initialize(self) -> bool:
        """Initialisation du gestionnaire de flux"""
        try:
            self.logger.info("üîÑ Initialisation Stream Processor Ultra-Avanc√©...")
            
            # V√©rification des d√©pendances
            dependencies = self._check_dependencies()
            self.logger.info(f"üì¶ D√©pendances streaming: {dependencies}")
            
            self.logger.info("‚úÖ Stream Processor initialis√© avec succ√®s")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation Stream Processor: {e}")
            return False
    
    def _check_dependencies(self) -> Dict[str, bool]:
        """V√©rification des d√©pendances streaming"""
        return {
            "kafka": KAFKA_AVAILABLE,
            "redis": REDIS_AVAILABLE,
            "websockets": WEBSOCKETS_AVAILABLE,
            "serialization": SERIALIZATION_LIBS_AVAILABLE,
            "compression": COMPRESSION_LIBS_AVAILABLE,
            "scientific": SCIENTIFIC_LIBS_AVAILABLE
        }
    
    async def create_stream(
        self,
        config: StreamConfig,
        stream_name: str = None
    ) -> str:
        """Cr√©ation d'un flux de donn√©es"""
        stream_name = stream_name or f"{config.stream_type.name}_{config.topic_name}"
        
        try:
            if config.stream_type == StreamType.KAFKA:
                processor = KafkaStreamProcessor(config)
            elif config.stream_type == StreamType.REDIS_STREAM:
                processor = RedisStreamProcessor(config)
            else:
                raise ValueError(f"Type de flux {config.stream_type} non support√©")
            
            # Initialisation
            success = await processor.initialize()
            if not success:
                raise Exception(f"√âchec initialisation processeur {config.stream_type}")
            
            # Enregistrement
            self._processors[stream_name] = processor
            self._active_streams[stream_name] = config
            
            # Mise √† jour m√©triques
            self._global_metrics["total_streams"] += 1
            
            self.logger.info(f"‚úÖ Flux {stream_name} cr√©√© ({config.stream_type.name})")
            return stream_name
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur cr√©ation flux {stream_name}: {e}")
            raise
    
    async def send_event(
        self,
        stream_name: str,
        event: StreamEvent
    ) -> bool:
        """Envoi d'√©v√©nement vers un flux"""
        if stream_name not in self._processors:
            raise ValueError(f"Flux {stream_name} non trouv√©")
        
        try:
            processor = self._processors[stream_name]
            success = await processor.send_event(event)
            
            if success:
                self._global_metrics["total_events"] += 1
                
                # D√©clenchement des handlers
                await self._trigger_event_handlers(event)
            
            return success
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur envoi √©v√©nement vers {stream_name}: {e}")
            return False
    
    async def _trigger_event_handlers(self, event: StreamEvent):
        """D√©clenchement des handlers d'√©v√©nements"""
        handlers = self._event_handlers.get(event.event_type, [])
        
        for handler in handlers:
            try:
                if asyncio.iscoroutinefunction(handler):
                    await handler(event)
                else:
                    handler(event)
            except Exception as e:
                self.logger.error(f"‚ùå Erreur handler √©v√©nement: {e}")
    
    def register_event_handler(
        self,
        event_type: EventType,
        handler: Callable[[StreamEvent], None]
    ):
        """Enregistrement d'un handler d'√©v√©nement"""
        self._event_handlers[event_type].append(handler)
        self.logger.info(f"‚úÖ Handler enregistr√© pour {event_type.name}")
    
    async def create_window(
        self,
        window_name: str,
        config: WindowConfig,
        target_stream: str
    ) -> str:
        """Cr√©ation d'une fen√™tre d'agr√©gation"""
        try:
            window_id = f"{window_name}_{int(time.time())}"
            
            # Cr√©ation de la fen√™tre selon le type
            if config.window_type == WindowType.TUMBLING:
                window = TumblingWindow(config, target_stream)
            elif config.window_type == WindowType.SLIDING:
                window = SlidingWindow(config, target_stream)
            else:
                raise ValueError(f"Type de fen√™tre {config.window_type} non support√©")
            
            # Initialisation
            await window.initialize()
            
            # Enregistrement
            self._windows[window_id] = window
            
            # Connexion au flux cible
            if target_stream in self._processors:
                await self._connect_window_to_stream(window_id, target_stream)
            
            self.logger.info(f"‚úÖ Fen√™tre {window_id} cr√©√©e ({config.window_type.name})")
            return window_id
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur cr√©ation fen√™tre {window_name}: {e}")
            raise
    
    async def _connect_window_to_stream(self, window_id: str, stream_name: str):
        """Connexion d'une fen√™tre √† un flux"""
        window = self._windows[window_id]
        processor = self._processors[stream_name]
        
        # Enregistrement du handler pour alimenter la fen√™tre
        async def window_handler(event: StreamEvent):
            await window.add_event(event)
        
        # Note: Ici il faudrait modifier les processeurs pour supporter les handlers
        # Pour l'instant, on enregistre au niveau global
        self.register_event_handler(EventType.METRIC, window_handler)
    
    async def get_system_status(self) -> Dict[str, Any]:
        """√âtat global du syst√®me de streaming"""
        # Collecte des m√©triques de tous les processeurs
        processor_metrics = {}
        total_throughput = 0.0
        total_latency = 0.0
        healthy_streams = 0
        
        for name, processor in self._processors.items():
            try:
                metrics = await processor.get_metrics()
                processor_metrics[name] = {
                    "stream_type": metrics.stream_type.name,
                    "events_per_second": metrics.events_per_second,
                    "avg_latency_ms": metrics.avg_latency_ms,
                    "error_rate": metrics.error_rate,
                    "is_healthy": metrics.is_healthy
                }
                
                total_throughput += metrics.events_per_second
                total_latency += metrics.avg_latency_ms
                
                if metrics.is_healthy:
                    healthy_streams += 1
                    
            except Exception as e:
                processor_metrics[name] = {"error": str(e)}
        
        # Calcul des moyennes
        avg_latency = total_latency / len(self._processors) if self._processors else 0.0
        
        # Mise √† jour m√©triques globales
        self._global_metrics.update({
            "total_throughput_eps": total_throughput,
            "avg_latency_ms": avg_latency
        })
        
        return {
            "global_metrics": self._global_metrics,
            "active_streams": len(self._processors),
            "healthy_streams": healthy_streams,
            "active_windows": len(self._windows),
            "processors": processor_metrics,
            "dependencies": self._check_dependencies(),
            "timestamp": datetime.utcnow().isoformat()
        }
    
    async def shutdown(self):
        """Arr√™t propre de tous les processeurs"""
        self.logger.info("üîÑ Arr√™t Stream Processor...")
        
        # Arr√™t des fen√™tres
        for window_id, window in self._windows.items():
            try:
                await window.shutdown()
            except Exception as e:
                self.logger.error(f"‚ùå Erreur arr√™t fen√™tre {window_id}: {e}")
        
        # Arr√™t des processeurs
        for name, processor in self._processors.items():
            try:
                await processor.shutdown()
                self.logger.info(f"‚úÖ Processeur {name} arr√™t√©")
            except Exception as e:
                self.logger.error(f"‚ùå Erreur arr√™t processeur {name}: {e}")
        
        # Nettoyage
        self._processors.clear()
        self._active_streams.clear()
        self._windows.clear()
        self._event_handlers.clear()
        
        self.logger.info("‚úÖ Stream Processor arr√™t√©")

# =============================================================================
# WINDOWS ET AGR√âGATIONS
# =============================================================================

class TumblingWindow:
    """Fen√™tre tumbling pour agr√©gations par intervalles fixes"""
    
    def __init__(self, config: WindowConfig, target_stream: str):
        self.config = config
        self.target_stream = target_stream
        self.logger = logging.getLogger(f"{__name__}.TumblingWindow")
        
        # Buffer des √©v√©nements
        self._current_window = []
        self._window_start = None
        self._window_number = 0
        
        # T√¢che de traitement
        self._processing_task = None
        self._is_running = False
    
    async def initialize(self):
        """Initialisation de la fen√™tre"""
        self._is_running = True
        self._window_start = datetime.utcnow()
        self._processing_task = asyncio.create_task(self._window_timer())
        
        self.logger.info(f"‚úÖ Fen√™tre tumbling initialis√©e ({self.config.size_ms}ms)")
    
    async def add_event(self, event: StreamEvent):
        """Ajout d'√©v√©nement √† la fen√™tre courante"""
        if self._is_running:
            self._current_window.append(event)
    
    async def _window_timer(self):
        """Timer pour fermeture automatique des fen√™tres"""
        while self._is_running:
            await asyncio.sleep(self.config.size_ms / 1000)  # Conversion ms vers s
            await self._close_current_window()
    
    async def _close_current_window(self):
        """Fermeture et traitement de la fen√™tre courante"""
        if not self._current_window:
            return
        
        window_events = self._current_window.copy()
        self._current_window.clear()
        
        # Calcul des agr√©gations
        aggregations = await self._compute_aggregations(window_events)
        
        # √âmission du r√©sultat
        await self._emit_window_result(aggregations, window_events)
        
        self._window_number += 1
        self._window_start = datetime.utcnow()
    
    async def _compute_aggregations(self, events: List[StreamEvent]) -> Dict[str, Any]:
        """Calcul des agr√©gations sur la fen√™tre"""
        if not events:
            return {}
        
        # Extraction des valeurs num√©riques
        values = []
        for event in events:
            if 'value' in event.data and isinstance(event.data['value'], (int, float)):
                values.append(event.data['value'])
        
        if not values and SCIENTIFIC_LIBS_AVAILABLE:
            values_array = np.array(values)
            
            aggregations = {
                'count': len(events),
                'sum': np.sum(values_array),
                'avg': np.mean(values_array),
                'min': np.min(values_array),
                'max': np.max(values_array),
                'std': np.std(values_array),
                'median': np.median(values_array)
            }
        else:
            # Fallback sans numpy
            aggregations = {
                'count': len(events),
                'sum': sum(values) if values else 0,
                'avg': sum(values) / len(values) if values else 0,
                'min': min(values) if values else 0,
                'max': max(values) if values else 0
            }
        
        return aggregations
    
    async def _emit_window_result(self, aggregations: Dict[str, Any], events: List[StreamEvent]):
        """√âmission du r√©sultat de fen√™tre"""
        result_event = StreamEvent(
            event_id=f"window_result_{self._window_number}",
            event_type=EventType.METRIC,
            data={
                'window_type': 'tumbling',
                'window_number': self._window_number,
                'aggregations': aggregations,
                'events_count': len(events),
                'window_start': self._window_start.isoformat(),
                'window_end': datetime.utcnow().isoformat()
            },
            source='tumbling_window'
        )
        
        self.logger.info(f"üìä Fen√™tre {self._window_number} ferm√©e: {aggregations.get('count', 0)} √©v√©nements")
    
    async def shutdown(self):
        """Arr√™t de la fen√™tre"""
        self._is_running = False
        
        if self._processing_task:
            self._processing_task.cancel()
        
        # Traitement final de la fen√™tre courante
        await self._close_current_window()

class SlidingWindow:
    """Fen√™tre glissante pour agr√©gations continues"""
    
    def __init__(self, config: WindowConfig, target_stream: str):
        self.config = config
        self.target_stream = target_stream
        self.logger = logging.getLogger(f"{__name__}.SlidingWindow")
        
        # Buffer circulaire des √©v√©nements avec timestamps
        self._events_buffer = deque()
        self._slide_interval = config.slide_ms or (config.size_ms // 4)  # 25% par d√©faut
        
        # T√¢che de traitement
        self._processing_task = None
        self._is_running = False
    
    async def initialize(self):
        """Initialisation de la fen√™tre glissante"""
        self._is_running = True
        self._processing_task = asyncio.create_task(self._sliding_timer())
        
        self.logger.info(f"‚úÖ Fen√™tre glissante initialis√©e ({self.config.size_ms}ms, slide: {self._slide_interval}ms)")
    
    async def add_event(self, event: StreamEvent):
        """Ajout d'√©v√©nement avec timestamp"""
        if self._is_running:
            self._events_buffer.append((event, time.time()))
            await self._cleanup_old_events()
    
    async def _cleanup_old_events(self):
        """Nettoyage des √©v√©nements trop anciens"""
        current_time = time.time()
        window_size_seconds = self.config.size_ms / 1000
        
        while (self._events_buffer and 
               current_time - self._events_buffer[0][1] > window_size_seconds):
            self._events_buffer.popleft()
    
    async def _sliding_timer(self):
        """Timer pour calculs p√©riodiques de la fen√™tre glissante"""
        while self._is_running:
            await asyncio.sleep(self._slide_interval / 1000)
            await self._process_sliding_window()
    
    async def _process_sliding_window(self):
        """Traitement de la fen√™tre glissante courante"""
        await self._cleanup_old_events()
        
        if not self._events_buffer:
            return
        
        # Extraction des √©v√©nements
        current_events = [event for event, _ in self._events_buffer]
        
        # Calcul des agr√©gations
        aggregations = await self._compute_aggregations(current_events)
        
        # √âmission du r√©sultat
        await self._emit_sliding_result(aggregations, current_events)
    
    async def _compute_aggregations(self, events: List[StreamEvent]) -> Dict[str, Any]:
        """Calcul des agr√©gations (similaire √† TumblingWindow)"""
        if not events:
            return {}
        
        values = []
        for event in events:
            if 'value' in event.data and isinstance(event.data['value'], (int, float)):
                values.append(event.data['value'])
        
        if values and SCIENTIFIC_LIBS_AVAILABLE:
            values_array = np.array(values)
            return {
                'count': len(events),
                'sum': np.sum(values_array),
                'avg': np.mean(values_array),
                'min': np.min(values_array),
                'max': np.max(values_array),
                'std': np.std(values_array)
            }
        else:
            return {
                'count': len(events),
                'sum': sum(values) if values else 0,
                'avg': sum(values) / len(values) if values else 0,
                'min': min(values) if values else 0,
                'max': max(values) if values else 0
            }
    
    async def _emit_sliding_result(self, aggregations: Dict[str, Any], events: List[StreamEvent]):
        """√âmission du r√©sultat de fen√™tre glissante"""
        result_event = StreamEvent(
            event_id=f"sliding_window_{int(time.time() * 1000)}",
            event_type=EventType.METRIC,
            data={
                'window_type': 'sliding',
                'aggregations': aggregations,
                'events_count': len(events),
                'window_size_ms': self.config.size_ms,
                'slide_interval_ms': self._slide_interval
            },
            source='sliding_window'
        )
        
        self.logger.debug(f"üìä Fen√™tre glissante: {aggregations.get('count', 0)} √©v√©nements")
    
    async def shutdown(self):
        """Arr√™t de la fen√™tre glissante"""
        self._is_running = False
        
        if self._processing_task:
            self._processing_task.cancel()
        
        # Traitement final
        await self._process_sliding_window()

# =============================================================================
# UTILITAIRES ET EXPORTS
# =============================================================================

async def create_stream_processor() -> StreamProcessor:
    """Cr√©ation et initialisation du processeur de flux"""
    processor = StreamProcessor()
    await processor.initialize()
    return processor

__all__ = [
    # Classes principales
    "StreamProcessor",
    "KafkaStreamProcessor",
    "RedisStreamProcessor",
    "TumblingWindow",
    "SlidingWindow",
    
    # Mod√®les
    "StreamEvent",
    "StreamConfig",
    "WindowConfig",
    "StreamMetrics",
    
    # Enums
    "StreamType",
    "EventType",
    "WindowType",
    "CompressionType",
    "SerializationType",
    
    # Utilitaires
    "create_stream_processor"
]
