"""
Syst√®me de Tests et Validation Ultra-Avanc√©

Ce module fournit un framework complet de tests automatis√©s pour :
- Validation des r√®gles d'alertes avec ML
- Tests de charge et performance en temps r√©el
- Simulation d'environnements complexes
- Validation de compliance automatique
- Tests de r√©gression intelligents
- Benchmarking et optimisation continue

√âquipe Engineering:
‚úÖ Lead Dev + Architecte IA : Fahed Mlaiel
‚úÖ QA Engineer (Test Automation/Performance)
‚úÖ DevOps Engineer (CI/CD/Infrastructure)
‚úÖ Security Engineer (Penetration Testing)

Copyright: ¬© 2025 Spotify Technology S.A.
"""

import asyncio
import pytest
import unittest
from unittest.mock import Mock, patch, AsyncMock
import json
import time
import logging
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, as_completed
import aiohttp
import asyncpg
import redis

# Test frameworks et outils
from locust import HttpUser, task, between
import hypothesis
from hypothesis import strategies as st
from hypothesis import given, settings, Verbosity
from faker import Faker
import factory

# Monitoring et m√©triques de tests
from prometheus_client import Counter, Histogram, Gauge
import structlog

logger = structlog.get_logger(__name__)
fake = Faker()

# M√©triques de test
TESTS_EXECUTED = Counter('tests_executed_total', 'Total tests executed', ['test_type', 'result'])
TEST_EXECUTION_TIME = Histogram('test_execution_duration_seconds', 'Test execution time', ['test_type'])
VALIDATION_ACCURACY = Gauge('validation_accuracy', 'Validation accuracy', ['validation_type'])
PERFORMANCE_BASELINE = Gauge('performance_baseline_ms', 'Performance baseline', ['operation'])

@dataclass
class TestConfiguration:
    """Configuration des tests"""
    enable_load_testing: bool = True
    enable_chaos_testing: bool = True
    enable_ml_validation: bool = True
    max_concurrent_tests: int = 50
    test_timeout: int = 300
    performance_threshold_ms: int = 1000
    accuracy_threshold: float = 0.95
    chaos_intensity: float = 0.3

@dataclass
class TestResult:
    """R√©sultat d'un test"""
    test_id: str
    test_type: str
    status: str
    execution_time: float
    details: Dict[str, Any]
    metrics: Dict[str, float]
    errors: List[str]
    warnings: List[str]
    timestamp: datetime

@dataclass
class ValidationRule:
    """R√®gle de validation"""
    rule_id: str
    rule_type: str
    condition: str
    expected_result: Any
    tolerance: float = 0.0
    priority: str = "medium"

class AlertRuleFactory(factory.Factory):
    """Factory pour g√©n√©rer des r√®gles d'alerte de test"""
    
    class Meta:
        model = dict
    
    rule_id = factory.Sequence(lambda n: f"test_rule_{n}")
    name = factory.Faker('sentence', nb_words=3)
    description = factory.Faker('text', max_nb_chars=200)
    severity = factory.Faker('random_element', elements=['low', 'medium', 'high', 'critical'])
    category = factory.Faker('random_element', elements=['infrastructure', 'application', 'security'])
    
    @factory.LazyAttribute
    def conditions(obj):
        return [{
            'type': fake.random_element(['threshold', 'pattern', 'anomaly']),
            'metric': fake.random_element(['cpu_usage', 'memory_usage', 'error_rate', 'response_time']),
            'operator': fake.random_element(['>', '<', '>=', '<=', '==']),
            'value': fake.random_int(1, 100),
            'time_window': fake.random_int(60, 3600)
        }]

class MetricsDataFactory(factory.Factory):
    """Factory pour g√©n√©rer des donn√©es de m√©triques de test"""
    
    class Meta:
        model = dict
    
    timestamp = factory.LazyFunction(datetime.utcnow)
    cpu_usage = factory.Faker('random_int', min=0, max=100)
    memory_usage = factory.Faker('random_int', min=0, max=100)
    disk_usage = factory.Faker('random_int', min=0, max=100)
    network_io = factory.Faker('random_int', min=0, max=1000)
    error_rate = factory.Faker('pyfloat', min_value=0.0, max_value=0.1, right_digits=4)
    response_time = factory.Faker('random_int', min=50, max=5000)
    active_users = factory.Faker('random_int', min=0, max=10000)

class RuleValidationEngine:
    """Moteur de validation des r√®gles"""
    
    def __init__(self, config: TestConfiguration):
        self.config = config
        self.validation_rules: List[ValidationRule] = []
        self.test_results: List[TestResult] = []
        self.ml_validators: Dict[str, Any] = {}
        
    async def validate_alert_rule(self, rule: Dict[str, Any], 
                                test_data: List[Dict[str, Any]]) -> TestResult:
        """Valide une r√®gle d'alerte avec des donn√©es de test"""
        try:
            start_time = time.time()
            test_id = f"rule_validation_{rule['rule_id']}_{int(time.time())}"
            
            errors = []
            warnings = []
            metrics = {}
            
            # Validation de la structure
            structure_valid = self._validate_rule_structure(rule)
            if not structure_valid['valid']:
                errors.extend(structure_valid['errors'])
            
            # Validation s√©mantique
            semantic_valid = await self._validate_rule_semantics(rule)
            if not semantic_valid['valid']:
                errors.extend(semantic_valid['errors'])
            warnings.extend(semantic_valid.get('warnings', []))
            
            # Validation avec donn√©es de test
            data_validation = await self._validate_with_test_data(rule, test_data)
            metrics.update(data_validation['metrics'])
            
            if data_validation['accuracy'] < self.config.accuracy_threshold:
                warnings.append(f"Accuracy below threshold: {data_validation['accuracy']:.2f}")
            
            # Validation de performance
            performance_result = await self._validate_performance(rule, test_data)
            metrics.update(performance_result['metrics'])
            
            if performance_result['avg_execution_time'] > self.config.performance_threshold_ms:
                warnings.append(f"Performance below threshold: {performance_result['avg_execution_time']:.2f}ms")
            
            # Validation ML (si activ√©e)
            if self.config.enable_ml_validation:
                ml_validation = await self._validate_with_ml(rule, test_data)
                metrics.update(ml_validation['metrics'])
                warnings.extend(ml_validation.get('warnings', []))
            
            execution_time = time.time() - start_time
            status = 'failed' if errors else ('warning' if warnings else 'passed')
            
            result = TestResult(
                test_id=test_id,
                test_type='rule_validation',
                status=status,
                execution_time=execution_time,
                details={
                    'rule_id': rule['rule_id'],
                    'structure_validation': structure_valid,
                    'semantic_validation': semantic_valid,
                    'data_validation': data_validation,
                    'performance_validation': performance_result,
                    'ml_validation': ml_validation if self.config.enable_ml_validation else None
                },
                metrics=metrics,
                errors=errors,
                warnings=warnings,
                timestamp=datetime.utcnow()
            )
            
            self.test_results.append(result)
            
            TESTS_EXECUTED.labels(test_type='rule_validation', result=status).inc()
            TEST_EXECUTION_TIME.labels(test_type='rule_validation').observe(execution_time)
            VALIDATION_ACCURACY.labels(validation_type='rule_validation').set(
                data_validation.get('accuracy', 0.0)
            )
            
            logger.info(
                "Rule validation completed",
                test_id=test_id,
                rule_id=rule['rule_id'],
                status=status,
                execution_time=execution_time,
                accuracy=data_validation.get('accuracy', 0.0)
            )
            
            return result
            
        except Exception as e:
            logger.error("Rule validation failed", error=str(e), rule_id=rule.get('rule_id'))
            return TestResult(
                test_id=f"failed_{int(time.time())}",
                test_type='rule_validation',
                status='error',
                execution_time=time.time() - start_time,
                details={'error': str(e)},
                metrics={},
                errors=[str(e)],
                warnings=[],
                timestamp=datetime.utcnow()
            )
    
    def _validate_rule_structure(self, rule: Dict[str, Any]) -> Dict[str, Any]:
        """Valide la structure d'une r√®gle"""
        required_fields = ['rule_id', 'name', 'conditions']
        optional_fields = ['description', 'severity', 'category', 'actions', 'metadata']
        
        errors = []
        warnings = []
        
        # V√©rification des champs obligatoires
        for field in required_fields:
            if field not in rule:
                errors.append(f"Missing required field: {field}")
        
        # Validation des types
        if 'rule_id' in rule and not isinstance(rule['rule_id'], str):
            errors.append("rule_id must be a string")
        
        if 'conditions' in rule:
            if not isinstance(rule['conditions'], list):
                errors.append("conditions must be a list")
            elif len(rule['conditions']) == 0:
                errors.append("conditions list cannot be empty")
            else:
                for i, condition in enumerate(rule['conditions']):
                    if not isinstance(condition, dict):
                        errors.append(f"condition[{i}] must be a dictionary")
                    else:
                        required_condition_fields = ['type', 'metric', 'operator', 'value']
                        for field in required_condition_fields:
                            if field not in condition:
                                errors.append(f"condition[{i}] missing required field: {field}")
        
        # V√©rification des valeurs valides
        if 'severity' in rule:
            valid_severities = ['low', 'medium', 'high', 'critical', 'emergency']
            if rule['severity'] not in valid_severities:
                warnings.append(f"Unknown severity: {rule['severity']}")
        
        return {
            'valid': len(errors) == 0,
            'errors': errors,
            'warnings': warnings
        }
    
    async def _validate_rule_semantics(self, rule: Dict[str, Any]) -> Dict[str, Any]:
        """Valide la s√©mantique d'une r√®gle"""
        errors = []
        warnings = []
        
        # Validation des conditions
        if 'conditions' in rule:
            for i, condition in enumerate(rule['conditions']):
                # Validation des op√©rateurs
                valid_operators = ['>', '<', '>=', '<=', '==', '!=', 'contains', 'matches']
                if condition.get('operator') not in valid_operators:
                    errors.append(f"condition[{i}] invalid operator: {condition.get('operator')}")
                
                # Validation des m√©triques
                common_metrics = [
                    'cpu_usage', 'memory_usage', 'disk_usage', 'network_io',
                    'error_rate', 'response_time', 'request_count', 'active_users'
                ]
                if condition.get('metric') not in common_metrics:
                    warnings.append(f"condition[{i}] unknown metric: {condition.get('metric')}")
                
                # Validation des valeurs
                if 'value' in condition:
                    value = condition['value']
                    metric = condition.get('metric', '')
                    
                    if 'usage' in metric and isinstance(value, (int, float)):
                        if value < 0 or value > 100:
                            warnings.append(f"condition[{i}] unusual percentage value: {value}")
                    
                    if 'rate' in metric and isinstance(value, (int, float)):
                        if value < 0 or value > 1:
                            warnings.append(f"condition[{i}] unusual rate value: {value}")
        
        # Validation de la coh√©rence
        if 'severity' in rule and 'conditions' in rule:
            high_severity_rules = ['critical', 'high', 'emergency']
            if rule['severity'] in high_severity_rules:
                if len(rule['conditions']) > 5:
                    warnings.append("High severity rule with many conditions may be too complex")
        
        return {
            'valid': len(errors) == 0,
            'errors': errors,
            'warnings': warnings
        }
    
    async def _validate_with_test_data(self, rule: Dict[str, Any], 
                                     test_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Valide une r√®gle avec des donn√©es de test"""
        try:
            total_tests = len(test_data)
            true_positives = 0
            false_positives = 0
            true_negatives = 0
            false_negatives = 0
            
            for data_point in test_data:
                # Simulation de l'√©valuation de la r√®gle
                rule_triggered = self._evaluate_rule_conditions(rule, data_point)
                
                # D√©termination si c'est une vraie anomalie (simulation)
                actual_anomaly = self._is_actual_anomaly(data_point)
                
                if rule_triggered and actual_anomaly:
                    true_positives += 1
                elif rule_triggered and not actual_anomaly:
                    false_positives += 1
                elif not rule_triggered and not actual_anomaly:
                    true_negatives += 1
                else:
                    false_negatives += 1
            
            # Calcul des m√©triques
            precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
            recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
            accuracy = (true_positives + true_negatives) / total_tests if total_tests > 0 else 0
            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            return {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1_score,
                'metrics': {
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall,
                    'f1_score': f1_score,
                    'true_positives': true_positives,
                    'false_positives': false_positives,
                    'true_negatives': true_negatives,
                    'false_negatives': false_negatives
                }
            }
            
        except Exception as e:
            logger.error("Test data validation failed", error=str(e))
            return {'accuracy': 0.0, 'metrics': {}}
    
    def _evaluate_rule_conditions(self, rule: Dict[str, Any], data: Dict[str, Any]) -> bool:
        """√âvalue les conditions d'une r√®gle contre des donn√©es"""
        try:
            conditions = rule.get('conditions', [])
            results = []
            
            for condition in conditions:
                metric = condition.get('metric')
                operator = condition.get('operator')
                value = condition.get('value')
                
                if metric in data:
                    data_value = data[metric]
                    
                    if operator == '>':
                        results.append(data_value > value)
                    elif operator == '<':
                        results.append(data_value < value)
                    elif operator == '>=':
                        results.append(data_value >= value)
                    elif operator == '<=':
                        results.append(data_value <= value)
                    elif operator == '==':
                        results.append(data_value == value)
                    elif operator == '!=':
                        results.append(data_value != value)
                    else:
                        results.append(False)
                else:
                    results.append(False)
            
            # Par d√©faut, toutes les conditions doivent √™tre vraies (AND)
            return all(results) if results else False
            
        except Exception as e:
            logger.error("Rule evaluation failed", error=str(e))
            return False
    
    def _is_actual_anomaly(self, data: Dict[str, Any]) -> bool:
        """D√©termine si un point de donn√©es est une vraie anomalie (simulation)"""
        # Simulation simple bas√©e sur des seuils
        cpu = data.get('cpu_usage', 0)
        memory = data.get('memory_usage', 0)
        error_rate = data.get('error_rate', 0)
        response_time = data.get('response_time', 0)
        
        # Conditions d'anomalie simul√©es
        return (
            cpu > 90 or 
            memory > 95 or 
            error_rate > 0.05 or 
            response_time > 3000
        )
    
    async def _validate_performance(self, rule: Dict[str, Any], 
                                  test_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Valide les performances d'ex√©cution d'une r√®gle"""
        execution_times = []
        
        for _ in range(min(100, len(test_data))):  # Test sur 100 √©chantillons max
            data_point = fake.random_element(test_data)
            
            start_time = time.time()
            self._evaluate_rule_conditions(rule, data_point)
            execution_time = (time.time() - start_time) * 1000  # en ms
            
            execution_times.append(execution_time)
        
        avg_execution_time = np.mean(execution_times)
        p95_execution_time = np.percentile(execution_times, 95)
        max_execution_time = np.max(execution_times)
        
        PERFORMANCE_BASELINE.labels(operation='rule_evaluation').set(avg_execution_time)
        
        return {
            'avg_execution_time': avg_execution_time,
            'p95_execution_time': p95_execution_time,
            'max_execution_time': max_execution_time,
            'metrics': {
                'avg_execution_time_ms': avg_execution_time,
                'p95_execution_time_ms': p95_execution_time,
                'max_execution_time_ms': max_execution_time
            }
        }
    
    async def _validate_with_ml(self, rule: Dict[str, Any], 
                              test_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Validation avec ML (anomaly detection)"""
        try:
            from sklearn.ensemble import IsolationForest
            from sklearn.preprocessing import StandardScaler
            
            # Pr√©paration des donn√©es
            if len(test_data) < 10:
                return {'metrics': {}, 'warnings': ['Insufficient data for ML validation']}
            
            features = []
            numeric_fields = ['cpu_usage', 'memory_usage', 'error_rate', 'response_time']
            
            for data_point in test_data:
                feature_vector = []
                for field in numeric_fields:
                    feature_vector.append(data_point.get(field, 0))
                features.append(feature_vector)
            
            features_array = np.array(features)
            
            # Normalisation
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(features_array)
            
            # Mod√®le d'isolation forest
            isolation_forest = IsolationForest(contamination=0.1, random_state=42)
            anomaly_predictions = isolation_forest.fit_predict(features_scaled)
            
            # Comparaison avec les pr√©dictions de la r√®gle
            rule_predictions = []
            for data_point in test_data:
                rule_predictions.append(1 if self._evaluate_rule_conditions(rule, data_point) else -1)
            
            # Calcul de l'accord entre ML et r√®gle
            agreement = np.mean(np.array(rule_predictions) == anomaly_predictions)
            
            warnings = []
            if agreement < 0.7:
                warnings.append(f"Low agreement between rule and ML model: {agreement:.2f}")
            
            return {
                'ml_agreement': agreement,
                'anomalies_detected_ml': np.sum(anomaly_predictions == -1),
                'anomalies_detected_rule': np.sum(np.array(rule_predictions) == 1),
                'metrics': {
                    'ml_rule_agreement': agreement,
                    'ml_anomalies': int(np.sum(anomaly_predictions == -1)),
                    'rule_anomalies': int(np.sum(np.array(rule_predictions) == 1))
                },
                'warnings': warnings
            }
            
        except Exception as e:
            logger.error("ML validation failed", error=str(e))
            return {'metrics': {}, 'warnings': [f'ML validation error: {str(e)}']}

class LoadTestingFramework:
    """Framework de tests de charge"""
    
    def __init__(self, config: TestConfiguration):
        self.config = config
        
    async def run_load_test(self, target_url: str, test_duration: int = 60) -> Dict[str, Any]:
        """Ex√©cute un test de charge"""
        try:
            start_time = time.time()
            
            # Configuration du test de charge
            results = {
                'test_id': f"load_test_{int(time.time())}",
                'target_url': target_url,
                'duration': test_duration,
                'concurrent_users': self.config.max_concurrent_tests,
                'requests_sent': 0,
                'requests_successful': 0,
                'requests_failed': 0,
                'avg_response_time': 0.0,
                'p95_response_time': 0.0,
                'errors': []
            }
            
            # Collecte des temps de r√©ponse
            response_times = []
            
            async with aiohttp.ClientSession() as session:
                tasks = []
                
                # Cr√©ation des t√¢ches concurrentes
                for user_id in range(self.config.max_concurrent_tests):
                    task = self._simulate_user_load(session, target_url, test_duration, user_id)
                    tasks.append(task)
                
                # Ex√©cution des tests
                completed_tasks = await asyncio.gather(*tasks, return_exceptions=True)
                
                # Agr√©gation des r√©sultats
                for task_result in completed_tasks:
                    if isinstance(task_result, Exception):
                        results['errors'].append(str(task_result))
                        results['requests_failed'] += 1
                    elif isinstance(task_result, dict):
                        results['requests_sent'] += task_result.get('requests_sent', 0)
                        results['requests_successful'] += task_result.get('requests_successful', 0)
                        results['requests_failed'] += task_result.get('requests_failed', 0)
                        response_times.extend(task_result.get('response_times', []))
            
            # Calcul des statistiques finales
            if response_times:
                results['avg_response_time'] = np.mean(response_times)
                results['p95_response_time'] = np.percentile(response_times, 95)
                results['min_response_time'] = np.min(response_times)
                results['max_response_time'] = np.max(response_times)
            
            results['total_execution_time'] = time.time() - start_time
            results['requests_per_second'] = results['requests_sent'] / results['total_execution_time']
            
            # M√©triques Prometheus
            TESTS_EXECUTED.labels(test_type='load_test', result='completed').inc()
            TEST_EXECUTION_TIME.labels(test_type='load_test').observe(results['total_execution_time'])
            
            logger.info(
                "Load test completed",
                test_id=results['test_id'],
                requests_sent=results['requests_sent'],
                avg_response_time=results['avg_response_time'],
                rps=results['requests_per_second']
            )
            
            return results
            
        except Exception as e:
            logger.error("Load test failed", error=str(e))
            return {'error': str(e)}
    
    async def _simulate_user_load(self, session: aiohttp.ClientSession, 
                                url: str, duration: int, user_id: int) -> Dict[str, Any]:
        """Simule la charge d'un utilisateur"""
        end_time = time.time() + duration
        user_results = {
            'user_id': user_id,
            'requests_sent': 0,
            'requests_successful': 0,
            'requests_failed': 0,
            'response_times': []
        }
        
        while time.time() < end_time:
            try:
                request_start = time.time()
                
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                    response_time = (time.time() - request_start) * 1000  # ms
                    user_results['response_times'].append(response_time)
                    user_results['requests_sent'] += 1
                    
                    if response.status == 200:
                        user_results['requests_successful'] += 1
                    else:
                        user_results['requests_failed'] += 1
                
                # D√©lai entre les requ√™tes
                await asyncio.sleep(0.1)
                
            except Exception as e:
                user_results['requests_failed'] += 1
                user_results['requests_sent'] += 1
        
        return user_results

class ChaosTestingEngine:
    """Moteur de tests de chaos"""
    
    def __init__(self, config: TestConfiguration):
        self.config = config
        self.chaos_scenarios = [
            'network_delay',
            'service_failure',
            'resource_exhaustion',
            'database_connection_loss',
            'memory_leak_simulation'
        ]
    
    async def run_chaos_experiment(self, target_system: str, 
                                 experiment_duration: int = 300) -> Dict[str, Any]:
        """Ex√©cute un experiment de chaos engineering"""
        try:
            experiment_id = f"chaos_{int(time.time())}"
            
            # S√©lection al√©atoire d'un sc√©nario
            scenario = fake.random_element(self.chaos_scenarios)
            
            results = {
                'experiment_id': experiment_id,
                'scenario': scenario,
                'target_system': target_system,
                'duration': experiment_duration,
                'impact_level': self.config.chaos_intensity,
                'metrics_before': {},
                'metrics_during': {},
                'metrics_after': {},
                'system_recovery_time': 0.0,
                'alerts_triggered': [],
                'issues_detected': []
            }
            
            # Collecte des m√©triques avant l'exp√©rience
            results['metrics_before'] = await self._collect_system_metrics(target_system)
            
            # Ex√©cution du sc√©nario de chaos
            chaos_task = asyncio.create_task(
                self._execute_chaos_scenario(scenario, target_system, experiment_duration)
            )
            
            # Monitoring pendant l'exp√©rience
            monitoring_task = asyncio.create_task(
                self._monitor_during_chaos(target_system, experiment_duration)
            )
            
            # Attente de completion
            chaos_result, monitoring_result = await asyncio.gather(
                chaos_task, monitoring_task, return_exceptions=True
            )
            
            if isinstance(chaos_result, dict):
                results.update(chaos_result)
            
            if isinstance(monitoring_result, dict):
                results['metrics_during'] = monitoring_result.get('metrics', {})
                results['alerts_triggered'] = monitoring_result.get('alerts', [])
            
            # Collecte des m√©triques apr√®s l'exp√©rience
            await asyncio.sleep(30)  # D√©lai pour la r√©cup√©ration
            results['metrics_after'] = await self._collect_system_metrics(target_system)
            
            # Calcul du temps de r√©cup√©ration
            results['system_recovery_time'] = self._calculate_recovery_time(
                results['metrics_before'], 
                results['metrics_after']
            )
            
            # Analyse des r√©sultats
            results['analysis'] = self._analyze_chaos_results(results)
            
            TESTS_EXECUTED.labels(test_type='chaos_test', result='completed').inc()
            
            logger.info(
                "Chaos experiment completed",
                experiment_id=experiment_id,
                scenario=scenario,
                recovery_time=results['system_recovery_time'],
                alerts_count=len(results['alerts_triggered'])
            )
            
            return results
            
        except Exception as e:
            logger.error("Chaos experiment failed", error=str(e))
            return {'error': str(e)}
    
    async def _collect_system_metrics(self, target_system: str) -> Dict[str, float]:
        """Collecte les m√©triques syst√®me"""
        # Simulation de collecte de m√©triques
        return {
            'cpu_usage': fake.random_int(20, 80),
            'memory_usage': fake.random_int(30, 70),
            'response_time': fake.random_int(100, 500),
            'error_rate': fake.pyfloat(min_value=0.0, max_value=0.02, right_digits=4),
            'requests_per_second': fake.random_int(50, 200)
        }
    
    async def _execute_chaos_scenario(self, scenario: str, target: str, duration: int) -> Dict[str, Any]:
        """Ex√©cute un sc√©nario de chaos sp√©cifique"""
        scenario_results = {
            'scenario_executed': scenario,
            'execution_successful': True,
            'impact_details': {}
        }
        
        if scenario == 'network_delay':
            # Simulation de latence r√©seau
            scenario_results['impact_details'] = {
                'delay_added_ms': fake.random_int(50, 500),
                'affected_connections': fake.random_int(10, 100)
            }
            
        elif scenario == 'service_failure':
            # Simulation de panne de service
            scenario_results['impact_details'] = {
                'service_down_duration': fake.random_int(30, 180),
                'affected_endpoints': fake.random_int(1, 5)
            }
            
        elif scenario == 'resource_exhaustion':
            # Simulation d'√©puisement des ressources
            scenario_results['impact_details'] = {
                'resource_type': fake.random_element(['cpu', 'memory', 'disk']),
                'utilization_percentage': fake.random_int(90, 99)
            }
            
        elif scenario == 'database_connection_loss':
            # Simulation de perte de connexion DB
            scenario_results['impact_details'] = {
                'connection_loss_duration': fake.random_int(15, 120),
                'affected_queries': fake.random_int(50, 500)
            }
            
        # Simulation de l'ex√©cution
        await asyncio.sleep(duration)
        
        return scenario_results
    
    async def _monitor_during_chaos(self, target: str, duration: int) -> Dict[str, Any]:
        """Monitor le syst√®me pendant l'exp√©rience de chaos"""
        metrics_history = []
        alerts = []
        
        end_time = time.time() + duration
        
        while time.time() < end_time:
            # Collecte p√©riodique des m√©triques
            current_metrics = await self._collect_system_metrics(target)
            metrics_history.append({
                'timestamp': datetime.utcnow().isoformat(),
                'metrics': current_metrics
            })
            
            # D√©tection d'alertes simul√©es
            if current_metrics['cpu_usage'] > 90:
                alerts.append({
                    'timestamp': datetime.utcnow().isoformat(),
                    'alert_type': 'high_cpu',
                    'value': current_metrics['cpu_usage']
                })
            
            if current_metrics['error_rate'] > 0.1:
                alerts.append({
                    'timestamp': datetime.utcnow().isoformat(),
                    'alert_type': 'high_error_rate',
                    'value': current_metrics['error_rate']
                })
            
            await asyncio.sleep(10)  # Monitoring toutes les 10 secondes
        
        return {
            'metrics': metrics_history,
            'alerts': alerts
        }
    
    def _calculate_recovery_time(self, metrics_before: Dict, metrics_after: Dict) -> float:
        """Calcule le temps de r√©cup√©ration du syst√®me"""
        # Simulation du calcul de r√©cup√©ration
        recovery_factors = []
        
        for metric in ['cpu_usage', 'memory_usage', 'response_time', 'error_rate']:
            if metric in metrics_before and metric in metrics_after:
                before_val = metrics_before[metric]
                after_val = metrics_after[metric]
                
                if metric in ['cpu_usage', 'memory_usage', 'response_time', 'error_rate']:
                    # Pour ces m√©triques, plus proche de la valeur avant = meilleure r√©cup√©ration
                    if before_val != 0:
                        recovery_factor = abs(after_val - before_val) / abs(before_val)
                        recovery_factors.append(recovery_factor)
        
        avg_recovery = np.mean(recovery_factors) if recovery_factors else 0.5
        
        # Simulation du temps de r√©cup√©ration (en secondes)
        base_recovery_time = 60  # 1 minute de base
        return base_recovery_time * (1 + avg_recovery)
    
    def _analyze_chaos_results(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Analyse les r√©sultats de l'exp√©rience de chaos"""
        analysis = {
            'system_resilience_score': 0.0,
            'recovery_performance': 'good',
            'critical_issues': [],
            'recommendations': []
        }
        
        # Score de r√©silience bas√© sur le temps de r√©cup√©ration
        recovery_time = results.get('system_recovery_time', 300)
        if recovery_time < 60:
            analysis['system_resilience_score'] = 0.9
            analysis['recovery_performance'] = 'excellent'
        elif recovery_time < 180:
            analysis['system_resilience_score'] = 0.7
            analysis['recovery_performance'] = 'good'
        elif recovery_time < 300:
            analysis['system_resilience_score'] = 0.5
            analysis['recovery_performance'] = 'moderate'
        else:
            analysis['system_resilience_score'] = 0.3
            analysis['recovery_performance'] = 'poor'
        
        # Analyse des alertes
        alerts_count = len(results.get('alerts_triggered', []))
        if alerts_count > 10:
            analysis['critical_issues'].append('Excessive alert volume during chaos')
            analysis['recommendations'].append('Review alert thresholds and reduce noise')
        
        # Recommandations g√©n√©rales
        if recovery_time > 180:
            analysis['recommendations'].append('Improve automated recovery mechanisms')
            analysis['recommendations'].append('Implement circuit breakers and fallback strategies')
        
        return analysis

# Tests Hypothesis pour la g√©n√©ration de donn√©es
class HypothesisTestStrategies:
    """Strat√©gies de test avec Hypothesis"""
    
    @staticmethod
    @st.composite
    def alert_rule_strategy(draw):
        """Strat√©gie pour g√©n√©rer des r√®gles d'alerte"""
        return {
            'rule_id': draw(st.text(min_size=5, max_size=20, alphabet=st.characters(whitelist_categories=('Lu', 'Ll', 'Nd', 'Pc')))),
            'name': draw(st.text(min_size=10, max_size=50)),
            'severity': draw(st.sampled_from(['low', 'medium', 'high', 'critical'])),
            'conditions': draw(st.lists(
                st.fixed_dictionaries({
                    'type': st.sampled_from(['threshold', 'pattern', 'anomaly']),
                    'metric': st.sampled_from(['cpu_usage', 'memory_usage', 'error_rate']),
                    'operator': st.sampled_from(['>', '<', '>=', '<=']),
                    'value': st.floats(min_value=0.0, max_value=100.0)
                }),
                min_size=1,
                max_size=5
            ))
        }
    
    @staticmethod
    @st.composite
    def metrics_data_strategy(draw):
        """Strat√©gie pour g√©n√©rer des donn√©es de m√©triques"""
        return {
            'timestamp': draw(st.datetimes(min_value=datetime(2024, 1, 1), max_value=datetime(2024, 12, 31))),
            'cpu_usage': draw(st.floats(min_value=0.0, max_value=100.0)),
            'memory_usage': draw(st.floats(min_value=0.0, max_value=100.0)),
            'disk_usage': draw(st.floats(min_value=0.0, max_value=100.0)),
            'error_rate': draw(st.floats(min_value=0.0, max_value=0.5)),
            'response_time': draw(st.floats(min_value=50.0, max_value=10000.0))
        }

class TestSuite:
    """Suite de tests compl√®te"""
    
    def __init__(self, config: TestConfiguration):
        self.config = config
        self.validation_engine = RuleValidationEngine(config)
        self.load_testing = LoadTestingFramework(config)
        self.chaos_testing = ChaosTestingEngine(config)
        
    async def run_comprehensive_test_suite(self, target_system: str) -> Dict[str, Any]:
        """Ex√©cute une suite de tests compl√®te"""
        try:
            start_time = time.time()
            
            suite_results = {
                'test_suite_id': f"comprehensive_{int(time.time())}",
                'target_system': target_system,
                'start_time': datetime.utcnow().isoformat(),
                'tests_executed': 0,
                'tests_passed': 0,
                'tests_failed': 0,
                'validation_results': [],
                'load_test_results': {},
                'chaos_test_results': {},
                'overall_score': 0.0,
                'recommendations': []
            }
            
            # 1. Tests de validation des r√®gles
            logger.info("Starting rule validation tests")
            for i in range(10):  # Test de 10 r√®gles g√©n√©r√©es
                test_rule = AlertRuleFactory()
                test_data = [MetricsDataFactory() for _ in range(100)]
                
                validation_result = await self.validation_engine.validate_alert_rule(test_rule, test_data)
                suite_results['validation_results'].append(validation_result)
                suite_results['tests_executed'] += 1
                
                if validation_result.status in ['passed', 'warning']:
                    suite_results['tests_passed'] += 1
                else:
                    suite_results['tests_failed'] += 1
            
            # 2. Tests de charge (si activ√©s)
            if self.config.enable_load_testing:
                logger.info("Starting load tests")
                load_result = await self.load_testing.run_load_test(
                    f"http://{target_system}/api/health", 60
                )
                suite_results['load_test_results'] = load_result
                suite_results['tests_executed'] += 1
                
                if 'error' not in load_result:
                    suite_results['tests_passed'] += 1
                else:
                    suite_results['tests_failed'] += 1
            
            # 3. Tests de chaos (si activ√©s)
            if self.config.enable_chaos_testing:
                logger.info("Starting chaos tests")
                chaos_result = await self.chaos_testing.run_chaos_experiment(target_system, 180)
                suite_results['chaos_test_results'] = chaos_result
                suite_results['tests_executed'] += 1
                
                if 'error' not in chaos_result:
                    suite_results['tests_passed'] += 1
                else:
                    suite_results['tests_failed'] += 1
            
            # Calcul du score global
            suite_results['overall_score'] = suite_results['tests_passed'] / suite_results['tests_executed'] if suite_results['tests_executed'] > 0 else 0.0
            
            # G√©n√©ration de recommandations
            suite_results['recommendations'] = self._generate_suite_recommendations(suite_results)
            
            suite_results['execution_time'] = time.time() - start_time
            suite_results['end_time'] = datetime.utcnow().isoformat()
            
            logger.info(
                "Comprehensive test suite completed",
                test_suite_id=suite_results['test_suite_id'],
                tests_executed=suite_results['tests_executed'],
                tests_passed=suite_results['tests_passed'],
                overall_score=suite_results['overall_score'],
                execution_time=suite_results['execution_time']
            )
            
            return suite_results
            
        except Exception as e:
            logger.error("Test suite execution failed", error=str(e))
            return {'error': str(e)}
    
    def _generate_suite_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """G√©n√®re des recommandations bas√©es sur les r√©sultats de la suite"""
        recommendations = []
        
        overall_score = results.get('overall_score', 0.0)
        
        if overall_score < 0.7:
            recommendations.append("üö® Score global faible - R√©vision compl√®te du syst√®me recommand√©e")
        elif overall_score < 0.9:
            recommendations.append("‚ö†Ô∏è Score mod√©r√© - Optimisations cibl√©es recommand√©es")
        else:
            recommendations.append("‚úÖ Excellent score - Syst√®me en bonne sant√©")
        
        # Recommandations sp√©cifiques aux tests de validation
        validation_failures = sum(1 for r in results.get('validation_results', []) if r.status == 'failed')
        if validation_failures > 0:
            recommendations.append(f"üîß {validation_failures} r√®gles ont √©chou√© √† la validation - R√©viser la logique")
        
        # Recommandations pour les tests de charge
        load_results = results.get('load_test_results', {})
        if load_results.get('avg_response_time', 0) > 1000:
            recommendations.append("üêå Temps de r√©ponse √©lev√© d√©tect√© - Optimiser les performances")
        
        # Recommandations pour les tests de chaos
        chaos_results = results.get('chaos_test_results', {})
        if chaos_results.get('system_recovery_time', 0) > 300:
            recommendations.append("üîÑ Temps de r√©cup√©ration long - Am√©liorer la r√©silience")
        
        return recommendations

# Tests unitaires avec pytest
class TestAlertRulesValidation:
    """Tests unitaires pour la validation des r√®gles"""
    
    @pytest.fixture
    def validation_engine(self):
        config = TestConfiguration()
        return RuleValidationEngine(config)
    
    @pytest.fixture
    def sample_rule(self):
        return {
            'rule_id': 'test_rule_001',
            'name': 'High CPU Usage Alert',
            'severity': 'high',
            'conditions': [{
                'type': 'threshold',
                'metric': 'cpu_usage',
                'operator': '>',
                'value': 80.0
            }]
        }
    
    @pytest.fixture
    def sample_test_data(self):
        return [
            {'cpu_usage': 85.0, 'memory_usage': 60.0, 'error_rate': 0.01},
            {'cpu_usage': 75.0, 'memory_usage': 70.0, 'error_rate': 0.02},
            {'cpu_usage': 95.0, 'memory_usage': 85.0, 'error_rate': 0.005}
        ]
    
    def test_rule_structure_validation_valid(self, validation_engine, sample_rule):
        """Test de validation de structure pour une r√®gle valide"""
        result = validation_engine._validate_rule_structure(sample_rule)
        assert result['valid'] is True
        assert len(result['errors']) == 0
    
    def test_rule_structure_validation_missing_field(self, validation_engine):
        """Test de validation de structure pour une r√®gle avec champ manquant"""
        invalid_rule = {
            'name': 'Test Rule',
            'severity': 'high'
            # Missing rule_id and conditions
        }
        result = validation_engine._validate_rule_structure(invalid_rule)
        assert result['valid'] is False
        assert 'Missing required field: rule_id' in result['errors']
        assert 'Missing required field: conditions' in result['errors']
    
    def test_rule_condition_evaluation(self, validation_engine, sample_rule):
        """Test d'√©valuation des conditions de r√®gle"""
        test_data = {'cpu_usage': 85.0, 'memory_usage': 60.0}
        result = validation_engine._evaluate_rule_conditions(sample_rule, test_data)
        assert result is True  # CPU > 80
        
        test_data_low = {'cpu_usage': 75.0, 'memory_usage': 60.0}
        result_low = validation_engine._evaluate_rule_conditions(sample_rule, test_data_low)
        assert result_low is False  # CPU < 80
    
    @pytest.mark.asyncio
    async def test_full_rule_validation(self, validation_engine, sample_rule, sample_test_data):
        """Test de validation compl√®te d'une r√®gle"""
        result = await validation_engine.validate_alert_rule(sample_rule, sample_test_data)
        
        assert result.test_type == 'rule_validation'
        assert result.status in ['passed', 'warning', 'failed']
        assert 'accuracy' in result.metrics
        assert result.execution_time > 0
    
    @given(HypothesisTestStrategies.alert_rule_strategy())
    @settings(max_examples=50, verbosity=Verbosity.verbose)
    def test_rule_validation_with_hypothesis(self, validation_engine, generated_rule):
        """Test de validation avec des r√®gles g√©n√©r√©es par Hypothesis"""
        # Ce test s'assure que la validation ne crash pas avec des r√®gles al√©atoires
        try:
            result = validation_engine._validate_rule_structure(generated_rule)
            assert isinstance(result, dict)
            assert 'valid' in result
            assert 'errors' in result
        except Exception as e:
            pytest.fail(f"Validation crashed with generated rule: {e}")

# Exemple d'utilisation compl√®te
async def demonstrate_testing_framework():
    """D√©montre le framework de tests complet"""
    
    # Configuration des tests
    config = TestConfiguration(
        enable_load_testing=True,
        enable_chaos_testing=True,
        enable_ml_validation=True,
        max_concurrent_tests=20,
        performance_threshold_ms=500
    )
    
    # Initialisation de la suite de tests
    test_suite = TestSuite(config)
    
    print("üß™ D√©marrage de la suite de tests compl√®te...")
    
    # Ex√©cution de la suite compl√®te
    results = await test_suite.run_comprehensive_test_suite("localhost:8000")
    
    print(f"\nüìä R√©sultats de la suite de tests:")
    print(f"   Tests ex√©cut√©s: {results.get('tests_executed', 0)}")
    print(f"   Tests r√©ussis: {results.get('tests_passed', 0)}")
    print(f"   Tests √©chou√©s: {results.get('tests_failed', 0)}")
    print(f"   Score global: {results.get('overall_score', 0.0):.2f}")
    
    print(f"\nüí° Recommandations:")
    for rec in results.get('recommendations', []):
        print(f"   - {rec}")
    
    print(f"\n‚è±Ô∏è Temps d'ex√©cution: {results.get('execution_time', 0):.2f} secondes")
    
    print("\n‚úÖ D√©monstration du framework de tests termin√©e!")

if __name__ == "__main__":
    # Ex√©cution des tests
    asyncio.run(demonstrate_testing_framework())
    
    # Ex√©cution des tests pytest
    # pytest test_validation_framework.py -v --tb=short
