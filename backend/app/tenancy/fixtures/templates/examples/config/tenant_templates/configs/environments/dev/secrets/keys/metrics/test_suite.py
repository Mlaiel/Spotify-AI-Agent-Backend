#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Comprehensive Test Suite & Validation Framework
==============================================

Ultra-advanced testing framework with comprehensive unit tests, integration tests,
performance benchmarks, security validation, and automated quality assurance.

Expert Development Team:
- Lead Dev + AI Architect
- Senior Backend Developer (Python/FastAPI/Django)
- ML Engineer (TensorFlow/PyTorch/Hugging Face)
- DBA & Data Engineer (PostgreSQL/Redis/MongoDB)
- Backend Security Specialist
- Microservices Architect

Project Lead: Fahed Mlaiel
"""

import asyncio
import json
import logging
import os
import sys
import time
import tempfile
import shutil
import statistics
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple
import unittest
import pytest
import concurrent.futures
import threading
import multiprocessing
import psutil
import hashlib
import random
import string

# Ajout du chemin parent pour les imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from . import (
    EnterpriseMetricsSystem, MetricDataPoint, MetricType, 
    MetricCategory, MetricSeverity, get_metrics_system
)
from .collector import MetricsCollectionAgent, CollectorConfig
from .monitor import AlertEngine, AlertRule, AlertPriority, HealthMonitor, MonitoringTarget
from .deploy import DeploymentOrchestrator, DeploymentConfig

# Configuration du logging pour les tests
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


class MetricsSystemTestSuite:
    """Suite de tests compl√®te pour le syst√®me de m√©triques."""
    
    def __init__(self):
        self.test_results = {}
        self.performance_results = {}
        self.temp_dir = None
        self.test_storage = None
        
    async def run_all_tests(self) -> Dict[str, Any]:
        """Ex√©cute tous les tests."""
        logging.info("üöÄ D√©marrage de la suite de tests compl√®te")
        
        # Pr√©paration de l'environnement de test
        await self._setup_test_environment()
        
        try:
            # Tests unitaires
            unit_results = await self._run_unit_tests()
            
            # Tests d'int√©gration
            integration_results = await self._run_integration_tests()
            
            # Tests de performance
            performance_results = await self._run_performance_tests()
            
            # Tests de s√©curit√©
            security_results = await self._run_security_tests()
            
            # Tests de robustesse
            stress_results = await self._run_stress_tests()
            
            # Validation de l'API
            api_results = await self._run_api_tests()
            
            # Compilation des r√©sultats
            results = {
                "summary": {
                    "total_tests": sum(len(r) for r in [
                        unit_results, integration_results, performance_results,
                        security_results, stress_results, api_results
                    ]),
                    "passed": sum(
                        sum(1 for t in r.values() if t.get("passed", False)) 
                        for r in [unit_results, integration_results, performance_results,
                                security_results, stress_results, api_results]
                    ),
                    "execution_time": time.time() - self.start_time
                },
                "unit_tests": unit_results,
                "integration_tests": integration_results,
                "performance_tests": performance_results,
                "security_tests": security_results,
                "stress_tests": stress_results,
                "api_tests": api_results
            }
            
            # Calcul du score global
            total_tests = results["summary"]["total_tests"]
            passed_tests = results["summary"]["passed"]
            results["summary"]["success_rate"] = (passed_tests / total_tests * 100) if total_tests > 0 else 0
            
            logging.info(f"‚úÖ Tests termin√©s: {passed_tests}/{total_tests} r√©ussis ({results['summary']['success_rate']:.1f}%)")
            
            return results
            
        finally:
            # Nettoyage
            await self._cleanup_test_environment()
    
    async def _setup_test_environment(self):
        """Configure l'environnement de test."""
        self.start_time = time.time()
        
        # Cr√©ation du r√©pertoire temporaire
        self.temp_dir = tempfile.mkdtemp(prefix="metrics_test_")
        logging.info(f"üìÅ R√©pertoire de test: {self.temp_dir}")
        
        # Configuration du stockage de test
        self.test_storage = get_metrics_system("sqlite", {"db_path": f"{self.temp_dir}/test.db"})
        await self.test_storage.start()
        
    async def _cleanup_test_environment(self):
        """Nettoie l'environnement de test."""
        if self.test_storage:
            await self.test_storage.stop()
        
        if self.temp_dir and os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)
            logging.info("üßπ Environnement de test nettoy√©")
    
    async def _run_unit_tests(self) -> Dict[str, Dict[str, Any]]:
        """Ex√©cute les tests unitaires."""
        logging.info("üî¨ Ex√©cution des tests unitaires")
        
        tests = {
            "test_metric_creation": self._test_metric_creation,
            "test_metric_storage": self._test_metric_storage,
            "test_metric_query": self._test_metric_query,
            "test_alert_rules": self._test_alert_rules,
            "test_anomaly_detection": self._test_anomaly_detection,
            "test_aggregations": self._test_aggregations,
            "test_data_validation": self._test_data_validation,
            "test_configuration": self._test_configuration
        }
        
        results = {}
        for test_name, test_func in tests.items():
            try:
                start_time = time.time()
                await test_func()
                execution_time = time.time() - start_time
                
                results[test_name] = {
                    "passed": True,
                    "execution_time": execution_time,
                    "message": "Test r√©ussi"
                }
                logging.info(f"‚úÖ {test_name}: OK ({execution_time:.3f}s)")
                
            except Exception as e:
                results[test_name] = {
                    "passed": False,
                    "execution_time": time.time() - start_time,
                    "error": str(e),
                    "message": f"Test √©chou√©: {e}"
                }
                logging.error(f"‚ùå {test_name}: √âCHEC - {e}")
        
        return results
    
    async def _test_metric_creation(self):
        """Test de cr√©ation de m√©triques."""
        # Test cr√©ation m√©trique basique
        metric = MetricDataPoint(
            metric_id="test.basic.metric",
            value=42.0,
            metric_type=MetricType.GAUGE,
            category=MetricCategory.SYSTEM
        )
        
        assert metric.metric_id == "test.basic.metric"
        assert metric.value == 42.0
        assert metric.metric_type == MetricType.GAUGE
        
        # Test avec tags et m√©tadonn√©es
        metric_with_tags = MetricDataPoint(
            metric_id="test.tagged.metric",
            value=100.0,
            metric_type=MetricType.COUNTER,
            tags={"env": "test", "service": "metrics"},
            metadata={"description": "Test metric with tags"}
        )
        
        assert len(metric_with_tags.tags) == 2
        assert metric_with_tags.tags["env"] == "test"
        assert "description" in metric_with_tags.metadata
        
        # Test validation des valeurs
        try:
            MetricDataPoint(
                metric_id="",  # ID vide devrait √©chouer
                value=42.0,
                metric_type=MetricType.GAUGE
            )
            raise AssertionError("Devrait √©chouer avec ID vide")
        except ValueError:
            pass  # Comportement attendu
    
    async def _test_metric_storage(self):
        """Test de stockage de m√©triques."""
        # Stockage d'une m√©trique simple
        metric = MetricDataPoint(
            metric_id="test.storage.metric",
            value=123.45,
            metric_type=MetricType.GAUGE,
            category=MetricCategory.PERFORMANCE
        )
        
        await self.test_storage.storage.store_metric(metric)
        
        # V√©rification que la m√©trique est stock√©e
        stored_metrics = await self.test_storage.storage.query_metrics(
            start_time=datetime.now() - timedelta(minutes=1),
            end_time=datetime.now() + timedelta(minutes=1)
        )
        
        assert len(stored_metrics) >= 1
        assert any(m.metric_id == "test.storage.metric" for m in stored_metrics)
        
        # Test de stockage en lot
        batch_metrics = []
        for i in range(10):
            batch_metrics.append(MetricDataPoint(
                metric_id=f"test.batch.metric_{i}",
                value=float(i),
                metric_type=MetricType.COUNTER
            ))
        
        for metric in batch_metrics:
            await self.test_storage.storage.store_metric(metric)
        
        # V√©rification du lot
        batch_stored = await self.test_storage.storage.query_metrics(
            metric_pattern="test.batch.*",
            start_time=datetime.now() - timedelta(minutes=1),
            end_time=datetime.now() + timedelta(minutes=1)
        )
        
        assert len(batch_stored) >= 10
    
    async def _test_metric_query(self):
        """Test de requ√™tes de m√©triques."""
        # Cr√©ation de m√©triques de test
        test_metrics = []
        base_time = datetime.now() - timedelta(hours=1)
        
        for i in range(20):
            test_metrics.append(MetricDataPoint(
                metric_id="test.query.cpu_usage",
                timestamp=base_time + timedelta(minutes=i*3),
                value=50.0 + (i % 10),
                metric_type=MetricType.GAUGE,
                tags={"host": f"server_{i%3}"}
            ))
        
        # Stockage des m√©triques de test
        for metric in test_metrics:
            await self.test_storage.storage.store_metric(metric)
        
        # Test requ√™te par pattern
        pattern_results = await self.test_storage.storage.query_metrics(
            metric_pattern="test.query.*",
            start_time=base_time - timedelta(minutes=30),
            end_time=datetime.now()
        )
        
        assert len(pattern_results) >= 20
        
        # Test requ√™te par plage de temps
        time_range_results = await self.test_storage.storage.query_metrics(
            start_time=base_time + timedelta(minutes=30),
            end_time=base_time + timedelta(minutes=45)
        )
        
        assert len(time_range_results) >= 5
        
        # Test agr√©gation (si support√©e)
        try:
            agg_results = await self.test_storage.storage.query_aggregated(
                metric_pattern="test.query.*",
                aggregation="avg",
                interval="10m",
                start_time=base_time,
                end_time=datetime.now()
            )
            # V√©rification que l'agr√©gation fonctionne
            assert len(agg_results) >= 1
        except AttributeError:
            pass  # Agr√©gation peut ne pas √™tre impl√©ment√©e
    
    async def _test_alert_rules(self):
        """Test des r√®gles d'alerte."""
        # Cr√©ation d'un moteur d'alertes de test
        alert_engine = AlertEngine(self.test_storage)
        
        # Test cr√©ation de r√®gle
        rule = AlertRule(
            rule_id="test_rule",
            name="Test Alert Rule",
            description="Rule for testing",
            metric_pattern="test.alert.*",
            threshold_value=80.0,
            comparison=">",
            priority=AlertPriority.HIGH
        )
        
        await alert_engine.add_rule(rule)
        assert "test_rule" in alert_engine.alert_rules
        
        # Test √©valuation de r√®gle
        test_metric = MetricDataPoint(
            metric_id="test.alert.cpu",
            value=85.0,  # Au-dessus du seuil
            metric_type=MetricType.GAUGE
        )
        
        await self.test_storage.storage.store_metric(test_metric)
        
        # Simulation d'√©valuation
        should_trigger = alert_engine._check_threshold_condition(85.0, 80.0, ">")
        assert should_trigger == True
        
        should_not_trigger = alert_engine._check_threshold_condition(75.0, 80.0, ">")
        assert should_not_trigger == False
    
    async def _test_anomaly_detection(self):
        """Test de d√©tection d'anomalies."""
        # Cr√©ation de donn√©es avec anomalie
        normal_values = [50.0 + random.gauss(0, 5) for _ in range(50)]
        anomaly_values = [150.0, 200.0]  # Valeurs anormalement √©lev√©es
        
        all_values = normal_values + anomaly_values
        
        # Test d√©tection simple par Z-score
        mean_val = statistics.mean(normal_values)
        std_val = statistics.stdev(normal_values)
        
        for anomaly_val in anomaly_values:
            z_score = abs(anomaly_val - mean_val) / std_val
            assert z_score > 3.0  # Seuil d'anomalie standard
        
        # Test que les valeurs normales ne sont pas d√©tect√©es comme anomalies
        for normal_val in normal_values[-10:]:  # Test sur les derni√®res valeurs
            z_score = abs(normal_val - mean_val) / std_val
            assert z_score <= 3.0
    
    async def _test_aggregations(self):
        """Test des fonctions d'agr√©gation."""
        # Donn√©es de test
        values = [10.0, 20.0, 30.0, 40.0, 50.0]
        
        # Test calculs d'agr√©gation
        assert statistics.mean(values) == 30.0
        assert min(values) == 10.0
        assert max(values) == 50.0
        assert sum(values) == 150.0
        
        # Test avec donn√©es vides
        try:
            statistics.mean([])
            raise AssertionError("Devrait √©chouer avec liste vide")
        except statistics.StatisticsError:
            pass  # Comportement attendu
    
    async def _test_data_validation(self):
        """Test de validation des donn√©es."""
        # Test validation des IDs de m√©triques
        valid_ids = ["system.cpu.usage", "crypto.key.access_count", "app.api.response_time"]
        invalid_ids = ["", "  ", "invalid..metric", "metric with spaces"]
        
        for valid_id in valid_ids:
            # Ces IDs devraient √™tre valides
            metric = MetricDataPoint(
                metric_id=valid_id,
                value=42.0,
                metric_type=MetricType.GAUGE
            )
            assert metric.metric_id == valid_id
        
        # Test validation des valeurs
        valid_values = [0.0, 42.0, -10.5, 1e6, 1e-6]
        for valid_value in valid_values:
            metric = MetricDataPoint(
                metric_id="test.value",
                value=valid_value,
                metric_type=MetricType.GAUGE
            )
            assert metric.value == valid_value
        
        # Test validation des timestamps
        valid_timestamp = datetime.now()
        metric = MetricDataPoint(
            metric_id="test.timestamp",
            timestamp=valid_timestamp,
            value=42.0,
            metric_type=MetricType.GAUGE
        )
        assert metric.timestamp == valid_timestamp
    
    async def _test_configuration(self):
        """Test de configuration du syst√®me."""
        # Test configuration du collecteur
        config = CollectorConfig(
            system_interval=30,
            security_interval=300,
            adaptive_sampling=True
        )
        
        assert config.system_interval == 30
        assert config.security_interval == 300
        assert config.adaptive_sampling == True
        
        # Test configuration de d√©ploiement
        deploy_config = DeploymentConfig(
            deployment_name="test-deployment",
            mode="development"
        )
        
        assert deploy_config.deployment_name == "test-deployment"
        assert hasattr(deploy_config, 'mode')
    
    async def _run_integration_tests(self) -> Dict[str, Dict[str, Any]]:
        """Ex√©cute les tests d'int√©gration."""
        logging.info("üîó Ex√©cution des tests d'int√©gration")
        
        tests = {
            "test_end_to_end_flow": self._test_end_to_end_flow,
            "test_collector_integration": self._test_collector_integration,
            "test_alert_integration": self._test_alert_integration,
            "test_storage_backends": self._test_storage_backends,
            "test_monitoring_integration": self._test_monitoring_integration
        }
        
        results = {}
        for test_name, test_func in tests.items():
            try:
                start_time = time.time()
                await test_func()
                execution_time = time.time() - start_time
                
                results[test_name] = {
                    "passed": True,
                    "execution_time": execution_time,
                    "message": "Test d'int√©gration r√©ussi"
                }
                logging.info(f"‚úÖ {test_name}: OK ({execution_time:.3f}s)")
                
            except Exception as e:
                results[test_name] = {
                    "passed": False,
                    "execution_time": time.time() - start_time,
                    "error": str(e),
                    "message": f"Test d'int√©gration √©chou√©: {e}"
                }
                logging.error(f"‚ùå {test_name}: √âCHEC - {e}")
        
        return results
    
    async def _test_end_to_end_flow(self):
        """Test du flux de bout en bout."""
        # 1. Cr√©ation d'une m√©trique
        metric = MetricDataPoint(
            metric_id="test.e2e.cpu_usage",
            value=85.0,
            metric_type=MetricType.GAUGE,
            category=MetricCategory.SYSTEM,
            tags={"host": "test-server"}
        )
        
        # 2. Stockage
        await self.test_storage.storage.store_metric(metric)
        
        # 3. Requ√™te
        results = await self.test_storage.storage.query_metrics(
            metric_pattern="test.e2e.*",
            start_time=datetime.now() - timedelta(minutes=1),
            end_time=datetime.now() + timedelta(minutes=1)
        )
        
        # 4. V√©rification
        assert len(results) >= 1
        found_metric = next((m for m in results if m.metric_id == "test.e2e.cpu_usage"), None)
        assert found_metric is not None
        assert found_metric.value == 85.0
        assert found_metric.tags.get("host") == "test-server"
    
    async def _test_collector_integration(self):
        """Test d'int√©gration du collecteur."""
        # Configuration du collecteur
        config = CollectorConfig(
            system_interval=1,  # Intervalle court pour le test
            max_concurrent_collectors=2
        )
        
        # Cr√©ation du collecteur
        collector = MetricsCollectionAgent(config, self.test_storage)
        
        # Test de d√©marrage et arr√™t
        await collector.start()
        await asyncio.sleep(2)  # Laisser le collecteur fonctionner
        await collector.stop()
        
        # V√©rification que des m√©triques ont √©t√© collect√©es
        assert collector.metrics_collected > 0
    
    async def _test_alert_integration(self):
        """Test d'int√©gration des alertes."""
        # Cr√©ation du moteur d'alertes
        alert_engine = AlertEngine(self.test_storage)
        await alert_engine.start()
        
        # Ajout d'une r√®gle de test
        rule = AlertRule(
            rule_id="integration_test_rule",
            name="Integration Test Rule",
            description="Test rule for integration",
            metric_pattern="test.integration.*",
            threshold_value=90.0,
            comparison=">",
            duration_seconds=1,
            priority=AlertPriority.HIGH
        )
        
        await alert_engine.add_rule(rule)
        
        # Injection de m√©triques qui devraient d√©clencher l'alerte
        trigger_metric = MetricDataPoint(
            metric_id="test.integration.cpu",
            value=95.0,  # Au-dessus du seuil
            metric_type=MetricType.GAUGE
        )
        
        await self.test_storage.storage.store_metric(trigger_metric)
        
        # Attente de l'√©valuation
        await asyncio.sleep(2)
        
        # V√©rification (l'alerte peut ne pas se d√©clencher dans l'environnement de test)
        status = alert_engine.get_engine_status()
        assert status["running"] == True
        assert status["rules_count"] >= 1
        
        await alert_engine.stop()
    
    async def _test_storage_backends(self):
        """Test des diff√©rents backends de stockage."""
        # Test SQLite (d√©j√† test√© dans la configuration principale)
        sqlite_storage = get_metrics_system("sqlite", {"db_path": f"{self.temp_dir}/test_sqlite.db"})
        await sqlite_storage.start()
        
        test_metric = MetricDataPoint(
            metric_id="test.storage.sqlite",
            value=42.0,
            metric_type=MetricType.GAUGE
        )
        
        await sqlite_storage.storage.store_metric(test_metric)
        
        results = await sqlite_storage.storage.query_metrics(
            metric_pattern="test.storage.*",
            start_time=datetime.now() - timedelta(minutes=1),
            end_time=datetime.now() + timedelta(minutes=1)
        )
        
        assert len(results) >= 1
        await sqlite_storage.stop()
        
        # Test Redis (si disponible)
        try:
            redis_storage = get_metrics_system("redis", {"redis_url": "redis://localhost:6379/15"})
            await redis_storage.start()
            
            await redis_storage.storage.store_metric(test_metric)
            await redis_storage.stop()
            
        except Exception as e:
            logging.warning(f"Redis non disponible pour les tests: {e}")
    
    async def _test_monitoring_integration(self):
        """Test d'int√©gration du monitoring."""
        # Cr√©ation du moteur d'alertes
        alert_engine = AlertEngine(self.test_storage)
        
        # Cr√©ation du moniteur de sant√©
        health_monitor = HealthMonitor(alert_engine)
        
        # Ajout d'une cible de test
        target = MonitoringTarget(
            target_id="test_target",
            name="Test Target",
            target_type="test",
            endpoint="127.0.0.1",
            port=80,
            check_interval=1
        )
        
        await health_monitor.add_target(target)
        
        # V√©rification que la cible est ajout√©e
        assert "test_target" in health_monitor.targets
        assert health_monitor.targets["test_target"].name == "Test Target"
    
    async def _run_performance_tests(self) -> Dict[str, Dict[str, Any]]:
        """Ex√©cute les tests de performance."""
        logging.info("‚ö° Ex√©cution des tests de performance")
        
        tests = {
            "test_metric_ingestion_rate": self._test_metric_ingestion_rate,
            "test_query_performance": self._test_query_performance,
            "test_concurrent_operations": self._test_concurrent_operations,
            "test_memory_usage": self._test_memory_usage,
            "test_large_dataset": self._test_large_dataset
        }
        
        results = {}
        for test_name, test_func in tests.items():
            try:
                start_time = time.time()
                performance_data = await test_func()
                execution_time = time.time() - start_time
                
                results[test_name] = {
                    "passed": True,
                    "execution_time": execution_time,
                    "performance_data": performance_data,
                    "message": "Test de performance r√©ussi"
                }
                logging.info(f"‚úÖ {test_name}: OK ({execution_time:.3f}s)")
                
            except Exception as e:
                results[test_name] = {
                    "passed": False,
                    "execution_time": time.time() - start_time,
                    "error": str(e),
                    "message": f"Test de performance √©chou√©: {e}"
                }
                logging.error(f"‚ùå {test_name}: √âCHEC - {e}")
        
        return results
    
    async def _test_metric_ingestion_rate(self) -> Dict[str, float]:
        """Test du taux d'ingestion de m√©triques."""
        num_metrics = 1000
        metrics = []
        
        # G√©n√©ration des m√©triques
        for i in range(num_metrics):
            metrics.append(MetricDataPoint(
                metric_id=f"perf.ingestion.metric_{i}",
                value=float(i),
                metric_type=MetricType.COUNTER
            ))
        
        # Test d'ingestion
        start_time = time.time()
        
        for metric in metrics:
            await self.test_storage.storage.store_metric(metric)
        
        ingestion_time = time.time() - start_time
        rate = num_metrics / ingestion_time
        
        logging.info(f"üìä Taux d'ingestion: {rate:.1f} m√©triques/seconde")
        
        return {
            "metrics_count": num_metrics,
            "ingestion_time": ingestion_time,
            "rate_per_second": rate
        }
    
    async def _test_query_performance(self) -> Dict[str, float]:
        """Test de performance des requ√™tes."""
        # Pr√©paration des donn√©es
        num_metrics = 500
        for i in range(num_metrics):
            metric = MetricDataPoint(
                metric_id=f"perf.query.metric_{i%10}",
                timestamp=datetime.now() - timedelta(minutes=i),
                value=float(i),
                metric_type=MetricType.GAUGE
            )
            await self.test_storage.storage.store_metric(metric)
        
        # Test de requ√™te simple
        start_time = time.time()
        results = await self.test_storage.storage.query_metrics(
            metric_pattern="perf.query.*",
            start_time=datetime.now() - timedelta(hours=1),
            end_time=datetime.now()
        )
        query_time = time.time() - start_time
        
        logging.info(f"üìä Temps de requ√™te: {query_time:.3f}s pour {len(results)} r√©sultats")
        
        return {
            "query_time": query_time,
            "results_count": len(results),
            "results_per_second": len(results) / query_time if query_time > 0 else 0
        }
    
    async def _test_concurrent_operations(self) -> Dict[str, float]:
        """Test des op√©rations concurrentes."""
        num_concurrent = 10
        metrics_per_task = 50
        
        async def write_metrics(task_id: int):
            for i in range(metrics_per_task):
                metric = MetricDataPoint(
                    metric_id=f"perf.concurrent.task_{task_id}.metric_{i}",
                    value=float(i),
                    metric_type=MetricType.COUNTER
                )
                await self.test_storage.storage.store_metric(metric)
        
        # Ex√©cution concurrente
        start_time = time.time()
        
        tasks = [write_metrics(i) for i in range(num_concurrent)]
        await asyncio.gather(*tasks)
        
        concurrent_time = time.time() - start_time
        total_metrics = num_concurrent * metrics_per_task
        rate = total_metrics / concurrent_time
        
        logging.info(f"üìä Op√©rations concurrentes: {rate:.1f} m√©triques/seconde ({num_concurrent} t√¢ches)")
        
        return {
            "concurrent_tasks": num_concurrent,
            "metrics_per_task": metrics_per_task,
            "total_time": concurrent_time,
            "total_rate": rate
        }
    
    async def _test_memory_usage(self) -> Dict[str, float]:
        """Test d'utilisation m√©moire."""
        if not hasattr(psutil, 'Process'):
            return {"memory_usage": 0, "memory_increase": 0}
        
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # Cr√©ation de nombreuses m√©triques en m√©moire
        metrics = []
        for i in range(1000):
            metrics.append(MetricDataPoint(
                metric_id=f"perf.memory.metric_{i}",
                value=float(i),
                metric_type=MetricType.GAUGE,
                tags={f"tag_{j}": f"value_{j}" for j in range(5)},
                metadata={f"meta_{j}": f"data_{j}" for j in range(3)}
            ))
        
        # Stockage des m√©triques
        for metric in metrics:
            await self.test_storage.storage.store_metric(metric)
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        logging.info(f"üìä Utilisation m√©moire: {final_memory:.1f}MB (+{memory_increase:.1f}MB)")
        
        return {
            "initial_memory_mb": initial_memory,
            "final_memory_mb": final_memory,
            "memory_increase_mb": memory_increase
        }
    
    async def _test_large_dataset(self) -> Dict[str, float]:
        """Test avec un large dataset."""
        num_metrics = 5000
        batch_size = 100
        
        start_time = time.time()
        
        # Cr√©ation par lots pour √©viter les probl√®mes de m√©moire
        for batch_start in range(0, num_metrics, batch_size):
            batch_metrics = []
            for i in range(batch_start, min(batch_start + batch_size, num_metrics)):
                batch_metrics.append(MetricDataPoint(
                    metric_id=f"perf.large.metric_{i%100}",
                    timestamp=datetime.now() - timedelta(seconds=i),
                    value=float(i),
                    metric_type=MetricType.GAUGE
                ))
            
            # Stockage du lot
            for metric in batch_metrics:
                await self.test_storage.storage.store_metric(metric)
        
        storage_time = time.time() - start_time
        
        # Test de requ√™te sur le large dataset
        query_start = time.time()
        results = await self.test_storage.storage.query_metrics(
            metric_pattern="perf.large.*",
            start_time=datetime.now() - timedelta(hours=2),
            end_time=datetime.now()
        )
        query_time = time.time() - query_start
        
        logging.info(f"üìä Large dataset: {len(results)} m√©triques stock√©es en {storage_time:.1f}s, requ√™te en {query_time:.3f}s")
        
        return {
            "dataset_size": num_metrics,
            "storage_time": storage_time,
            "query_time": query_time,
            "results_count": len(results)
        }
    
    async def _run_security_tests(self) -> Dict[str, Dict[str, Any]]:
        """Ex√©cute les tests de s√©curit√©."""
        logging.info("üîí Ex√©cution des tests de s√©curit√©")
        
        tests = {
            "test_input_validation": self._test_input_validation,
            "test_sql_injection": self._test_sql_injection,
            "test_data_sanitization": self._test_data_sanitization,
            "test_access_control": self._test_access_control
        }
        
        results = {}
        for test_name, test_func in tests.items():
            try:
                start_time = time.time()
                await test_func()
                execution_time = time.time() - start_time
                
                results[test_name] = {
                    "passed": True,
                    "execution_time": execution_time,
                    "message": "Test de s√©curit√© r√©ussi"
                }
                logging.info(f"‚úÖ {test_name}: OK ({execution_time:.3f}s)")
                
            except Exception as e:
                results[test_name] = {
                    "passed": False,
                    "execution_time": time.time() - start_time,
                    "error": str(e),
                    "message": f"Test de s√©curit√© √©chou√©: {e}"
                }
                logging.error(f"‚ùå {test_name}: √âCHEC - {e}")
        
        return results
    
    async def _test_input_validation(self):
        """Test de validation des entr√©es."""
        # Test avec des IDs malveillants
        malicious_ids = [
            "../../../etc/passwd",
            "<script>alert('xss')</script>",
            "'; DROP TABLE metrics; --",
            "\\x00\\x01\\x02",
            "A" * 10000  # ID tr√®s long
        ]
        
        for malicious_id in malicious_ids:
            try:
                metric = MetricDataPoint(
                    metric_id=malicious_id,
                    value=42.0,
                    metric_type=MetricType.GAUGE
                )
                # Si aucune exception n'est lev√©e, v√©rifier que l'ID est sanitis√©
                assert len(metric.metric_id) < 1000  # Limite raisonnable
                
            except (ValueError, TypeError):
                pass  # Comportement attendu pour les entr√©es invalides
    
    async def _test_sql_injection(self):
        """Test de protection contre l'injection SQL."""
        # Test avec des patterns d'injection SQL
        sql_injection_patterns = [
            "test'; DROP TABLE metrics; --",
            "test' OR '1'='1",
            "test' UNION SELECT * FROM users --",
            "test'; INSERT INTO metrics VALUES (1,2,3); --"
        ]
        
        for pattern in sql_injection_patterns:
            # Tentative de stockage avec pattern malveillant
            metric = MetricDataPoint(
                metric_id="test.security.injection",
                value=42.0,
                metric_type=MetricType.GAUGE,
                tags={"malicious": pattern}
            )
            
            await self.test_storage.storage.store_metric(metric)
            
            # V√©rification que le syst√®me fonctionne toujours
            results = await self.test_storage.storage.query_metrics(
                metric_pattern="test.security.*",
                start_time=datetime.now() - timedelta(minutes=1),
                end_time=datetime.now() + timedelta(minutes=1)
            )
            
            # Le syst√®me devrait toujours fonctionner normalement
            assert isinstance(results, list)
    
    async def _test_data_sanitization(self):
        """Test de sanitisation des donn√©es."""
        # Test avec des caract√®res sp√©ciaux
        special_chars = "!@#$%^&*()[]{}|\\:;\"'<>,.?/~`"
        
        metric = MetricDataPoint(
            metric_id="test.sanitization.special_chars",
            value=42.0,
            metric_type=MetricType.GAUGE,
            tags={"special": special_chars},
            metadata={"description": special_chars}
        )
        
        await self.test_storage.storage.store_metric(metric)
        
        # V√©rification que les donn√©es sont stock√©es correctement
        results = await self.test_storage.storage.query_metrics(
            metric_pattern="test.sanitization.*",
            start_time=datetime.now() - timedelta(minutes=1),
            end_time=datetime.now() + timedelta(minutes=1)
        )
        
        assert len(results) >= 1
    
    async def _test_access_control(self):
        """Test du contr√¥le d'acc√®s."""
        # Ce test d√©pend de l'impl√©mentation du contr√¥le d'acc√®s
        # Pour l'instant, on v√©rifie que le syst√®me fonctionne normalement
        
        metric = MetricDataPoint(
            metric_id="test.access.control",
            value=42.0,
            metric_type=MetricType.GAUGE
        )
        
        await self.test_storage.storage.store_metric(metric)
        
        results = await self.test_storage.storage.query_metrics(
            metric_pattern="test.access.*",
            start_time=datetime.now() - timedelta(minutes=1),
            end_time=datetime.now() + timedelta(minutes=1)
        )
        
        assert len(results) >= 1
    
    async def _run_stress_tests(self) -> Dict[str, Dict[str, Any]]:
        """Ex√©cute les tests de stress."""
        logging.info("üí™ Ex√©cution des tests de stress")
        
        tests = {
            "test_high_load": self._test_high_load,
            "test_memory_pressure": self._test_memory_pressure,
            "test_connection_limits": self._test_connection_limits,
            "test_error_recovery": self._test_error_recovery
        }
        
        results = {}
        for test_name, test_func in tests.items():
            try:
                start_time = time.time()
                stress_data = await test_func()
                execution_time = time.time() - start_time
                
                results[test_name] = {
                    "passed": True,
                    "execution_time": execution_time,
                    "stress_data": stress_data,
                    "message": "Test de stress r√©ussi"
                }
                logging.info(f"‚úÖ {test_name}: OK ({execution_time:.3f}s)")
                
            except Exception as e:
                results[test_name] = {
                    "passed": False,
                    "execution_time": time.time() - start_time,
                    "error": str(e),
                    "message": f"Test de stress √©chou√©: {e}"
                }
                logging.error(f"‚ùå {test_name}: √âCHEC - {e}")
        
        return results
    
    async def _test_high_load(self) -> Dict[str, float]:
        """Test de charge √©lev√©e."""
        num_tasks = 20
        metrics_per_task = 100
        
        async def high_load_task(task_id: int):
            for i in range(metrics_per_task):
                metric = MetricDataPoint(
                    metric_id=f"stress.high_load.task_{task_id}.metric_{i}",
                    value=float(i),
                    metric_type=MetricType.COUNTER
                )
                await self.test_storage.storage.store_metric(metric)
                
                # Petite pause pour √©viter de surcharger
                if i % 10 == 0:
                    await asyncio.sleep(0.001)
        
        start_time = time.time()
        
        # Lancement de toutes les t√¢ches en parall√®le
        tasks = [high_load_task(i) for i in range(num_tasks)]
        await asyncio.gather(*tasks)
        
        load_time = time.time() - start_time
        total_metrics = num_tasks * metrics_per_task
        rate = total_metrics / load_time
        
        logging.info(f"üìä Test de charge: {total_metrics} m√©triques en {load_time:.1f}s ({rate:.1f}/sec)")
        
        return {
            "total_metrics": total_metrics,
            "load_time": load_time,
            "rate_per_second": rate,
            "concurrent_tasks": num_tasks
        }
    
    async def _test_memory_pressure(self) -> Dict[str, float]:
        """Test de pression m√©moire."""
        # Cr√©ation d'un grand nombre d'objets en m√©moire
        large_objects = []
        
        for i in range(1000):
            # Cr√©ation de m√©triques avec beaucoup de m√©tadonn√©es
            metric = MetricDataPoint(
                metric_id=f"stress.memory.metric_{i}",
                value=float(i),
                metric_type=MetricType.GAUGE,
                tags={f"tag_{j}": f"value_{j}" * 100 for j in range(10)},
                metadata={f"meta_{j}": f"data_{j}" * 50 for j in range(5)}
            )
            large_objects.append(metric)
        
        # Stockage de tous les objets
        for metric in large_objects:
            await self.test_storage.storage.store_metric(metric)
        
        # Mesure de la m√©moire utilis√©e
        memory_usage = 0
        if hasattr(psutil, 'Process'):
            process = psutil.Process()
            memory_usage = process.memory_info().rss / 1024 / 1024  # MB
        
        return {
            "objects_created": len(large_objects),
            "memory_usage_mb": memory_usage
        }
    
    async def _test_connection_limits(self) -> Dict[str, float]:
        """Test des limites de connexion."""
        # Ce test d√©pend du backend de stockage
        # Pour SQLite, on teste les acc√®s concurrents
        
        num_concurrent = 50
        
        async def concurrent_access(task_id: int):
            metric = MetricDataPoint(
                metric_id=f"stress.connection.task_{task_id}",
                value=float(task_id),
                metric_type=MetricType.GAUGE
            )
            await self.test_storage.storage.store_metric(metric)
            
            # Lecture imm√©diate
            results = await self.test_storage.storage.query_metrics(
                metric_pattern=f"stress.connection.task_{task_id}",
                start_time=datetime.now() - timedelta(minutes=1),
                end_time=datetime.now() + timedelta(minutes=1)
            )
            return len(results)
        
        start_time = time.time()
        
        # Acc√®s concurrents
        tasks = [concurrent_access(i) for i in range(num_concurrent)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        connection_time = time.time() - start_time
        
        # Comptage des succ√®s
        successful_connections = sum(1 for r in results if isinstance(r, int) and r > 0)
        
        return {
            "concurrent_connections": num_concurrent,
            "successful_connections": successful_connections,
            "connection_time": connection_time,
            "success_rate": successful_connections / num_concurrent * 100
        }
    
    async def _test_error_recovery(self) -> Dict[str, float]:
        """Test de r√©cup√©ration d'erreur."""
        errors_encountered = 0
        successful_operations = 0
        
        # Simulation d'erreurs et de r√©cup√©ration
        for i in range(100):
            try:
                if i % 10 == 0:
                    # Simulation d'une erreur (m√©trique invalide)
                    metric = MetricDataPoint(
                        metric_id="",  # ID invalide
                        value=float(i),
                        metric_type=MetricType.GAUGE
                    )
                    await self.test_storage.storage.store_metric(metric)
                else:
                    # Op√©ration normale
                    metric = MetricDataPoint(
                        metric_id=f"stress.error_recovery.metric_{i}",
                        value=float(i),
                        metric_type=MetricType.GAUGE
                    )
                    await self.test_storage.storage.store_metric(metric)
                    successful_operations += 1
                    
            except Exception:
                errors_encountered += 1
                # Continue avec l'op√©ration suivante (r√©cup√©ration)
        
        return {
            "total_operations": 100,
            "successful_operations": successful_operations,
            "errors_encountered": errors_encountered,
            "recovery_rate": successful_operations / 100 * 100
        }
    
    async def _run_api_tests(self) -> Dict[str, Dict[str, Any]]:
        """Ex√©cute les tests d'API."""
        logging.info("üåê Ex√©cution des tests d'API")
        
        # Pour l'instant, tests basiques sans serveur HTTP
        tests = {
            "test_metric_serialization": self._test_metric_serialization,
            "test_json_compatibility": self._test_json_compatibility,
            "test_data_formats": self._test_data_formats
        }
        
        results = {}
        for test_name, test_func in tests.items():
            try:
                start_time = time.time()
                await test_func()
                execution_time = time.time() - start_time
                
                results[test_name] = {
                    "passed": True,
                    "execution_time": execution_time,
                    "message": "Test d'API r√©ussi"
                }
                logging.info(f"‚úÖ {test_name}: OK ({execution_time:.3f}s)")
                
            except Exception as e:
                results[test_name] = {
                    "passed": False,
                    "execution_time": time.time() - start_time,
                    "error": str(e),
                    "message": f"Test d'API √©chou√©: {e}"
                }
                logging.error(f"‚ùå {test_name}: √âCHEC - {e}")
        
        return results
    
    async def _test_metric_serialization(self):
        """Test de s√©rialisation des m√©triques."""
        metric = MetricDataPoint(
            metric_id="test.api.serialization",
            value=42.0,
            metric_type=MetricType.GAUGE,
            tags={"env": "test", "service": "api"},
            metadata={"description": "Test metric for serialization"}
        )
        
        # Test de conversion en dictionnaire
        metric_dict = {
            "metric_id": metric.metric_id,
            "timestamp": metric.timestamp.isoformat(),
            "value": metric.value,
            "metric_type": metric.metric_type.value,
            "category": metric.category.value,
            "severity": metric.severity.value,
            "tags": metric.tags,
            "metadata": metric.metadata
        }
        
        # V√©rification de la structure
        assert "metric_id" in metric_dict
        assert "timestamp" in metric_dict
        assert "value" in metric_dict
        assert isinstance(metric_dict["tags"], dict)
        assert isinstance(metric_dict["metadata"], dict)
    
    async def _test_json_compatibility(self):
        """Test de compatibilit√© JSON."""
        metric = MetricDataPoint(
            metric_id="test.api.json",
            value=123.45,
            metric_type=MetricType.COUNTER,
            tags={"unicode": "√±√°√©√≠√≥√∫", "special": "!@#$%"}
        )
        
        # Test de s√©rialisation JSON
        metric_json = json.dumps({
            "metric_id": metric.metric_id,
            "value": metric.value,
            "metric_type": metric.metric_type.value,
            "tags": metric.tags
        }, ensure_ascii=False)
        
        # Test de d√©s√©rialisation
        parsed_data = json.loads(metric_json)
        
        assert parsed_data["metric_id"] == metric.metric_id
        assert parsed_data["value"] == metric.value
        assert parsed_data["tags"]["unicode"] == "√±√°√©√≠√≥√∫"
    
    async def _test_data_formats(self):
        """Test des formats de donn√©es."""
        # Test avec diff√©rents types de valeurs
        test_values = [
            0,
            42,
            -10,
            3.14159,
            1e10,
            1e-10
        ]
        
        for value in test_values:
            metric = MetricDataPoint(
                metric_id="test.api.formats",
                value=float(value),
                metric_type=MetricType.GAUGE
            )
            
            # V√©rification que la valeur est correctement stock√©e
            assert metric.value == float(value)
        
        # Test avec diff√©rents formats de timestamp
        timestamps = [
            datetime.now(),
            datetime.now() - timedelta(hours=1),
            datetime.now() + timedelta(minutes=30)
        ]
        
        for ts in timestamps:
            metric = MetricDataPoint(
                metric_id="test.api.timestamp",
                timestamp=ts,
                value=42.0,
                metric_type=MetricType.GAUGE
            )
            
            assert metric.timestamp == ts


async def run_comprehensive_tests():
    """Fonction principale pour ex√©cuter tous les tests."""
    print("üß™ D√©marrage de la Suite de Tests Compl√®te")
    print("=" * 60)
    print(f"Expert Development Team - Projet dirig√© par Fahed Mlaiel")
    print("=" * 60)
    
    # Cr√©ation de la suite de tests
    test_suite = MetricsSystemTestSuite()
    
    try:
        # Ex√©cution de tous les tests
        results = await test_suite.run_all_tests()
        
        # Affichage des r√©sultats
        print("\n" + "=" * 60)
        print("üìä R√âSULTATS DES TESTS")
        print("=" * 60)
        
        summary = results["summary"]
        print(f"üìà Tests totaux: {summary['total_tests']}")
        print(f"‚úÖ Tests r√©ussis: {summary['passed']}")
        print(f"‚ùå Tests √©chou√©s: {summary['total_tests'] - summary['passed']}")
        print(f"üéØ Taux de r√©ussite: {summary['success_rate']:.1f}%")
        print(f"‚è±Ô∏è  Temps d'ex√©cution: {summary['execution_time']:.1f}s")
        
        # D√©tails par cat√©gorie
        categories = ["unit_tests", "integration_tests", "performance_tests", "security_tests", "stress_tests", "api_tests"]
        
        for category in categories:
            if category in results:
                category_results = results[category]
                passed = sum(1 for t in category_results.values() if t.get("passed", False))
                total = len(category_results)
                print(f"\nüìã {category.replace('_', ' ').title()}: {passed}/{total}")
                
                for test_name, test_result in category_results.items():
                    status = "‚úÖ" if test_result.get("passed", False) else "‚ùå"
                    time_str = f"({test_result.get('execution_time', 0):.3f}s)"
                    print(f"  {status} {test_name} {time_str}")
        
        # R√©sultats de performance
        if "performance_tests" in results:
            print("\n" + "=" * 60)
            print("‚ö° M√âTRIQUES DE PERFORMANCE")
            print("=" * 60)
            
            for test_name, test_result in results["performance_tests"].items():
                if test_result.get("passed") and "performance_data" in test_result:
                    perf_data = test_result["performance_data"]
                    print(f"\nüìä {test_name}:")
                    for key, value in perf_data.items():
                        if isinstance(value, float):
                            print(f"  ‚Ä¢ {key}: {value:.3f}")
                        else:
                            print(f"  ‚Ä¢ {key}: {value}")
        
        # Sauvegarde des r√©sultats
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"test_results_{timestamp}.json"
        
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        print(f"\nüíæ R√©sultats sauvegard√©s dans: {results_file}")
        
        # Score final
        if summary["success_rate"] >= 95:
            print("\nüèÜ EXCELLENT! Tous les tests sont r√©ussis.")
        elif summary["success_rate"] >= 80:
            print("\nüëç BIEN! La plupart des tests sont r√©ussis.")
        else:
            print("\n‚ö†Ô∏è  ATTENTION! Plusieurs tests ont √©chou√©.")
        
        print("\n" + "=" * 60)
        print("üéâ Suite de tests termin√©e avec succ√®s!")
        print(f"D√©velopp√© par l'√âquipe d'Experts - Dirig√© par Fahed Mlaiel")
        print("=" * 60)
        
        return results
        
    except Exception as e:
        print(f"\nüí• ERREUR FATALE dans la suite de tests: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    # Ex√©cution de la suite de tests
    asyncio.run(run_comprehensive_tests())
