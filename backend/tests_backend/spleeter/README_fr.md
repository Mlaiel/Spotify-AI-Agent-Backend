# ğŸµ Spotify AI Agent - README Tests Spleeter
# ============================================
# 
# Guide complet pour l'exÃ©cution et la maintenance
# des tests du module Spleeter enterprise.
#
# ğŸ–ï¸ DÃ©veloppÃ© par l'Ã©quipe d'experts enterprise

# ğŸµ Tests Spleeter - Guide Complet

Bienvenue dans la suite de tests complÃ¨te du module **Spleeter** de l'agent IA Spotify ! ğŸš€

Cette documentation vous guidera Ã  travers l'installation, la configuration et l'exÃ©cution de tous les types de tests disponibles.

## ğŸ“‹ Table des MatiÃ¨res

- [ğŸ¯ Vue d'Ensemble](#-vue-densemble)
- [ğŸ› ï¸ Installation](#ï¸-installation)
- [ğŸš€ DÃ©marrage Rapide](#-dÃ©marrage-rapide)
- [ğŸ“Š Types de Tests](#-types-de-tests)
- [âš™ï¸ Configuration](#ï¸-configuration)
- [ğŸ”§ Commandes Makefile](#-commandes-makefile)
- [ğŸ Script Python](#-script-python)
- [ğŸ“ˆ Couverture de Code](#-couverture-de-code)
- [ğŸ­ CI/CD](#-cicd)
- [ğŸ” Debugging](#-debugging)
- [ğŸ“ Contribuer](#-contribuer)

## ğŸ¯ Vue d'Ensemble

La suite de tests Spleeter comprend **11 modules de test** couvrant tous les aspects du systÃ¨me :

### ğŸ“ Structure des Tests

```
tests_backend/spleeter/
â”œâ”€â”€ ğŸ“‹ conftest.py              # Configuration globale & fixtures
â”œâ”€â”€ ğŸ§ª test_core.py             # Tests moteur principal
â”œâ”€â”€ ğŸ¤– test_models.py           # Tests gestion modÃ¨les
â”œâ”€â”€ ğŸµ test_processor.py        # Tests traitement audio
â”œâ”€â”€ ğŸ’¾ test_cache.py            # Tests systÃ¨me cache
â”œâ”€â”€ ğŸ”§ test_utils.py            # Tests utilitaires
â”œâ”€â”€ ğŸ“Š test_monitoring.py       # Tests monitoring
â”œâ”€â”€ âš ï¸ test_exceptions.py       # Tests gestion erreurs
â”œâ”€â”€ ğŸ”— test_integration.py      # Tests d'intÃ©gration
â”œâ”€â”€ âš¡ test_performance.py      # Tests performance
â”œâ”€â”€ ğŸ› ï¸ test_helpers.py          # Utilitaires de test
â”œâ”€â”€ ğŸ“‹ Makefile                 # Automatisation make
â”œâ”€â”€ ğŸ run_tests.sh             # Script bash automation
â”œâ”€â”€ âš™ï¸ pyproject.toml           # Configuration pytest
â””â”€â”€ ğŸ“– README.md                # Cette documentation
```

### ğŸ–ï¸ Couverture de Test

- **Tests Unitaires** : Validation de chaque composant individuellement
- **Tests d'IntÃ©gration** : Validation des workflows complets
- **Tests de Performance** : Benchmarks et optimisations
- **Tests de Stress** : Validation sous charge Ã©levÃ©e
- **Tests de SÃ©curitÃ©** : Validation des contrÃ´les de sÃ©curitÃ©
- **Tests de RÃ©gression** : PrÃ©vention des rÃ©gressions

## ğŸ› ï¸ Installation

### PrÃ©requis SystÃ¨me

```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install -y python3 python3-pip ffmpeg libsndfile1

# macOS
brew install python ffmpeg libsndfile

# Windows (avec chocolatey)
choco install python ffmpeg
```

### Installation des DÃ©pendances Python

```bash
# Installation automatique via le script
cd /workspaces/Achiri/spotify-ai-agent/backend/tests_backend/spleeter
chmod +x run_tests.sh
./run_tests.sh --install-deps

# Ou installation manuelle
pip install pytest pytest-asyncio pytest-cov pytest-mock pytest-benchmark
pip install black flake8 mypy isort coverage[toml]
pip install pytest-html pytest-xdist pytest-timeout
```

### Installation via Makefile

```bash
# Installation complÃ¨te des dÃ©pendances
make install-deps

# VÃ©rification de la configuration
make show-config
```

## ğŸš€ DÃ©marrage Rapide

### Tests Rapides (< 1 minute)

```bash
# Via Makefile
make test-fast

# Via script bash
./run_tests.sh --smoke

# Via pytest direct
pytest -m "not slow and not performance and not stress" --timeout=30
```

### Tests Complets

```bash
# Pipeline CI/CD complet
./run_tests.sh --ci

# Tous les tests via Makefile
make test-all

# Tests avec couverture
make coverage-html
```

### Tests SpÃ©cifiques

```bash
# Tests par module
make test-core          # Moteur principal
make test-models        # Gestion modÃ¨les
make test-processor     # Traitement audio
make test-cache         # SystÃ¨me cache

# Tests par type
make test-unit          # Tests unitaires
make test-integration   # Tests d'intÃ©gration
make test-performance   # Tests de performance
```

## ğŸ“Š Types de Tests

### ğŸ§ª Tests Unitaires

Validation individuelle de chaque composant :

```bash
# ExÃ©cution des tests unitaires
pytest test_core.py test_models.py test_processor.py \
       test_cache.py test_utils.py test_monitoring.py test_exceptions.py \
       -v -m "not slow"
```

**Couverture** :
- âœ… Initialisation des engines
- âœ… SÃ©paration audio basique
- âœ… Gestion des modÃ¨les ML
- âœ… Cache multi-niveaux
- âœ… Utilitaires et validation
- âœ… Monitoring systÃ¨me
- âœ… Gestion d'exceptions

### ğŸ”— Tests d'IntÃ©gration

Validation des workflows end-to-end :

```bash
# Tests d'intÃ©gration
pytest test_integration.py -v -m "integration" --timeout=120
```

**ScÃ©narios** :
- âœ… SÃ©paration complÃ¨te avec cache
- âœ… Traitement batch avec monitoring
- âœ… Gestion d'erreurs robuste
- âœ… Workflows concurrents

### âš¡ Tests de Performance

Benchmarks et optimisations :

```bash
# Tests de performance
pytest test_performance.py -v -m "performance" --benchmark-only
```

**MÃ©triques** :
- â±ï¸ Temps de sÃ©paration audio
- ğŸ“Š Utilisation mÃ©moire
- ğŸš€ Throughput batch
- ğŸ’¾ Performance cache
- ğŸ“ˆ Monitoring overhead

### ğŸ’ª Tests de Stress

Validation sous charge Ã©levÃ©e :

```bash
# Tests de stress
pytest test_performance.py -v -m "stress" --timeout=600
```

**ScÃ©narios** :
- ğŸ”¥ Charge CPU maximale
- ğŸ’§ Stress mÃ©moire
- ğŸŒŠ Traitement concurrent
- ğŸ¯ StabilitÃ© longue durÃ©e

## âš™ï¸ Configuration

### Configuration pytest (pyproject.toml)

La configuration centralisÃ©e dans `pyproject.toml` inclut :

```toml
[tool.pytest.ini_options]
markers = [
    "slow: tests lents",
    "fast: tests rapides", 
    "unit: tests unitaires",
    "integration: tests d'intÃ©gration",
    "performance: tests de performance",
    "stress: tests de stress"
]

addopts = [
    "-v",
    "--tb=short",
    "--strict-markers",
    "--durations=10",
    "--maxfail=3"
]
```

### Variables d'Environnement

```bash
export SPLEETER_TEST_MODE="true"
export SPLEETER_LOG_LEVEL="DEBUG"
export SPLEETER_CACHE_DISABLED="true"
export COVERAGE_MIN="85"
```

### Configuration Coverage

```toml
[tool.coverage.run]
source = ["backend/spleeter"]
branch = true
omit = ["*/tests/*", "*/__pycache__/*"]

[tool.coverage.report]
fail_under = 85
show_missing = true
```

## ğŸ”§ Commandes Makefile

### Commandes Principales

```bash
make help              # Affichage de l'aide complÃ¨te
make test              # Tests de base (rapides)
make test-all          # Tous les tests (complet)
make coverage          # Analyse de couverture console
make coverage-html     # Rapport HTML de couverture
make lint              # VÃ©rifications linting
make format            # Formatage automatique
make clean             # Nettoyage fichiers temporaires
```

### Tests par Module

```bash
make test-core         # Tests moteur principal
make test-models       # Tests gestion modÃ¨les  
make test-processor    # Tests traitement audio
make test-cache        # Tests systÃ¨me cache
make test-utils        # Tests utilitaires
make test-monitoring   # Tests monitoring
make test-exceptions   # Tests gestion erreurs
```

### Tests par Type

```bash
make test-unit         # Tests unitaires
make test-integration  # Tests d'intÃ©gration
make test-performance  # Tests de performance
make test-stress       # Tests de stress
make test-fast         # Tests rapides seulement
make test-slow         # Tests lents seulement
```

### Utilitaires AvancÃ©s

```bash
make test-parallel     # Tests en parallÃ¨le
make test-report       # Tests avec rapport HTML
make benchmark         # Benchmarks spÃ©cifiques
make test-security     # Tests de sÃ©curitÃ©
make test-regression   # Tests de rÃ©gression
make smoke-test        # Tests de smoke
make acceptance-test   # Tests d'acceptation
```

## ğŸ Script Python

### Utilisation du Script

```bash
# Rendre le script exÃ©cutable
chmod +x run_tests.sh

# Aide complÃ¨te
./run_tests.sh --help

# Pipeline CI/CD complet
./run_tests.sh --ci

# Tests spÃ©cifiques
./run_tests.sh --unit
./run_tests.sh --integration
./run_tests.sh --performance
./run_tests.sh --stress
```

### Options Disponibles

| Option | Description |
|--------|-------------|
| `--ci` | Pipeline CI/CD complet |
| `--unit` | Tests unitaires uniquement |
| `--integration` | Tests d'intÃ©gration uniquement |
| `--performance` | Tests de performance uniquement |
| `--stress` | Tests de stress uniquement |
| `--coverage` | Analyse de couverture uniquement |
| `--quality` | VÃ©rifications de qualitÃ© uniquement |
| `--smoke` | Tests de smoke (vÃ©rification rapide) |
| `--install-deps` | Installation des dÃ©pendances |
| `--cleanup` | Nettoyage des fichiers temporaires |

### Configuration du Script

```bash
# Variables d'environnement pour personnalisation
export COVERAGE_MIN=90           # Couverture minimale
export TIMEOUT_UNIT=45          # Timeout tests unitaires
export TIMEOUT_INTEGRATION=180  # Timeout tests intÃ©gration
export TIMEOUT_PERFORMANCE=450  # Timeout tests performance
export TIMEOUT_STRESS=900       # Timeout tests stress
```

## ğŸ“ˆ Couverture de Code

### GÃ©nÃ©ration des Rapports

```bash
# Rapport console
make coverage

# Rapport HTML dÃ©taillÃ©
make coverage-html
# Ouverture automatique: coverage_html/index.html

# Rapport XML (pour CI/CD)
make coverage-xml
# Fichier gÃ©nÃ©rÃ©: coverage.xml
```

### Seuils de Couverture

- **Couverture Globale** : â‰¥ 85%
- **Couverture par Fichier** : â‰¥ 80%
- **Couverture Branches** : â‰¥ 75%

### Analyse de la Couverture

```bash
# Couverture diffÃ©rentielle
make diff-coverage

# Couverture avec contexte
pytest --cov=../../spleeter \
       --cov-context=test \
       --cov-branch
```

## ğŸ­ CI/CD

### GitHub Actions

Le workflow GitHub Actions inclut :

```yaml
# .github/workflows/spleeter-tests.yml
name: ğŸµ Spleeter Tests CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    - cron: '0 2 * * *'  # Tests quotidiens

jobs:
  smoke-tests:    # Tests de vÃ©rification rapide
  unit-tests:     # Tests unitaires multi-plateforme
  integration-tests: # Tests d'intÃ©gration avec services
  performance-tests: # Tests de performance et benchmarks
  stress-tests:   # Tests de charge et stress
  security-tests: # Tests de sÃ©curitÃ©
  code-quality:   # VÃ©rifications qualitÃ© de code
  final-report:   # Rapport consolidÃ©
```

### Configuration Locale

```bash
# Simulation locale du pipeline CI/CD
./run_tests.sh --ci

# VÃ©rification pre-commit
make pre-commit

# Validation complÃ¨te
make check-all
```

### IntÃ©gration Continue

```bash
# Tests rapides pour dÃ©veloppement
make test-fast

# Tests complets pour release
make test-all coverage-html

# VÃ©rification avant commit
make pre-commit
```

## ğŸ” Debugging

### Debug de Tests SpÃ©cifiques

```bash
# Mode debug verbose
make test-debug TEST=test_core.py::test_engine_initialization

# Debug avec pdb
pytest test_core.py::test_engine_initialization -vvv -s --pdb

# Profiling de performance
make profile-tests
```

### Logs de Debug

```bash
# Activation des logs dÃ©taillÃ©s
export SPLEETER_LOG_LEVEL="DEBUG"
pytest test_core.py -vvv -s --log-cli-level=DEBUG
```

### Tests en Mode Watch

```bash
# Mode watch pour dÃ©veloppement
make watch-tests

# Tests continus avec pytest-watch
pip install pytest-watch
ptw -- -m "not slow"
```

### Debugging d'Ã‰checs

```bash
# Tests avec informations d'Ã©chec Ã©tendues
pytest --tb=long --capture=no

# GÃ©nÃ©ration de rapports d'Ã©chec
pytest --html=failure_report.html --self-contained-html
```

## ğŸ“Š MÃ©triques et Monitoring

### MÃ©triques CollectÃ©es

- **Performance** : Temps d'exÃ©cution, throughput
- **Ressources** : CPU, mÃ©moire, I/O
- **QualitÃ©** : Couverture, complexitÃ©
- **FiabilitÃ©** : Taux de rÃ©ussite, stabilitÃ©

### Rapports Disponibles

```bash
# Rapport HTML complet
make test-report

# MÃ©triques de performance
make test-metrics

# Export JSON des rÃ©sultats
make test-export
```

### Monitoring Continue

```bash
# Tests avec monitoring des ressources
pytest test_performance.py::TestResourceMonitoring -v

# Suivi de la mÃ©moire
pytest --memray

# Profiling CPU
pytest --profile
```

## ğŸ”§ Maintenance

### Mise Ã  Jour des DÃ©pendances

```bash
# VÃ©rification des versions
pip list --outdated

# Mise Ã  jour sÃ©curisÃ©e
pip install --upgrade -r requirements/testing.txt

# VÃ©rification de compatibilitÃ©
./run_tests.sh --smoke
```

### Nettoyage PÃ©riodique

```bash
# Nettoyage complet
make clean

# Suppression des caches
rm -rf .pytest_cache __pycache__ .coverage

# Nettoyage des artifacts
rm -rf coverage_html test_report.html
```

### Optimisation des Tests

```bash
# Tests en parallÃ¨le
make test-parallel

# Optimisation des fixtures
pytest --setup-show

# Analyse des durÃ©es
pytest --durations=0
```

## ğŸ” SÃ©curitÃ©

### Tests de SÃ©curitÃ©

```bash
# Tests de sÃ©curitÃ© spÃ©cifiques
make test-security

# Scan de vulnÃ©rabilitÃ©s
bandit -r ../../spleeter/

# VÃ©rification des dÃ©pendances
safety check
```

### Validation des EntrÃ©es

```bash
# Tests de validation
pytest -k "validation or sanitize" test_utils.py

# Tests d'injection
pytest -k "security" test_exceptions.py
```

## ğŸ“ Contribuer

### Ajout de Nouveaux Tests

1. **CrÃ©er le fichier de test** dans le rÃ©pertoire appropriÃ©
2. **Utiliser les fixtures** dÃ©finies dans `conftest.py`
3. **Ajouter les marqueurs** appropriÃ©s (`@pytest.mark.unit`, etc.)
4. **Documenter les tests** avec des docstrings claires
5. **VÃ©rifier la couverture** avec `make coverage`

### Structure d'un Test

```python
import pytest
from unittest.mock import Mock, patch

class TestNewFeature:
    """Tests pour la nouvelle fonctionnalitÃ©."""
    
    @pytest.mark.unit
    @pytest.mark.fast
    def test_basic_functionality(self, mock_engine):
        """Test de la fonctionnalitÃ© de base."""
        # Arrange
        input_data = "test_input"
        expected = "expected_output"
        
        # Act
        result = mock_engine.process(input_data)
        
        # Assert
        assert result == expected
    
    @pytest.mark.integration
    @pytest.mark.slow
    async def test_integration_workflow(self):
        """Test d'intÃ©gration complet."""
        # Test d'intÃ©gration avec vraies dÃ©pendances
        pass
```

### Guidelines de Test

- **Nommage** : `test_<functionality>_<scenario>`
- **Isolation** : Chaque test doit Ãªtre indÃ©pendant
- **Mocking** : Utiliser les mocks pour les dÃ©pendances externes
- **Assertions** : Assertions claires et spÃ©cifiques
- **Documentation** : Docstrings explicatives
- **Marqueurs** : Utilisation appropriÃ©e des marqueurs pytest

### Validation Pre-Commit

```bash
# Validation complÃ¨te avant commit
make pre-commit

# VÃ©rifications individuelles
make format    # Formatage du code
make lint      # VÃ©rifications linting
make type-check # VÃ©rification des types
make test-fast # Tests rapides
```

## ğŸ¯ Objectifs de QualitÃ©

### Cibles de Performance

- **Tests Unitaires** : < 30 secondes total
- **Tests d'IntÃ©gration** : < 2 minutes total  
- **Tests de Performance** : < 5 minutes total
- **SÃ©paration Audio** : < 5 secondes/fichier

### Cibles de Couverture

- **Couverture Globale** : â‰¥ 85%
- **Couverture Branches** : â‰¥ 75%
- **Modules Critiques** : â‰¥ 90%

### Cibles de FiabilitÃ©

- **Taux de RÃ©ussite** : â‰¥ 99%
- **StabilitÃ©** : 0 tests flaky
- **ReproductibilitÃ©** : 100%

## ğŸš€ Performances et Optimisations

### Optimisations de Test

```bash
# Tests en parallÃ¨le
pytest -n auto

# Cache des rÃ©sultats
pytest --cache-clear  # Reset cache
pytest --lf          # Last failed only
pytest --ff          # Failed first

# Optimisation mÃ©moire
pytest --maxfail=1 --tb=no
```

### Monitoring des Performances

```bash
# Profiling dÃ©taillÃ©
pytest --profile --profile-svg

# Monitoring mÃ©moire
pytest --memray --memray-bin-path=memory_profile.bin

# Analyse des goulots d'Ã©tranglement
pytest --durations=20
```

## ğŸ“ Support et Aide

### Ressources

- **Documentation** : Ce README et les docstrings
- **Exemples** : Fichiers de test existants
- **Configuration** : `pyproject.toml`, `conftest.py`
- **Scripts** : `Makefile`, `run_tests.sh`

### Commandes d'Aide

```bash
# Aide Makefile
make help

# Aide script bash
./run_tests.sh --help

# Aide pytest
pytest --help

# Configuration active
make show-config
```

### DÃ©pannage Courant

**Tests qui Ã©chouent** :
```bash
# Debug mode verbose
pytest -vvv -s --tb=long

# Tests isolÃ©s
pytest test_file.py::test_function -v
```

**Performance lente** :
```bash
# Profiling
make profile-tests

# Tests rapides seulement
make test-fast
```

**ProblÃ¨mes de couverture** :
```bash
# Rapport dÃ©taillÃ©
make coverage-html

# Fichiers manquÃ©s
coverage report --show-missing
```

---

## ğŸ‰ Conclusion

Cette suite de tests complÃ¨te garantit la qualitÃ©, la performance et la fiabilitÃ© du module Spleeter. Utilisez cette documentation comme rÃ©fÃ©rence pour tous vos besoins de test !

**Bon testing ! ğŸš€ğŸµ**

---

*DÃ©veloppÃ© avec â¤ï¸ par l'Ã©quipe d'experts enterprise pour l'agent IA Spotify*
