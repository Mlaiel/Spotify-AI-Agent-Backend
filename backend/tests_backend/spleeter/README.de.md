# ğŸµ Spotify AI Agent - Spleeter Tests VollstÃ¤ndiger Leitfaden
# ==========================================================
# 
# VollstÃ¤ndiger Leitfaden fÃ¼r die AusfÃ¼hrung und Wartung
# der Tests fÃ¼r das Enterprise Spleeter-Modul.
#
# ğŸ–ï¸ Entwickelt vom Enterprise-Expertenteam

# ğŸµ Spleeter Tests - VollstÃ¤ndiger Leitfaden

Willkommen zur vollstÃ¤ndigen Test-Suite fÃ¼r das **Spleeter**-Modul des Spotify AI-Agenten! ğŸš€

Diese Dokumentation fÃ¼hrt Sie durch Installation, Konfiguration und AusfÃ¼hrung aller verfÃ¼gbaren Testtypen.

## ğŸ“‹ Inhaltsverzeichnis

- [ğŸ¯ Ãœberblick](#-Ã¼berblick)
- [ğŸ› ï¸ Installation](#ï¸-installation)
- [ğŸš€ Schnellstart](#-schnellstart)
- [ğŸ“Š Testtypen](#-testtypen)
- [âš™ï¸ Konfiguration](#ï¸-konfiguration)
- [ğŸ”§ Makefile-Befehle](#-makefile-befehle)
- [ğŸ Python-Skript](#-python-skript)
- [ğŸ“ˆ Code-Abdeckung](#-code-abdeckung)
- [ğŸ­ CI/CD](#-cicd)
- [ğŸ” Debugging](#-debugging)
- [ğŸ“ Mitwirken](#-mitwirken)

## ğŸ¯ Ãœberblick

Die Spleeter-Test-Suite umfasst **11 Testmodule**, die alle Systemaspekte abdecken:

### ğŸ“ Teststruktur

```
tests_backend/spleeter/
â”œâ”€â”€ ğŸ“‹ conftest.py              # Globale Konfiguration & Fixtures
â”œâ”€â”€ ğŸ§ª test_core.py             # Kern-Engine-Tests
â”œâ”€â”€ ğŸ¤– test_models.py           # Modellverwaltungs-Tests
â”œâ”€â”€ ğŸµ test_processor.py        # Audio-Verarbeitungs-Tests
â”œâ”€â”€ ğŸ’¾ test_cache.py            # Cache-System-Tests
â”œâ”€â”€ ğŸ”§ test_utils.py            # Hilfsprogramm-Tests
â”œâ”€â”€ ğŸ“Š test_monitoring.py       # Ãœberwachungs-Tests
â”œâ”€â”€ âš ï¸ test_exceptions.py       # Fehlerbehandlungs-Tests
â”œâ”€â”€ ğŸ”— test_integration.py      # Integrations-Tests
â”œâ”€â”€ âš¡ test_performance.py      # Leistungs-Tests
â”œâ”€â”€ ğŸ› ï¸ test_helpers.py          # Test-Hilfsprogramme
â”œâ”€â”€ ğŸ“‹ Makefile                 # Make-Automatisierung
â”œâ”€â”€ ğŸ run_tests.sh             # Bash-Automatisierungsskript
â”œâ”€â”€ âš™ï¸ pyproject.toml           # Pytest-Konfiguration
â””â”€â”€ ğŸ“– README.de.md             # Diese Dokumentation
```

### ğŸ–ï¸ Test-Abdeckung

- **Unit-Tests**: Validierung einzelner Komponenten
- **Integrations-Tests**: Validierung vollstÃ¤ndiger Workflows
- **Leistungs-Tests**: Benchmarks und Optimierungen
- **Stress-Tests**: Validierung unter hoher Last
- **Sicherheits-Tests**: Validierung von Sicherheitskontrollen
- **Regressions-Tests**: RegressionsprÃ¤vention

## ğŸ› ï¸ Installation

### System-Voraussetzungen

```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install -y python3 python3-pip ffmpeg libsndfile1

# macOS
brew install python ffmpeg libsndfile

# Windows (mit chocolatey)
choco install python ffmpeg
```

### Python-AbhÃ¤ngigkeiten Installation

```bash
# Automatische Installation Ã¼ber Skript
cd /workspaces/Achiri/spotify-ai-agent/backend/tests_backend/spleeter
chmod +x run_tests.sh
./run_tests.sh --install-deps

# Oder manuelle Installation
pip install pytest pytest-asyncio pytest-cov pytest-mock pytest-benchmark
pip install black flake8 mypy isort coverage[toml]
pip install pytest-html pytest-xdist pytest-timeout
```

### Installation Ã¼ber Makefile

```bash
# VollstÃ¤ndige AbhÃ¤ngigkeits-Installation
make install-deps

# KonfigurationsÃ¼berprÃ¼fung
make show-config
```

## ğŸš€ Schnellstart

### Schnelle Tests (< 1 Minute)

```bash
# Ãœber Makefile
make test-fast

# Ãœber Bash-Skript
./run_tests.sh --smoke

# Ãœber direktes pytest
pytest -m "not slow and not performance and not stress" --timeout=30
```

### VollstÃ¤ndige Tests

```bash
# VollstÃ¤ndige CI/CD-Pipeline
./run_tests.sh --ci

# Alle Tests Ã¼ber Makefile
make test-all

# Tests mit Abdeckung
make coverage-html
```

### Spezifische Tests

```bash
# Tests nach Modul
make test-core          # Kern-Engine
make test-models        # Modellverwaltung
make test-processor     # Audio-Verarbeitung
make test-cache         # Cache-System

# Tests nach Typ
make test-unit          # Unit-Tests
make test-integration   # Integrations-Tests
make test-performance   # Leistungs-Tests
```

## ğŸ“Š Testtypen

### ğŸ§ª Unit-Tests

Individuelle Validierung jeder Komponente:

```bash
# Unit-Test-AusfÃ¼hrung
pytest test_core.py test_models.py test_processor.py \
       test_cache.py test_utils.py test_monitoring.py test_exceptions.py \
       -v -m "not slow"
```

**Abdeckung**:
- âœ… Engine-Initialisierung
- âœ… Grundlegende Audio-Trennung
- âœ… ML-Modellverwaltung
- âœ… Mehrstufiger Cache
- âœ… Hilfsprogramme und Validierung
- âœ… System-Ãœberwachung
- âœ… Exception-Handling

### ğŸ”— Integrations-Tests

End-to-End-Workflow-Validierung:

```bash
# Integrations-Tests
pytest test_integration.py -v -m "integration" --timeout=120
```

**Szenarien**:
- âœ… VollstÃ¤ndige Trennung mit Cache
- âœ… Batch-Verarbeitung mit Ãœberwachung
- âœ… Robuste Fehlerbehandlung
- âœ… NebenlÃ¤ufige Workflows

### âš¡ Leistungs-Tests

Benchmarks und Optimierungen:

```bash
# Leistungs-Tests
pytest test_performance.py -v -m "performance" --benchmark-only
```

**Metriken**:
- â±ï¸ Audio-Trennungszeit
- ğŸ“Š Speicherverbrauch
- ğŸš€ Batch-Durchsatz
- ğŸ’¾ Cache-Leistung
- ğŸ“ˆ Ãœberwachungs-Overhead

### ğŸ’ª Stress-Tests

Validierung unter hoher Last:

```bash
# Stress-Tests
pytest test_performance.py -v -m "stress" --timeout=600
```

**Szenarien**:
- ğŸ”¥ Maximale CPU-Last
- ğŸ’§ Speicher-Stress
- ğŸŒŠ NebenlÃ¤ufige Verarbeitung
- ğŸ¯ Langzeit-StabilitÃ¤t

## âš™ï¸ Konfiguration

### Pytest-Konfiguration (pyproject.toml)

Zentralisierte Konfiguration in `pyproject.toml` umfasst:

```toml
[tool.pytest.ini_options]
markers = [
    "slow: langsame Tests",
    "fast: schnelle Tests", 
    "unit: Unit-Tests",
    "integration: Integrations-Tests",
    "performance: Leistungs-Tests",
    "stress: Stress-Tests"
]

addopts = [
    "-v",
    "--tb=short",
    "--strict-markers",
    "--durations=10",
    "--maxfail=3"
]
```

### Umgebungsvariablen

```bash
export SPLEETER_TEST_MODE="true"
export SPLEETER_LOG_LEVEL="DEBUG"
export SPLEETER_CACHE_DISABLED="true"
export COVERAGE_MIN="85"
```

### Coverage-Konfiguration

```toml
[tool.coverage.run]
source = ["backend/spleeter"]
branch = true
omit = ["*/tests/*", "*/__pycache__/*"]

[tool.coverage.report]
fail_under = 85
show_missing = true
```

## ğŸ”§ Makefile-Befehle

### Hauptbefehle

```bash
make help              # VollstÃ¤ndige Hilfe anzeigen
make test              # Basis-Tests (schnell)
make test-all          # Alle Tests (vollstÃ¤ndig)
make coverage          # Konsolen-Coverage-Analyse
make coverage-html     # HTML-Coverage-Bericht
make lint              # Linting-ÃœberprÃ¼fungen
make format            # Automatische Formatierung
make clean             # TemporÃ¤re Dateien aufrÃ¤umen
```

### Tests nach Modul

```bash
make test-core         # Kern-Engine-Tests
make test-models       # Modellverwaltungs-Tests  
make test-processor    # Audio-Verarbeitungs-Tests
make test-cache        # Cache-System-Tests
make test-utils        # Hilfsprogramm-Tests
make test-monitoring   # Ãœberwachungs-Tests
make test-exceptions   # Fehlerbehandlungs-Tests
```

### Tests nach Typ

```bash
make test-unit         # Unit-Tests
make test-integration  # Integrations-Tests
make test-performance  # Leistungs-Tests
make test-stress       # Stress-Tests
make test-fast         # Nur schnelle Tests
make test-slow         # Nur langsame Tests
```

### Erweiterte Hilfsprogramme

```bash
make test-parallel     # Parallele Tests
make test-report       # Tests mit HTML-Bericht
make benchmark         # Spezifische Benchmarks
make test-security     # Sicherheits-Tests
make test-regression   # Regressions-Tests
make smoke-test        # Smoke-Tests
make acceptance-test   # Akzeptanz-Tests
```

## ğŸ Python-Skript

### Skript-Verwendung

```bash
# Skript ausfÃ¼hrbar machen
chmod +x run_tests.sh

# VollstÃ¤ndige Hilfe
./run_tests.sh --help

# VollstÃ¤ndige CI/CD-Pipeline
./run_tests.sh --ci

# Spezifische Tests
./run_tests.sh --unit
./run_tests.sh --integration
./run_tests.sh --performance
./run_tests.sh --stress
```

### VerfÃ¼gbare Optionen

| Option | Beschreibung |
|--------|-------------|
| `--ci` | VollstÃ¤ndige CI/CD-Pipeline |
| `--unit` | Nur Unit-Tests |
| `--integration` | Nur Integrations-Tests |
| `--performance` | Nur Leistungs-Tests |
| `--stress` | Nur Stress-Tests |
| `--coverage` | Nur Coverage-Analyse |
| `--quality` | Nur QualitÃ¤ts-Checks |
| `--smoke` | Smoke-Tests (schnelle ÃœberprÃ¼fung) |
| `--install-deps` | AbhÃ¤ngigkeiten installieren |
| `--cleanup` | TemporÃ¤re Dateien aufrÃ¤umen |

### Skript-Konfiguration

```bash
# Umgebungsvariablen zur Anpassung
export COVERAGE_MIN=90           # Mindest-Coverage
export TIMEOUT_UNIT=45          # Unit-Test-Timeout
export TIMEOUT_INTEGRATION=180  # Integrations-Test-Timeout
export TIMEOUT_PERFORMANCE=450  # Leistungs-Test-Timeout
export TIMEOUT_STRESS=900       # Stress-Test-Timeout
```

## ğŸ“ˆ Code-Abdeckung

### Bericht-Generierung

```bash
# Konsolen-Bericht
make coverage

# Detaillierter HTML-Bericht
make coverage-html
# Automatisches Ã–ffnen: coverage_html/index.html

# XML-Bericht (fÃ¼r CI/CD)
make coverage-xml
# Generierte Datei: coverage.xml
```

### Coverage-Schwellenwerte

- **Globale Coverage**: â‰¥ 85%
- **Pro-Datei-Coverage**: â‰¥ 80%
- **Branch-Coverage**: â‰¥ 75%

### Coverage-Analyse

```bash
# Differentielle Coverage
make diff-coverage

# Coverage mit Kontext
pytest --cov=../../spleeter \
       --cov-context=test \
       --cov-branch
```

## ğŸ­ CI/CD

### GitHub Actions

Der GitHub Actions-Workflow umfasst:

```yaml
# .github/workflows/spleeter-tests.yml
name: ğŸµ Spleeter Tests CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    - cron: '0 2 * * *'  # TÃ¤gliche Tests

jobs:
  smoke-tests:    # Schnelle ÃœberprÃ¼fungs-Tests
  unit-tests:     # Multi-Plattform Unit-Tests
  integration-tests: # Integrations-Tests mit Services
  performance-tests: # Leistungs-Tests und Benchmarks
  stress-tests:   # Last- und Stress-Tests
  security-tests: # Sicherheits-Tests
  code-quality:   # Code-QualitÃ¤ts-Checks
  final-report:   # Konsolidierter Bericht
```

### Lokale Konfiguration

```bash
# Lokale CI/CD-Pipeline-Simulation
./run_tests.sh --ci

# Pre-Commit-ÃœberprÃ¼fung
make pre-commit

# VollstÃ¤ndige Validierung
make check-all
```

### Kontinuierliche Integration

```bash
# Schnelle Tests fÃ¼r Entwicklung
make test-fast

# VollstÃ¤ndige Tests fÃ¼r Release
make test-all coverage-html

# Pre-Commit-ÃœberprÃ¼fung
make pre-commit
```

## ğŸ” Debugging

### Spezifisches Test-Debug

```bash
# Verbose Debug-Modus
make test-debug TEST=test_core.py::test_engine_initialization

# Debug mit pdb
pytest test_core.py::test_engine_initialization -vvv -s --pdb

# Leistungs-Profiling
make profile-tests
```

### Debug-Logs

```bash
# Detaillierte Log-Aktivierung
export SPLEETER_LOG_LEVEL="DEBUG"
pytest test_core.py -vvv -s --log-cli-level=DEBUG
```

### Watch-Modus Tests

```bash
# Watch-Modus fÃ¼r Entwicklung
make watch-tests

# Kontinuierliche Tests mit pytest-watch
pip install pytest-watch
ptw -- -m "not slow"
```

### Fehler-Debugging

```bash
# Tests mit erweiterten Fehlerinformationen
pytest --tb=long --capture=no

# Fehlerbericht-Generierung
pytest --html=failure_report.html --self-contained-html
```

## ğŸ“Š Metriken und Ãœberwachung

### Gesammelte Metriken

- **Leistung**: AusfÃ¼hrungszeit, Durchsatz
- **Ressourcen**: CPU, Speicher, I/O
- **QualitÃ¤t**: Abdeckung, KomplexitÃ¤t
- **ZuverlÃ¤ssigkeit**: Erfolgsrate, StabilitÃ¤t

### VerfÃ¼gbare Berichte

```bash
# VollstÃ¤ndiger HTML-Bericht
make test-report

# Leistungs-Metriken
make test-metrics

# JSON-Ergebnis-Export
make test-export
```

### Kontinuierliche Ãœberwachung

```bash
# Tests mit Ressourcen-Ãœberwachung
pytest test_performance.py::TestResourceMonitoring -v

# Speicher-Verfolgung
pytest --memray

# CPU-Profiling
pytest --profile
```

## ğŸ”§ Wartung

### AbhÃ¤ngigkeits-Updates

```bash
# Versions-ÃœberprÃ¼fung
pip list --outdated

# Sicheres Update
pip install --upgrade -r requirements/testing.txt

# KompatibilitÃ¤ts-ÃœberprÃ¼fung
./run_tests.sh --smoke
```

### Periodische AufrÃ¤umung

```bash
# VollstÃ¤ndige AufrÃ¤umung
make clean

# Cache-Entfernung
rm -rf .pytest_cache __pycache__ .coverage

# Artefakt-AufrÃ¤umung
rm -rf coverage_html test_report.html
```

### Test-Optimierung

```bash
# Parallele Tests
make test-parallel

# Fixture-Optimierung
pytest --setup-show

# Dauer-Analyse
pytest --durations=0
```

## ğŸ” Sicherheit

### Sicherheits-Tests

```bash
# Spezifische Sicherheits-Tests
make test-security

# Schwachstellen-Scan
bandit -r ../../spleeter/

# AbhÃ¤ngigkeits-ÃœberprÃ¼fung
safety check
```

### Eingabe-Validierung

```bash
# Validierungs-Tests
pytest -k "validation or sanitize" test_utils.py

# Injection-Tests
pytest -k "security" test_exceptions.py
```

## ğŸ“ Mitwirken

### Neue Tests hinzufÃ¼gen

1. **Test-Datei erstellen** im entsprechenden Verzeichnis
2. **Fixtures verwenden** definiert in `conftest.py`
3. **Entsprechende Marker hinzufÃ¼gen** (`@pytest.mark.unit`, etc.)
4. **Tests dokumentieren** mit klaren Docstrings
5. **Coverage Ã¼berprÃ¼fen** mit `make coverage`

### Test-Struktur

```python
import pytest
from unittest.mock import Mock, patch

class TestNewFeature:
    """Tests fÃ¼r neue FunktionalitÃ¤t."""
    
    @pytest.mark.unit
    @pytest.mark.fast
    def test_basic_functionality(self, mock_engine):
        """Test der GrundfunktionalitÃ¤t."""
        # Arrange
        input_data = "test_input"
        expected = "expected_output"
        
        # Act
        result = mock_engine.process(input_data)
        
        # Assert
        assert result == expected
    
    @pytest.mark.integration
    @pytest.mark.slow
    async def test_integration_workflow(self):
        """VollstÃ¤ndiger Integrations-Test."""
        # Integrations-Test mit echten AbhÃ¤ngigkeiten
        pass
```

### Test-Richtlinien

- **Benennung**: `test_<functionality>_<scenario>`
- **Isolation**: Jeder Test muss unabhÃ¤ngig sein
- **Mocking**: Mocks fÃ¼r externe AbhÃ¤ngigkeiten verwenden
- **Assertions**: Klare und spezifische Assertions
- **Dokumentation**: ErklÃ¤rende Docstrings
- **Marker**: Angemessene pytest-Marker-Verwendung

### Pre-Commit-Validierung

```bash
# VollstÃ¤ndige Validierung vor Commit
make pre-commit

# Einzelne ÃœberprÃ¼fungen
make format    # Code-Formatierung
make lint      # Linting-ÃœberprÃ¼fungen
make type-check # Typ-ÃœberprÃ¼fung
make test-fast # Schnelle Tests
```

## ğŸ¯ QualitÃ¤tsziele

### Leistungs-Ziele

- **Unit-Tests**: < 30 Sekunden gesamt
- **Integrations-Tests**: < 2 Minuten gesamt  
- **Leistungs-Tests**: < 5 Minuten gesamt
- **Audio-Trennung**: < 5 Sekunden/Datei

### Coverage-Ziele

- **Globale Coverage**: â‰¥ 85%
- **Branch-Coverage**: â‰¥ 75%
- **Kritische Module**: â‰¥ 90%

### ZuverlÃ¤ssigkeits-Ziele

- **Erfolgsrate**: â‰¥ 99%
- **StabilitÃ¤t**: 0 instabile Tests
- **Reproduzierbarkeit**: 100%

## ğŸš€ Leistung und Optimierungen

### Test-Optimierungen

```bash
# Parallele Tests
pytest -n auto

# Ergebnis-Caching
pytest --cache-clear  # Cache zurÃ¼cksetzen
pytest --lf          # Nur letzte Fehlgeschlagene
pytest --ff          # Fehlgeschlagene zuerst

# Speicher-Optimierung
pytest --maxfail=1 --tb=no
```

### Leistungs-Ãœberwachung

```bash
# Detailliertes Profiling
pytest --profile --profile-svg

# Speicher-Ãœberwachung
pytest --memray --memray-bin-path=memory_profile.bin

# Engpass-Analyse
pytest --durations=20
```

## ğŸ“ Support und Hilfe

### Ressourcen

- **Dokumentation**: Dieses README und Docstrings
- **Beispiele**: Bestehende Test-Dateien
- **Konfiguration**: `pyproject.toml`, `conftest.py`
- **Skripte**: `Makefile`, `run_tests.sh`

### Hilfe-Befehle

```bash
# Makefile-Hilfe
make help

# Bash-Skript-Hilfe
./run_tests.sh --help

# Pytest-Hilfe
pytest --help

# Aktive Konfiguration
make show-config
```

### HÃ¤ufige ProblemlÃ¶sungen

**Fehlschlagende Tests**:
```bash
# Verbose Debug-Modus
pytest -vvv -s --tb=long

# Isolierte Tests
pytest test_file.py::test_function -v
```

**Langsame Leistung**:
```bash
# Profiling
make profile-tests

# Nur schnelle Tests
make test-fast
```

**Coverage-Probleme**:
```bash
# Detaillierter Bericht
make coverage-html

# Fehlende Dateien
coverage report --show-missing
```

---

## ğŸ‰ Fazit

Diese vollstÃ¤ndige Test-Suite gewÃ¤hrleistet die QualitÃ¤t, Leistung und ZuverlÃ¤ssigkeit des Spleeter-Moduls. Verwenden Sie diese Dokumentation als Referenz fÃ¼r alle Ihre Test-Anforderungen!

**Frohes Testen! ğŸš€ğŸµ**

---

*Entwickelt mit â¤ï¸ vom Enterprise-Expertenteam fÃ¼r den Spotify AI-Agent*
