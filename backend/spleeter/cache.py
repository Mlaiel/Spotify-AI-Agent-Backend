"""
üéµ Spotify AI Agent - Cache Manager
==================================

Syst√®me de cache avanc√© pour optimiser les performances de Spleeter
avec support Redis, m√©moire et stockage persistant.

üéñÔ∏è D√©velopp√© par l'√©quipe d'experts enterprise
"""

import os
import pickle
import json
import hashlib
import asyncio
import aiofiles
from typing import Any, Dict, List, Optional, Union, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import logging
import time
from datetime import datetime, timedelta
import tempfile
import shutil
import sqlite3
from contextlib import asynccontextmanager
import numpy as np

from .exceptions import SpleeterError
from .monitoring import PerformanceMonitor

logger = logging.getLogger(__name__)


@dataclass
class CacheEntry:
    """Entr√©e de cache avec m√©tadonn√©es"""
    key: str
    value_hash: str
    size_bytes: int
    created_at: float
    last_accessed: float
    access_count: int
    ttl: Optional[float] = None
    tags: List[str] = None
    
    def __post_init__(self):
        if self.tags is None:
            self.tags = []
    
    def is_expired(self) -> bool:
        """V√©rifie si l'entr√©e a expir√©"""
        if self.ttl is None:
            return False
        return time.time() > (self.created_at + self.ttl)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convertit en dictionnaire"""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'CacheEntry':
        """Cr√©e depuis un dictionnaire"""
        return cls(**data)


class CacheConfig:
    """Configuration du syst√®me de cache"""
    
    def __init__(
        self,
        # Stockage
        cache_dir: Optional[str] = None,
        max_memory_mb: int = 512,
        max_disk_gb: float = 5.0,
        
        # Redis (optionnel)
        redis_enabled: bool = False,
        redis_url: str = "redis://localhost:6379",
        redis_db: int = 0,
        redis_password: Optional[str] = None,
        
        # TTL par d√©faut
        default_ttl: int = 3600,  # 1 heure
        audio_cache_ttl: int = 7200,  # 2 heures
        model_cache_ttl: int = 86400,  # 24 heures
        
        # Strat√©gies
        eviction_policy: str = "lru",  # lru, lfu, ttl
        compression_enabled: bool = True,
        encryption_enabled: bool = False,
        
        # Performance
        cleanup_interval: int = 300,  # 5 minutes
        batch_size: int = 100
    ):
        self.cache_dir = Path(cache_dir or os.path.expanduser("~/.spleeter/cache"))
        self.max_memory_bytes = max_memory_mb * 1024 * 1024
        self.max_disk_bytes = max_disk_gb * 1024 * 1024 * 1024
        
        self.redis_enabled = redis_enabled
        self.redis_url = redis_url
        self.redis_db = redis_db
        self.redis_password = redis_password
        
        self.default_ttl = default_ttl
        self.audio_cache_ttl = audio_cache_ttl
        self.model_cache_ttl = model_cache_ttl
        
        self.eviction_policy = eviction_policy
        self.compression_enabled = compression_enabled
        self.encryption_enabled = encryption_enabled
        
        self.cleanup_interval = cleanup_interval
        self.batch_size = batch_size
        
        # Cr√©er le r√©pertoire de cache
        self.cache_dir.mkdir(parents=True, exist_ok=True)


class MemoryCache:
    """Cache en m√©moire avec LRU"""
    
    def __init__(self, max_size_bytes: int):
        self.max_size_bytes = max_size_bytes
        self._cache = {}
        self._access_order = []
        self._current_size = 0
    
    def get(self, key: str) -> Optional[Any]:
        """R√©cup√®re une valeur du cache"""
        if key not in self._cache:
            return None
        
        # Mise √† jour de l'ordre d'acc√®s
        if key in self._access_order:
            self._access_order.remove(key)
        self._access_order.append(key)
        
        entry = self._cache[key]
        entry.last_accessed = time.time()
        entry.access_count += 1
        
        return entry
    
    def set(self, key: str, entry: CacheEntry) -> bool:
        """Stocke une valeur dans le cache"""
        # V√©rifier si l'entr√©e existe d√©j√†
        if key in self._cache:
            old_entry = self._cache[key]
            self._current_size -= old_entry.size_bytes
            self._access_order.remove(key)
        
        # V√©rifier l'espace disponible
        while self._current_size + entry.size_bytes > self.max_size_bytes and self._access_order:
            self._evict_lru()
        
        if self._current_size + entry.size_bytes > self.max_size_bytes:
            return False  # Pas assez de place
        
        # Ajouter l'entr√©e
        self._cache[key] = entry
        self._access_order.append(key)
        self._current_size += entry.size_bytes
        
        return True
    
    def remove(self, key: str) -> bool:
        """Supprime une entr√©e du cache"""
        if key not in self._cache:
            return False
        
        entry = self._cache[key]
        del self._cache[key]
        self._access_order.remove(key)
        self._current_size -= entry.size_bytes
        
        return True
    
    def _evict_lru(self):
        """√âvince l'entr√©e la moins r√©cemment utilis√©e"""
        if not self._access_order:
            return
        
        lru_key = self._access_order[0]
        self.remove(lru_key)
    
    def clear(self):
        """Vide le cache"""
        self._cache.clear()
        self._access_order.clear()
        self._current_size = 0
    
    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques du cache"""
        return {
            "entries": len(self._cache),
            "size_bytes": self._current_size,
            "max_size_bytes": self.max_size_bytes,
            "usage_percentage": (self._current_size / self.max_size_bytes) * 100
        }


class DiskCache:
    """Cache sur disque avec base de donn√©es SQLite"""
    
    def __init__(self, cache_dir: Path, max_size_bytes: int):
        self.cache_dir = cache_dir
        self.max_size_bytes = max_size_bytes
        self.db_path = cache_dir / "cache.db"
        
        # Initialiser la base de donn√©es
        self._init_database()
    
    def _init_database(self):
        """Initialise la base de donn√©es SQLite"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS cache_entries (
                    key TEXT PRIMARY KEY,
                    value_hash TEXT NOT NULL,
                    size_bytes INTEGER NOT NULL,
                    created_at REAL NOT NULL,
                    last_accessed REAL NOT NULL,
                    access_count INTEGER NOT NULL,
                    ttl REAL,
                    tags TEXT,
                    file_path TEXT NOT NULL
                )
            """)
            
            # Index pour optimiser les requ√™tes
            conn.execute("CREATE INDEX IF NOT EXISTS idx_last_accessed ON cache_entries(last_accessed)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_created_at ON cache_entries(created_at)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_ttl ON cache_entries(ttl)")
            
            conn.commit()
    
    async def get(self, key: str) -> Optional[Tuple[CacheEntry, Any]]:
        """R√©cup√®re une entr√©e du cache disque"""
        def _get_from_db():
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute(
                    "SELECT * FROM cache_entries WHERE key = ?",
                    (key,)
                )
                row = cursor.fetchone()
                return row
        
        # R√©cup√©ration depuis la DB
        loop = asyncio.get_event_loop()
        row = await loop.run_in_executor(None, _get_from_db)
        
        if not row:
            return None
        
        # Reconstruction de l'entr√©e
        entry_data = {
            "key": row[0],
            "value_hash": row[1],
            "size_bytes": row[2],
            "created_at": row[3],
            "last_accessed": row[4],
            "access_count": row[5],
            "ttl": row[6],
            "tags": json.loads(row[7]) if row[7] else []
        }
        
        entry = CacheEntry.from_dict(entry_data)
        file_path = Path(row[8])
        
        # V√©rifier l'expiration
        if entry.is_expired():
            await self.remove(key)
            return None
        
        # V√©rifier que le fichier existe
        if not file_path.exists():
            await self.remove(key)
            return None
        
        # Charger la valeur
        try:
            async with aiofiles.open(file_path, 'rb') as f:
                data = await f.read()
                value = pickle.loads(data)
            
            # Mise √† jour des statistiques d'acc√®s
            entry.last_accessed = time.time()
            entry.access_count += 1
            await self._update_access_stats(key, entry)
            
            return entry, value
            
        except Exception as e:
            logger.error(f"Erreur lecture cache {key}: {e}")
            await self.remove(key)
            return None
    
    async def set(self, key: str, value: Any, entry: CacheEntry) -> bool:
        """Stocke une entr√©e dans le cache disque"""
        # G√©n√©rer le chemin du fichier
        value_hash = hashlib.md5(str(value).encode()).hexdigest()
        file_path = self.cache_dir / f"{key}_{value_hash}.cache"
        
        try:
            # S√©rialiser la valeur
            data = pickle.dumps(value)
            entry.size_bytes = len(data)
            entry.value_hash = value_hash
            
            # V√©rifier l'espace disponible
            await self._ensure_space(entry.size_bytes)
            
            # Sauvegarder le fichier
            async with aiofiles.open(file_path, 'wb') as f:
                await f.write(data)
            
            # Ins√©rer dans la base de donn√©es
            def _insert_db():
                with sqlite3.connect(self.db_path) as conn:
                    conn.execute("""
                        INSERT OR REPLACE INTO cache_entries 
                        (key, value_hash, size_bytes, created_at, last_accessed, 
                         access_count, ttl, tags, file_path)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        entry.key,
                        entry.value_hash,
                        entry.size_bytes,
                        entry.created_at,
                        entry.last_accessed,
                        entry.access_count,
                        entry.ttl,
                        json.dumps(entry.tags),
                        str(file_path)
                    ))
                    conn.commit()
            
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, _insert_db)
            
            return True
            
        except Exception as e:
            logger.error(f"Erreur stockage cache {key}: {e}")
            if file_path.exists():
                file_path.unlink()
            return False
    
    async def remove(self, key: str) -> bool:
        """Supprime une entr√©e du cache"""
        def _remove_from_db():
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute(
                    "SELECT file_path FROM cache_entries WHERE key = ?",
                    (key,)
                )
                row = cursor.fetchone()
                
                if row:
                    conn.execute("DELETE FROM cache_entries WHERE key = ?", (key,))
                    conn.commit()
                    return row[0]
                return None
        
        loop = asyncio.get_event_loop()
        file_path_str = await loop.run_in_executor(None, _remove_from_db)
        
        if file_path_str:
            file_path = Path(file_path_str)
            if file_path.exists():
                file_path.unlink()
            return True
        
        return False
    
    async def _update_access_stats(self, key: str, entry: CacheEntry):
        """Met √† jour les statistiques d'acc√®s"""
        def _update():
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("""
                    UPDATE cache_entries 
                    SET last_accessed = ?, access_count = ?
                    WHERE key = ?
                """, (entry.last_accessed, entry.access_count, key))
                conn.commit()
        
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, _update)
    
    async def _ensure_space(self, required_bytes: int):
        """S'assure qu'il y a assez d'espace disque"""
        def _get_current_size():
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("SELECT SUM(size_bytes) FROM cache_entries")
                result = cursor.fetchone()[0]
                return result or 0
        
        loop = asyncio.get_event_loop()
        current_size = await loop.run_in_executor(None, _get_current_size)
        
        # Nettoyer si n√©cessaire
        while current_size + required_bytes > self.max_size_bytes:
            evicted = await self._evict_entries(1)
            if not evicted:
                break
            current_size = await loop.run_in_executor(None, _get_current_size)
    
    async def _evict_entries(self, count: int) -> int:
        """√âvince des entr√©es selon la politique LRU"""
        def _get_lru_keys():
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    SELECT key FROM cache_entries 
                    ORDER BY last_accessed ASC 
                    LIMIT ?
                """, (count,))
                return [row[0] for row in cursor.fetchall()]
        
        loop = asyncio.get_event_loop()
        lru_keys = await loop.run_in_executor(None, _get_lru_keys)
        
        evicted_count = 0
        for key in lru_keys:
            if await self.remove(key):
                evicted_count += 1
        
        return evicted_count
    
    async def cleanup_expired(self) -> int:
        """Nettoie les entr√©es expir√©es"""
        def _get_expired_keys():
            current_time = time.time()
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    SELECT key FROM cache_entries 
                    WHERE ttl IS NOT NULL AND (created_at + ttl) < ?
                """, (current_time,))
                return [row[0] for row in cursor.fetchall()]
        
        loop = asyncio.get_event_loop()
        expired_keys = await loop.run_in_executor(None, _get_expired_keys)
        
        removed_count = 0
        for key in expired_keys:
            if await self.remove(key):
                removed_count += 1
        
        logger.info(f"Nettoyage cache: {removed_count} entr√©es expir√©es supprim√©es")
        return removed_count
    
    async def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques du cache disque"""
        def _get_stats():
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    SELECT 
                        COUNT(*) as entries,
                        SUM(size_bytes) as total_size,
                        AVG(access_count) as avg_access_count,
                        MIN(created_at) as oldest_entry,
                        MAX(last_accessed) as newest_access
                    FROM cache_entries
                """)
                row = cursor.fetchone()
                return row
        
        loop = asyncio.get_event_loop()
        stats = await loop.run_in_executor(None, _get_stats)
        
        return {
            "entries": stats[0] or 0,
            "size_bytes": stats[1] or 0,
            "max_size_bytes": self.max_size_bytes,
            "usage_percentage": ((stats[1] or 0) / self.max_size_bytes) * 100,
            "avg_access_count": stats[2] or 0,
            "oldest_entry": stats[3],
            "newest_access": stats[4]
        }


class RedisCache:
    """Cache Redis pour distribu√©"""
    
    def __init__(self, config: CacheConfig):
        self.config = config
        self._redis = None
    
    async def connect(self):
        """Connexion √† Redis"""
        try:
            import aioredis
            
            self._redis = await aioredis.from_url(
                self.config.redis_url,
                db=self.config.redis_db,
                password=self.config.redis_password,
                encoding="utf-8",
                decode_responses=False  # Pour les donn√©es binaires
            )
            
            # Test de connexion
            await self._redis.ping()
            logger.info("Connexion Redis √©tablie")
            
        except ImportError:
            logger.warning("aioredis non disponible - cache Redis d√©sactiv√©")
            self._redis = None
        except Exception as e:
            logger.error(f"Erreur connexion Redis: {e}")
            self._redis = None
    
    async def get(self, key: str) -> Optional[Tuple[CacheEntry, Any]]:
        """R√©cup√®re une entr√©e depuis Redis"""
        if not self._redis:
            return None
        
        try:
            # R√©cup√©ration des m√©tadonn√©es
            metadata_key = f"meta:{key}"
            metadata_data = await self._redis.get(metadata_key)
            
            if not metadata_data:
                return None
            
            metadata = json.loads(metadata_data)
            entry = CacheEntry.from_dict(metadata)
            
            # V√©rifier l'expiration
            if entry.is_expired():
                await self.remove(key)
                return None
            
            # R√©cup√©ration de la valeur
            value_key = f"data:{key}"
            value_data = await self._redis.get(value_key)
            
            if not value_data:
                await self.remove(key)
                return None
            
            value = pickle.loads(value_data)
            
            # Mise √† jour des statistiques d'acc√®s
            entry.last_accessed = time.time()
            entry.access_count += 1
            await self._update_metadata(key, entry)
            
            return entry, value
            
        except Exception as e:
            logger.error(f"Erreur lecture Redis {key}: {e}")
            return None
    
    async def set(self, key: str, value: Any, entry: CacheEntry) -> bool:
        """Stocke une entr√©e dans Redis"""
        if not self._redis:
            return False
        
        try:
            # S√©rialisation
            value_data = pickle.dumps(value)
            entry.size_bytes = len(value_data)
            
            # Stockage des donn√©es
            value_key = f"data:{key}"
            metadata_key = f"meta:{key}"
            
            # Pipeline pour atomicit√©
            pipe = self._redis.pipeline()
            
            # Donn√©es
            if entry.ttl:
                pipe.setex(value_key, int(entry.ttl), value_data)
                pipe.setex(metadata_key, int(entry.ttl), json.dumps(entry.to_dict()))
            else:
                pipe.set(value_key, value_data)
                pipe.set(metadata_key, json.dumps(entry.to_dict()))
            
            await pipe.execute()
            return True
            
        except Exception as e:
            logger.error(f"Erreur stockage Redis {key}: {e}")
            return False
    
    async def remove(self, key: str) -> bool:
        """Supprime une entr√©e de Redis"""
        if not self._redis:
            return False
        
        try:
            value_key = f"data:{key}"
            metadata_key = f"meta:{key}"
            
            result = await self._redis.delete(value_key, metadata_key)
            return result > 0
            
        except Exception as e:
            logger.error(f"Erreur suppression Redis {key}: {e}")
            return False
    
    async def _update_metadata(self, key: str, entry: CacheEntry):
        """Met √† jour les m√©tadonn√©es"""
        if not self._redis:
            return
        
        try:
            metadata_key = f"meta:{key}"
            ttl = await self._redis.ttl(metadata_key)
            
            if ttl > 0:
                await self._redis.setex(metadata_key, ttl, json.dumps(entry.to_dict()))
            else:
                await self._redis.set(metadata_key, json.dumps(entry.to_dict()))
                
        except Exception as e:
            logger.error(f"Erreur mise √† jour m√©tadonn√©es Redis {key}: {e}")
    
    async def cleanup(self):
        """Ferme la connexion Redis"""
        if self._redis:
            await self._redis.close()


class CacheManager:
    """
    Gestionnaire de cache multi-niveaux pour Spleeter
    
    Features:
    - Cache m√©moire (L1) + disque (L2) + Redis (L3)
    - √âviction automatique LRU/LFU/TTL
    - Compression et chiffrement optionnels
    - Nettoyage automatique
    - M√©triques d√©taill√©es
    """
    
    def __init__(self, config: Optional[CacheConfig] = None):
        """
        Initialise le gestionnaire de cache
        
        Args:
            config: Configuration du cache
        """
        self.config = config or CacheConfig()
        
        # Composants de cache
        self.memory_cache = MemoryCache(self.config.max_memory_bytes)
        self.disk_cache = DiskCache(self.config.cache_dir, self.config.max_disk_bytes)
        self.redis_cache = RedisCache(self.config) if self.config.redis_enabled else None
        
        # Monitoring
        self.monitor = PerformanceMonitor()
        
        # T√¢che de nettoyage
        self._cleanup_task = None
        self._running = False
        
        # Statistiques
        self._stats = {
            "hits": 0,
            "misses": 0,
            "sets": 0,
            "evictions": 0
        }
        
        logger.info(f"CacheManager initialis√©: {self.config.cache_dir}")
    
    async def start(self):
        """D√©marre le gestionnaire de cache"""
        if self._running:
            return
        
        self._running = True
        
        # Connexion Redis si activ√©
        if self.redis_cache:
            await self.redis_cache.connect()
        
        # D√©marrage du nettoyage automatique
        self._cleanup_task = asyncio.create_task(self._cleanup_loop())
        
        logger.info("CacheManager d√©marr√©")
    
    async def stop(self):
        """Arr√™te le gestionnaire de cache"""
        if not self._running:
            return
        
        self._running = False
        
        # Arr√™t du nettoyage
        if self._cleanup_task:
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass
        
        # Fermeture Redis
        if self.redis_cache:
            await self.redis_cache.cleanup()
        
        logger.info("CacheManager arr√™t√©")
    
    async def _cleanup_loop(self):
        """Boucle de nettoyage automatique"""
        while self._running:
            try:
                await asyncio.sleep(self.config.cleanup_interval)
                await self._cleanup_expired()
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Erreur nettoyage cache: {e}")
    
    async def _cleanup_expired(self):
        """Nettoie les entr√©es expir√©es"""
        removed_count = await self.disk_cache.cleanup_expired()
        if removed_count > 0:
            self._stats["evictions"] += removed_count
    
    def _get_cache_key(self, key: str, namespace: str = "default") -> str:
        """G√©n√®re une cl√© de cache normalis√©e"""
        return f"{namespace}:{hashlib.md5(key.encode()).hexdigest()}"
    
    def _determine_ttl(self, tags: List[str]) -> Optional[float]:
        """D√©termine le TTL selon les tags"""
        if "audio" in tags:
            return self.config.audio_cache_ttl
        elif "model" in tags:
            return self.config.model_cache_ttl
        else:
            return self.config.default_ttl
    
    async def get(
        self,
        key: str,
        namespace: str = "default"
    ) -> Optional[Any]:
        """
        R√©cup√®re une valeur du cache
        
        Args:
            key: Cl√© de cache
            namespace: Namespace de la cl√©
            
        Returns:
            Valeur cach√©e ou None
        """
        cache_key = self._get_cache_key(key, namespace)
        
        self.monitor.start_timer("cache_get")
        
        try:
            # L1: Cache m√©moire
            entry = self.memory_cache.get(cache_key)
            if entry and not entry.is_expired():
                self._stats["hits"] += 1
                return entry
            
            # L2: Cache disque
            result = await self.disk_cache.get(cache_key)
            if result:
                entry, value = result
                
                # Promotion vers le cache m√©moire
                self.memory_cache.set(cache_key, entry)
                
                self._stats["hits"] += 1
                return value
            
            # L3: Cache Redis
            if self.redis_cache:
                result = await self.redis_cache.get(cache_key)
                if result:
                    entry, value = result
                    
                    # Promotion vers les caches sup√©rieurs
                    self.memory_cache.set(cache_key, entry)
                    await self.disk_cache.set(cache_key, value, entry)
                    
                    self._stats["hits"] += 1
                    return value
            
            # Cache miss
            self._stats["misses"] += 1
            return None
            
        finally:
            self.monitor.end_timer("cache_get")
    
    async def set(
        self,
        key: str,
        value: Any,
        namespace: str = "default",
        ttl: Optional[float] = None,
        tags: Optional[List[str]] = None
    ) -> bool:
        """
        Stocke une valeur dans le cache
        
        Args:
            key: Cl√© de cache
            value: Valeur √† cacher
            namespace: Namespace de la cl√©
            ttl: Dur√©e de vie en secondes
            tags: Tags pour classification
            
        Returns:
            True si stock√© avec succ√®s
        """
        cache_key = self._get_cache_key(key, namespace)
        tags = tags or []
        
        # D√©terminer le TTL
        if ttl is None:
            ttl = self._determine_ttl(tags)
        
        # Cr√©er l'entr√©e
        entry = CacheEntry(
            key=cache_key,
            value_hash="",  # Sera calcul√© par chaque cache
            size_bytes=0,   # Sera calcul√© par chaque cache
            created_at=time.time(),
            last_accessed=time.time(),
            access_count=1,
            ttl=ttl,
            tags=tags
        )
        
        self.monitor.start_timer("cache_set")
        
        try:
            success = False
            
            # Stockage dans les caches (ordre inverse pour √©viter les incoh√©rences)
            
            # L3: Redis
            if self.redis_cache:
                redis_success = await self.redis_cache.set(cache_key, value, entry)
                success = success or redis_success
            
            # L2: Disque
            disk_success = await self.disk_cache.set(cache_key, value, entry)
            success = success or disk_success
            
            # L1: M√©moire
            memory_success = self.memory_cache.set(cache_key, entry)
            success = success or memory_success
            
            if success:
                self._stats["sets"] += 1
            
            return success
            
        finally:
            self.monitor.end_timer("cache_set")
    
    async def remove(
        self,
        key: str,
        namespace: str = "default"
    ) -> bool:
        """
        Supprime une entr√©e du cache
        
        Args:
            key: Cl√© √† supprimer
            namespace: Namespace de la cl√©
            
        Returns:
            True si supprim√©
        """
        cache_key = self._get_cache_key(key, namespace)
        
        success = False
        
        # Suppression de tous les niveaux
        memory_success = self.memory_cache.remove(cache_key)
        disk_success = await self.disk_cache.remove(cache_key)
        
        success = memory_success or disk_success
        
        if self.redis_cache:
            redis_success = await self.redis_cache.remove(cache_key)
            success = success or redis_success
        
        return success
    
    async def clear(self, namespace: Optional[str] = None):
        """
        Vide le cache
        
        Args:
            namespace: Namespace √† vider (tous si None)
        """
        if namespace is None:
            # Vider compl√®tement
            self.memory_cache.clear()
            
            # Pour le disque et Redis, il faudrait une m√©thode clear sp√©cifique
            logger.warning("clear() complet non impl√©ment√© pour disque/Redis")
        else:
            # Vider un namespace sp√©cifique (complexe √† impl√©menter)
            logger.warning("clear() par namespace non impl√©ment√©")
    
    async def invalidate_by_tags(self, tags: List[str]):
        """
        Invalide les entr√©es ayant certains tags
        
        Args:
            tags: Tags √† invalider
        """
        # Implementation complexe n√©cessaire pour scanner tous les caches
        logger.warning("invalidate_by_tags() non impl√©ment√©")
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Retourne les statistiques du cache
        
        Returns:
            Dictionnaire des statistiques
        """
        memory_stats = self.memory_cache.get_stats()
        
        total_requests = self._stats["hits"] + self._stats["misses"]
        hit_rate = (self._stats["hits"] / total_requests * 100) if total_requests > 0 else 0
        
        return {
            "requests": {
                "total": total_requests,
                "hits": self._stats["hits"],
                "misses": self._stats["misses"],
                "hit_rate_percent": hit_rate
            },
            "operations": {
                "sets": self._stats["sets"],
                "evictions": self._stats["evictions"]
            },
            "memory_cache": memory_stats,
            "config": {
                "max_memory_mb": self.config.max_memory_bytes / 1024 / 1024,
                "max_disk_gb": self.config.max_disk_bytes / 1024 / 1024 / 1024,
                "redis_enabled": self.config.redis_enabled,
                "compression_enabled": self.config.compression_enabled
            }
        }
    
    async def get_detailed_stats(self) -> Dict[str, Any]:
        """
        Retourne les statistiques d√©taill√©es incluant le disque
        
        Returns:
            Statistiques compl√®tes
        """
        stats = self.get_stats()
        
        # Ajout des stats disque (asynchrone)
        disk_stats = await self.disk_cache.get_stats()
        stats["disk_cache"] = disk_stats
        
        return stats
    
    @asynccontextmanager
    async def transaction(self):
        """
        Context manager pour les transactions de cache
        
        Usage:
            async with cache_manager.transaction():
                await cache_manager.set("key1", value1)
                await cache_manager.set("key2", value2)
                # Commit automatique √† la sortie
        """
        # Pour l'instant, pas de vraie transaction
        # Implementation future avec rollback possible
        try:
            yield self
        except Exception:
            # Rollback des op√©rations si n√©cessaire
            raise
    
    async def cleanup(self):
        """Nettoyage des ressources"""
        await self.stop()
        await self.monitor.cleanup()
        logger.info("CacheManager nettoy√©")
